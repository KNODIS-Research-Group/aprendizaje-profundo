{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Métricas para problemas de clasificación y regresión<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-02-27</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35405b3c",
   "metadata": {},
   "source": [
    "En un esquema de aprendizaje supervisado, el objetivo es modelar de algún modo la relación entre las características medidas de los datos y alguna etiqueta asociada a dichos datos. De esta manera, si se logra un modelo capaz de capturar la naturaleza de dicha relación, podremos usarlo para aplicar etiquetas a datos nuevos y desconocidos.\n",
    "\n",
    "Este objetivo se da en dos tareas distintas, tanto en la definición de su arquitectura como en la medición de su rendimiento:\n",
    "\n",
    "- **Tareas de regresión**, también llamadas tareas de ajuste, en las que **las etiquetas son cantidades continuas**, y\n",
    "- **Tareas de clasificación**, en las que **las etiquetas son categorías discretas**.\n",
    "\n",
    "El autor [Peter Drucker](https://es.wikipedia.org/wiki/Peter_F._Drucker) dijo una vez: «_No se puede mejorar lo que no se puede medir_». Aquí es donde las métricas resultan útiles: evalúan la eficacia con la que el algoritmo representa el conjunto de datos. Pero cada tarea diferente viene con su propio conjunto de métricas, y es interesante conocerlas para entender el estado de nuestro modelo predictivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6d24f",
   "metadata": {},
   "source": [
    "Propondremos soluciones a dos tipos diferentes de problemas, uno de clasificación y otro de regresión. Para ellos, también definiremos una serie de métricas para medir su rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({\"axes.grid\" : False})\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a697a8e",
   "metadata": {},
   "source": [
    "## Problemas de regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa76c3e",
   "metadata": {},
   "source": [
    "Esta clase de problemas consiste en **predecir un valor continuo a partir de un conjunto de características de entrada**. Este enfoque se utiliza en múltiples aplicaciones, como la estimación de precios, la proyección de ventas o, como en el caso que nos ocupa, la evaluación de la progresión de una enfermedad.\n",
    "\n",
    "En este ejemplo trabajaremos con el _dataset_ de diabetes, un conjunto de datos clínicos en el que se han registrado 10 variables relevantes sobre pacientes. Estas variables resumen información extraída de estudios médicos y reflejan distintos aspectos de la salud, tales como medidas relacionadas con la respuesta a la insulina y otros indicadores fisiológicos.\n",
    "\n",
    "El objetivo es construir un modelo sencillo que, a partir de estas características, sea capaz de predecir la progresión de la diabetes un año después, representada por un valor numérico. Esta tarea ilustra cómo las técnicas de aprendizaje automático pueden aplicarse en el ámbito de la salud para identificar patrones y predecir la evolución de condiciones médicas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ffc508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el conjunto de datos de diabetes ...\n",
    "data = sklearn.datasets.load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "# ... y lo separamos en dos conjuntos, entrenamiento y test\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X,  # Las entradas para los ejemplos de nuestro problema\n",
    "    y,  # Sus salidas correspondientes\n",
    "    test_size=0.2,  # Separación 80%/20%\n",
    "    random_state=42,  # Para ayudar a la reproducibilidad\n",
    ")\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(f'Training shape: {x_train.shape} input, {y_train.shape} output')\n",
    "print(f'Test shape:     {x_test.shape} input, {y_test.shape} output')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8d8a63",
   "metadata": {},
   "source": [
    "Cuando entrenamos una red neuronal, alimentar el modelo directamente con los datos en bruto puede traernos algunos inconvenientes. Esto se debe a que las diferentes características pueden tener rangos de valores muy distintos, y aquellos con números mucho más altos tienden a «robar» el protagonismo durante el aprendizaje, haciendo que el modelo se enfoque en ellos y, en consecuencia, descuide otras variables que también pueden ser importantes. Esto no solo alarga el tiempo de entrenamiento, sino que también puede dificultar que el modelo encuentre una solución óptima, e incluso provocar problemas como el desvanecimiento o la explosión de gradientes.\n",
    "\n",
    "Para evitar estos problemas y asegurar que cada característica aporte de manera equilibrada, es fundamental normalizar los datos antes de introducirlos en la red. Una técnica común es la normalización mediante la [puntuación estándar o _z-score_](https://en.wikipedia.org/wiki/Standard_score). Básicamente, se trata de restar la media de cada característica y dividir el resultado por su desviación estándar, de modo que todas las variables queden centradas alrededor de $0$ y con una desviación estándar de $1$. Este proceso equilibra la escala de los datos, aunque asume que cada característica sigue una distribución cercana a la normal. Si este no es el caso, puede ser necesario explorar otras técnicas de normalización o transformación para preparar adecuadamente los datos para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e42b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"x_train example before: {x_train[1]}...\")\n",
    "mean = x_train.mean(axis=0)\n",
    "std = x_train.std(axis=0)\n",
    "x_train = (x_train - mean) / std\n",
    "x_test = (x_test - mean) / std  # Usamos los mismos parámetros de normalización\n",
    "print(f\"x_train example after: {x_train[1]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ed8679",
   "metadata": {},
   "source": [
    "Crearemos un modelo de dos capas ocultas de $8$ neuronas cada una (activación ReLU), seguidas de una única neurona de salida (ya que vamos a predecir un único valor) y, aunque en muchos casos las salidas suelen estar normalizadas, en este caso la activación de la última neurona será lineal para no añadir complicación al modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9a74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.LazyLinear(out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=8),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=1),\n",
    ")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0787935",
   "metadata": {},
   "source": [
    "Ahora, entrenaremos el modelo para ver cómo se comporta el proceso de entrenamiento. Para este proceso vamos a separar un pequeño conjunto que usaremos para la validación de nuestro modelo durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0813b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "n_val = int(len(x_train) * val_split)\n",
    "indices = torch.randperm(len(x_train))\n",
    "val_indices = indices[:n_val]\n",
    "train_indices = indices[n_val:]\n",
    "\n",
    "x_train_final = x_train[train_indices]\n",
    "y_train_final = y_train[train_indices]\n",
    "x_val = x_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "\n",
    "epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train_final)\n",
    "    loss = criterion(y_pred, y_train_final)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(x_val)\n",
    "        val_loss = criterion(y_val_pred, y_val)\n",
    "    val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b2015",
   "metadata": {},
   "source": [
    "Ahora veamos cómo han evolucionado las pérdidas durante el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d6a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a89a46",
   "metadata": {},
   "source": [
    "Un entrenamiento que nos muestra claramente que el modelo está sobreepecializado. No es el objetivo abordar esto ahora, es un problema para nuestro yo futuro. Ahora lo que haremos es predecir las salidas del conjunto de pruebas y a ponerlas frente a frente con las reales, para poder compararlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a23b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ŷ_test = model(x_test)\n",
    "    test_loss = criterion(ŷ_test, y_test)\n",
    "    rmse_test = torch.sqrt(test_loss)\n",
    "print(\"Test MSE loss:\", test_loss.item())\n",
    "print(\"Test RMSE:\", rmse_test.item())\n",
    "\n",
    "results = torch.cat((y_test, ŷ_test), dim=1).numpy()\n",
    "print(\"Ground truth vs predicción (first 10 rows):\")\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29a8dc",
   "metadata": {},
   "source": [
    "A simple vista parece que el modelo no predice demasiado bien. Para ver cómo de bien, sacar conclusiones o, al menos, poder comparar entre modelos, existen diferentes métricas. En adelante nos conviene trabajar con las respuestas como array, no como tensores, así que extraeremos las respuestas a arrays de numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e74ae3-3ea6-4ee6-932e-3baac5fea5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.cpu().detach().numpy()\n",
    "ŷ_test = ŷ_test.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc02c88",
   "metadata": {},
   "source": [
    "### Error absoluto medio (MAE, del inglés _mean absolute error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfe8d7",
   "metadata": {},
   "source": [
    "Mide la media de las diferencias absolutas entre los valores previstos y los reales. La fórmula para calcular el MAE es\n",
    "\n",
    "$$\n",
    "MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "$$\n",
    "\n",
    "siendo:\n",
    "\n",
    "- $n$ el número de observaciones del conjunto de pruebas,\n",
    "- $y_i$ el valor real de la $i$-ésima observación en el conjunto de prueba, y\n",
    "- $\\hat{y}_i$ el valor previsto de la observación $i$-ésima del conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684b19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y, ŷ):\n",
    "    return np.abs(y - ŷ).mean()\n",
    "\n",
    "print(f'MAE = {mae(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f059797",
   "metadata": {},
   "source": [
    "El MAE es una métrica útil porque es interpretable y fácil de entender. Un valor MAE más bajo indica un mejor ajuste entre los valores previstos y los reales, y un valor 0 indica un ajuste perfecto.\n",
    "\n",
    "También es menos sensible a los valores atípicos que otras métricas como el MSE, que eleva al cuadrado las diferencias entre los valores predichos y los reales, y puede verse muy influido por los valores extremos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de55e2e",
   "metadata": {},
   "source": [
    "### Error cuadrático medio (MSE, del inglés _mean squared error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2145b4",
   "metadata": {},
   "source": [
    "Mide la media de las diferencias al cuadrado entre los valores previstos y los reales. La fórmula para calcular el MSE es\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562e58e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y, ŷ):\n",
    "    return ((y - ŷ) ** 2).mean()\n",
    "\n",
    "print(f'MSE = {mse(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15633878",
   "metadata": {},
   "source": [
    "El MSE es una métrica útil porque penaliza más los errores grandes que los pequeños, debido a que eleva al cuadrado las diferencias entre los valores predichos y los reales.\n",
    "\n",
    "Sin embargo, como implica elevar los errores al cuadrado, puede estar muy influenciado por valores atípicos y no ser tan interpretable como otras métricas como MAE. Un valor de MSE más bajo indica un mejor ajuste entre los valores predichos y los reales, y un valor de 0 indica un ajuste perfecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa0c84",
   "metadata": {},
   "source": [
    "### Raíz del error cuadrático medio (RMSE, del inglés _root mean squared error_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfff6ca7",
   "metadata": {},
   "source": [
    "Mide la raíz cuadrada de la media de las diferencias al cuadrado entre los valores previstos y los reales. La fórmula para calcular el RMSE es:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ade9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y, ŷ):\n",
    "    return mse(y, ŷ) ** .5\n",
    "\n",
    "print(f'RMSE = {rmse(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071fac96",
   "metadata": {},
   "source": [
    "El RMSE es una métrica útil porque tiene las mismas unidades que la variable que se predice, lo que facilita su interpretación.\n",
    "\n",
    "También es menos sensible a los valores atípicos que el MSE, porque toma la raíz cuadrada de la suma de los errores al cuadrado, lo que ayuda a \"deshacer\" el efecto de cuadratura en los errores grandes. Un valor de RMSE más bajo indica un mejor ajuste entre los valores predichos y los reales, y un valor de 0 indica un ajuste perfecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820d045",
   "metadata": {},
   "source": [
    "### Coeficiente de determinación ($R^2$))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b6101d",
   "metadata": {},
   "source": [
    "Mide qué porcentaje de la varianza de la variable dependiente ($y$, $\\hat{y}$ e $\\bar{y}$) explica el modelo de regresión que hemos ajustado.\n",
    "\n",
    "Esto es, tenemos unos datos, y queremos ver cómo se relacionan con la variable que intentamos predecir. $R^2$ nos dice cuánto de esa variavión en los datos se logra explicar gracias a las variables que hemos incluido en nuestro modelo.\n",
    "\n",
    "La fórmula para calcular $R^2$ es:\n",
    "\n",
    "$$R^2(y, \\hat{y}) = 1 - \\frac{\\sum_1^{n} (y_i - \\hat{y}_i)^2}{\\sum_1^{n}(y_i - \\bar{y})^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e8043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y, ŷ):\n",
    "    return 1 - ((y - ŷ) ** 2).sum() / ((y - y.mean()) ** 2).sum()\n",
    "\n",
    "print(f'R² = {r2(y_test, ŷ_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8e3207",
   "metadata": {},
   "source": [
    "El resultado de $R^2$ se interpreta de la siguiente manera:\n",
    "\n",
    "- $R^2 = 1$: El modelo **predice perfectamente los valores**, es decir, explica el 100% de la variación.\n",
    "- $R^2 = 0$: El modelo **no explica nada**, y sería tan útil como predecir siempre la media de la variable.\n",
    "- $R^2 < 0$: Aunque es raro, puede ocurrir cuando el modelo **es peor** que usar simplemente la media de los datos para predecir.\n",
    "\n",
    "Es una métrica muy útil porque es fácil de interpretar y puede utilizarse para comparar el rendimiento de distintos modelos. Sin embargo, tiene algunas limitaciones, como no poder distinguir entre un buen ajuste y un ajuste excesivo, y verse afectado por el número de variables independientes del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37fa74e",
   "metadata": {},
   "source": [
    "## Problemas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec0b7",
   "metadata": {},
   "source": [
    "Como decíamos en la introducción, clasificar significa predecir etiquetas discretas. Esto nos lleva a considerar los diferentes tipos de problemas de clasificación que podemos encontrar:\n",
    "\n",
    "- **Clasificación binaria**: Las etiquetas son dos y sólo dos. Por ejemplo, si queremos predecir si un paciente tiene o no una enfermedad.\n",
    "- **Clasificación multiclase**: Las etiquetas son más de dos. Por ejemplo, si queremos predecir el tipo de tumor a partir de una imagen.\n",
    "- **Clasificación multietiqueta**: Las etiquetas son más de dos y puede haber más de una etiqueta correcta. Por ejemplo, si queremos predecir el tipo de tumor a partir de una imagen y también queremos saber si el tumor es maligno o benigno.\n",
    "\n",
    "Diferentes tipos de problemas requieren diferentes formas de abordar cómo se está produciendo el error. En esta sección veremos cómo abordar cada uno de estos problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cccd65c",
   "metadata": {},
   "source": [
    "### Clasificación binaria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926e3553",
   "metadata": {},
   "source": [
    "En este caso intentaremos resolver un problema de clasificación binaria con un modelo sencillo para ilustrar el problema. Para ello aprovecharemos la función `make_classification` de la librería `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4a3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2\n",
    ")\n",
    "plt.scatter(X[:,0], X[:,1], marker=\"+\", c=Y, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e20b8a",
   "metadata": {},
   "source": [
    "Ahora dividiremos el conjunto en el conjunto de entrenamiento, con el 90% de los puntos, y el conjunto de prueba, con el 10% restante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b337e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = X[:-1000,:], Y[:-1000]\n",
    "x_test, y_test = X[-1000:,:], Y[-1000:]\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], marker=\"+\", c=y_train, cmap='bwr');\n",
    "plt.title('Training set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], marker=\"+\", c=y_test, cmap='bwr');\n",
    "plt.title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb832e4",
   "metadata": {},
   "source": [
    "#### El modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f95be9",
   "metadata": {},
   "source": [
    "Ahora crearemos un modelo capaz de clasificar valores procedentes de esta distribución. La arquitectura es indiferente, pero lo importante es que la salida es una única neurona con función de activación sigmoide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b053640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.LazyLinear(out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b11c18",
   "metadata": {},
   "source": [
    "Ahora entrenemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13322dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "n_val = int(len(x_train) * val_split)\n",
    "indices = torch.randperm(len(x_train))\n",
    "val_indices = indices[:n_val]\n",
    "train_indices = indices[n_val:]\n",
    "\n",
    "x_train_final = x_train[train_indices]\n",
    "y_train_final = y_train[train_indices]\n",
    "x_val = x_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "\n",
    "epochs = 1000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train_final)\n",
    "    loss = criterion(y_pred, y_train_final)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(x_val)\n",
    "        val_loss = criterion(y_val_pred, y_val)\n",
    "    val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f959c37",
   "metadata": {},
   "source": [
    "Veamos cómo ha evolucionado el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc9af96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fcb1c2",
   "metadata": {},
   "source": [
    "Recorramos algunas de las diferentes métricas que existen para evaluar un modelo de clasificación. Para ello, extraeremos las predicciones de nuestro modelo sobre el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c7ca36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    ŷ_test = (model(x_test) >= 0.5).float()\n",
    "    test_loss = criterion(ŷ_test, y_test)\n",
    "print(\"Test binary cross entropy loss:\", test_loss.item())\n",
    "\n",
    "results = torch.cat((y_test, ŷ_test), dim=1).numpy()\n",
    "print(\"Ground truth vs predicción (first 10 rows):\")\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a87fd7-f9b5-4426-83d0-97268e8565da",
   "metadata": {},
   "source": [
    "En adelante nos conviene trabajar con las respuestas como array y no como tensores, así que extraeremos las respuestas a arrays de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1ef112-5172-4130-93ad-0fbcd794cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.cpu().detach().numpy()\n",
    "ŷ_test = ŷ_test.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e8b498",
   "metadata": {},
   "source": [
    "#### Matriz de confusión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d8992f",
   "metadata": {},
   "source": [
    "No es una métrica como tal, sino una tabla que se utiliza en problemas de clasificación para evaluar dónde se han cometido errores en el modelo. Se utiliza para problemas de clasificación en los que la salida puede ser de dos o más tipos de clases, aunque aquí la explicaremos para problemas de clasificación binaria (de dos clases).\n",
    "\n",
    "La idea es que las filas representan las clases reales que deberían haber sido los resultados, mientras que las columnas representan las predicciones que hemos hecho. Utilizando esta tabla es fácil identificar qué predicciones son erróneas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ffc694",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
    "    y_test, ŷ_test\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7362f3",
   "metadata": {},
   "source": [
    "Prácticamente todas las métricas de rendimiento en problemas de clasificación se basan en los valores de esta matriz. Antes de explicar estas métricas, conviene conocer los nombres por los que se conocen los valores de acierto y error:\n",
    "\n",
    "1. **Verdaderos positivos** ($TP$, del inglés _true positives_): Aquellos casos en los que la clase real y la predicha por el modelo son verdaderas. Por ejemplo, el caso en el que una persona tiene cáncer (`true`) y el modelo clasifica su caso como cáncer (`true`).\n",
    "2. **Verdaderos negativos** ($TN$, del inglés _true negatives_): Aquellos casos en los que la clase real y la clase predicha por el modelo son falsas. Por ejemplo, el caso en que una persona **no** tiene cáncer (`falso`) y el modelo clasifica su caso como **no** tiene cáncer (`falso`).\n",
    "3. **Falsos positivos** ($FP$, del inglés _false positives_): Aquellos casos en los que la clase real es falsa, pero el modelo la predice como verdadera. Por ejemplo, una persona que **no** tiene cáncer, pero cuyo caso el modelo predice que tiene cáncer.\n",
    "4. **Falsos negativos** ($FN$, _false negatives_): Aquellos casos en los que la clase real es verdadera, pero la predicha por el modelo es falsa. Por ejemplo, el caso de una persona que sí tiene cáncer, pero para la que el modelo predice que no lo tiene.\n",
    "\n",
    "Por supuesto, buscamos el escenario en el que haya $0$ falsos positivos y $0$ falsos negativos, pero en la vida real no es así, ya que prácticamente ningún modelo será preciso al $100\\%$. Por lo tanto, siempre habrá algún error asociado a cada modelo que utilicemos para predecir la clase real de la variable objetivo. Esto dará lugar a falsos positivos y falsos negativos, que también estarán relacionados: cuando unos disminuyan otros aumentarán y viceversa.\n",
    "\n",
    "Así que cuáles son preferibles depende del problema:\n",
    "\n",
    "1. **Minimizar los falsos negativos**: Suele ser aconsejable en casos en los que pasar por alto un caso positivo es un gran error. En el caso de la detección previa de un cáncer, es preferible cometer el error de un falso positivo (al paciente se le diagnostica un cáncer cuando no lo tiene) que un falso negativo (no se le diagnostica cuando sí lo tiene), porque en este último caso no se realizaría ningún examen más.\n",
    "2. **Minimizar los falsos positivos**: Suele ser preferible en el caso contrario. Por ejemplo, en un caso de detección de spam, suele ser preferible que un correo basura no sea detectado como tal; si en este caso minimizamos los falsos negativos, los falsos positivos aumentarían y por tanto nuestro sistema podría eliminar correos auténticos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9f294",
   "metadata": {},
   "source": [
    "#### Exactitud (_accuracy_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd83486b",
   "metadata": {},
   "source": [
    "Mide la proporción de instancias clasificadas correctamente sobre el número total de instancias del conjunto de datos. La fórmula para calcular la exactitud es:\n",
    "\n",
    "$$\n",
    "Acc = \\frac{TP + TN}{TP + FP + TN + FN}\n",
    "$$\n",
    "\n",
    "Es decir, el número de casos clasificados correctamente ($TP$ y $TN$) dividido por el número total de casos ($TP$, $TN$, $FP$ y $FN$). Responde a la pregunta: **¿Con qué frecuencia acierta el modelo?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc4f302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, ŷ):\n",
    "    return np.sum(y == ŷ) / len(y)\n",
    "\n",
    "print(f'Accuracy = {accuracy(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696c3f97",
   "metadata": {},
   "source": [
    "La exactitud es una métrica sencilla e intuitiva, fácil de entender e interpretar. Sin embargo, puede que no sea la mejor métrica en todos los casos, sobre todo cuando las clases del conjunto de datos están desequilibradas.\n",
    "\n",
    "Por ejemplo, si estamos evaluando si se va a producir una fusión del núcleo de una central nuclear, un modelo que aprenda a decir siempre que no puede acertar el $99,9999999\\%$ de las veces; desde el punto de vista de la precisión, el modelo está bien, pero siendo objetivo no vale para nada. En estos casos, otras métricas como la precisión, el recuerdo o la puntuación F1 pueden ser más apropiadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c4e94",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35606491",
   "metadata": {},
   "source": [
    "Mide la proporción de predicciones positivas verdaderas (es decir, el número de instancias clasificadas correctamente como positivas) de todas las predicciones positivas realizadas por el modelo (es decir, la suma de predicciones positivas verdaderas y falsas positivas). La fórmula para calcular la precisión es:\n",
    "\n",
    "$$\n",
    "Pre = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "Responde a la pregunta: **De los positivos predichos, ¿qué porcentaje es realmente positivo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a483ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, ŷ):\n",
    "    TP = np.sum((ŷ_test == 1) & (y_test == 1))\n",
    "    FP = np.sum((ŷ_test == 1) & (y_test == 0))\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "print(f'Precision = {precision(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b83482f",
   "metadata": {},
   "source": [
    "Es una métrica útil cuando queremos minimizar los falsos positivos, es decir, cuando queremos asegurarnos de que las instancias que clasificamos como positivas lo son realmente. Por ejemplo, supongamos que tenemos un modelo de clasificación binario que predice si un determinado correo electrónico es spam o no. Si el modelo predice que $100$ correos electrónicos son spam y $80$ de esas predicciones son correctas, mientras que $20$ son incorrectas, la precisión es de $0,8$, lo que significa que de todos los correos electrónicos predichos como spam por el modelo, el $80\\%$ eran realmente spam.\n",
    "\n",
    "Sin embargo, la precisión por sí sola puede no ser suficiente para evaluar el rendimiento de un modelo, y debe utilizarse junto con otras métricas como _recall_ o la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1608783",
   "metadata": {},
   "source": [
    "#### Recuperación (_recall_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7644f1",
   "metadata": {},
   "source": [
    "También conocida como _sensitivity_, es una métrica que mide la proporción de predicciones positivas verdaderas (es decir, el número de instancias clasificadas correctamente como positivas) de todas las instancias positivas reales del conjunto de datos. La fórmula para calcular el recuerdo es\n",
    "\n",
    "$$\n",
    "Rec = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "Responde a la pregunta: **De todos los casos positivos, ¿qué porcentaje ha predicho el modelo?**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862c8965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y, ŷ):\n",
    "    TP = np.sum((ŷ_test == 1) & (y_test == 1))\n",
    "    FN = np.sum((ŷ_test == 0) & (y_test == 1))\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "print(f'Recall = {recall(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd11b9c6",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que tenemos un modelo de clasificación binaria que predice si un determinado paciente tiene cáncer. Si en el conjunto de datos hay 100 pacientes que realmente tienen la enfermedad, y el modelo identifica correctamente a 80 de ellos como positivos, mientras que $20$ no son detectados, la recuperación del modelo sería de $0,8$, lo que significa que de todos los pacientes que realmente tienen la enfermedad, el modelo identificó correctamente al $80\\%$ de ellos.\n",
    "\n",
    "Es una métrica útil cuando queremos minimizar los falsos negativos, es decir, cuando queremos asegurarnos de que todos los casos positivos se identifican correctamente como positivos. Sin embargo, el recuerdo por sí solo puede no ser suficiente para evaluar el rendimiento de un modelo, y debe utilizarse junto con otras métricas como la precisión o la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e2d4c8",
   "metadata": {},
   "source": [
    "#### Especificidad (_specificity_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c84cfe",
   "metadata": {},
   "source": [
    "Es lo contrario de la sensibilidad, que mide la proporción de predicciones negativas verdaderas (es decir, el número de ejemplos clasificados correctamente como negativos) de todas las instancias negativas reales del conjunto de datos. La fórmula para calcular la especificidad es:\n",
    "\n",
    "$$\n",
    "Spe = \\frac{TN}{TN + FP}\n",
    "$$\n",
    "\n",
    "Responde a la pregunta: **¿Qué tan bien predice el modelo los resultados negativos?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558898f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def specificity(y, ŷ):\n",
    "    TN = np.sum((ŷ_test == 0) & (y_test == 0))\n",
    "    FP = np.sum((ŷ_test == 1) & (y_test == 0))\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "print(f'Specificity = {specificity(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74583f",
   "metadata": {},
   "source": [
    "Por ejemplo, supongamos que tenemos la misma clasificación binaria que antes. Si en el conjunto de datos hay 100 pacientes que no tienen cáncer, y el modelo identifica correctamente a $70$ de ellos como negativos, mientras que $30$ se clasifican incorrectamente como positivos, la especificidad del modelo sería de $0,7$, lo que significa que de todos los pacientes que no tienen la enfermedad, el modelo identificó correctamente al $70\\%$ de ellos.\n",
    "\n",
    "La especificidad es una métrica útil cuando queremos minimizar los falsos positivos, es decir, cuando queremos asegurarnos de que todas las instancias negativas se identifican correctamente como negativas. Sin embargo, la especificidad por sí sola puede no ser suficiente para evaluar el rendimiento de un modelo, y debe utilizarse junto con otras métricas como la sensibilidad (_recall_) o la puntuación F1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a5b0f",
   "metadata": {},
   "source": [
    "#### _F1 Score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c3d84c",
   "metadata": {},
   "source": [
    "Es la media armónica de la precisión y la recuperación, y proporciona una medida equilibrada del rendimiento del modelo. La fórmula para calcular el _F1 score_ se define como:\n",
    "\n",
    "$$\n",
    "F1 = 2 \\cdot \\frac{Pre \\cdot Rec}{Pre + Rec}\n",
    "$$\n",
    "\n",
    "Su valor oscila entre $0$ y $1$, donde 1 indica una precisión y recuperación perfectas, y 0 un rendimiento deficiente. Un _F1 score_ alta indica que el modelo tiene un buen equilibrio entre precisión y recuperación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb814b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(y, ŷ):\n",
    "    pre = precision(y, ŷ)\n",
    "    rec = recall(y, ŷ)\n",
    "    return 2 * pre * rec / (pre + rec)\n",
    "\n",
    "print(f'F1 Score = {f1_score(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f05897",
   "metadata": {},
   "source": [
    "Por ejemplo, teniendo el mismo modelo de clasificación binaria que antes, si el modelo tiene una precisión de $0,8$ y una recuperación de $0,7$, la puntuación F1 del modelo sería de $0,75$, lo que significa que el modelo tiene un rendimiento equilibrado entre precisión y recuperación, con una puntuación F1 de $0,75$.\n",
    "\n",
    "El _F1 score_ es una métrica útil cuando queremos evaluar el rendimiento general de un modelo, sobre todo cuando la precisión y la recuperación son métricas importantes para el problema en cuestión. Sin embargo, puede no ser adecuado para problemas en los que la precisión o la recuperación son más importantes que la otra. En tales casos, la precisión o la recuperación pueden utilizarse como métrica única para la evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a36bf0",
   "metadata": {},
   "source": [
    "#### Curvas ROC (del inglés _receiver operating characteristic_) y AUC (del inglés _area under the curve_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43466763",
   "metadata": {},
   "source": [
    "**La curva ROC** (del inglés, _Receiver Operating Characteristic_) es una representación gráfica que muestra cómo se comporta un modelo de clasificación a la hora de distinguir entre dos clases. En el eje vertical se sitúa la tasa de verdaderos positivos (sensibilidad) y en el horizontal la tasa de falsos positivos. Con diferentes umbrales, la curva te permite ver el equilibrio entre detectar correctamente los positivos y evitar falsos positivos.\n",
    "\n",
    "El área bajo la curva (AUC) resume este comportamiento en un único número que es, como su propio nombre indica, el área que queda bajo la ROC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20342f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = sklearn.metrics.roc_curve(y_test, ŷ_test)\n",
    "roc_auc = sklearn.metrics.auc(fpr, tpr)\n",
    "\n",
    "plt.fill_between(fpr, tpr, alpha=0.3)\n",
    "plt.plot(fpr, tpr, lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--')  # Reference line\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC for our classification problem')\n",
    "plt.xlabel('False positive rate (1 - Specificity)')\n",
    "plt.ylabel('True positive rate (Recall)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bed7195",
   "metadata": {},
   "source": [
    "Cuando la ROC queda cerca de la **esquina superior izquierda** indica que el modelo consigue una alta tasa de _true positives_ con una baja tasa de _false positives_, lo cual es **deseable**. Sin embargo, cuanto más se acerca a la **diagonal**, más se comporta como una **selección aleatoria**, ya que al aumentar una tasa se incrementa proporcionalmente la otra.\n",
    "\n",
    "El AUC resume el rendimiento general del modelo:\n",
    "\n",
    "- **AUC $ = 1$**: Indica una clasificación perfecta.\n",
    "- **AUC $ = 0.5$**: Equivale a adivinar al azar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c252c94",
   "metadata": {},
   "source": [
    "### Clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d87be",
   "metadata": {},
   "source": [
    "La clasificación multiclase es un problema de aprendizaje automático en el que queremos predecir a qué categoría pertenece una entrada, eligiendo entre varias opciones. A diferencia de la clasificación binaria, donde solo hay dos posibilidades, aquí hay más de dos clases.\n",
    "\n",
    "Por ejemplo, podríamos tener un problema en el que se deba identificar la raza de un gato, eligiendo entre «siamés», «persa» y «común europeo». Otro caso sería predecir el género de una película a partir de opciones como «acción», «comedia», «drama» y «ciencia ficción».\n",
    "\n",
    "En este tipo de clasificación, **cada ejemplo tiene asignada una y solo una etiqueta verdadera**, y el modelo aprende a relacionar las características de la entrada con la etiqueta correcta. Para ello, se pueden utilizar algoritmos como los árboles de decisión, las máquinas de vectores soporte, la regresión logística o las redes neuronales.\n",
    "\n",
    "La evaluación de estos modelos se realiza con métricas como la exactitud, la precisión, la recuperación y la puntuación F1. La exactitud nos indica el porcentaje de predicciones correctas, mientras que las otras métricas ofrecen una visión más detallada del rendimiento del modelo en cada clase.\n",
    "\n",
    "Vamos a intentar resolver un problema similar, pero en el que intervengan más de dos clases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f9519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    ")\n",
    "plt.scatter(X[:,0], X[:,1], marker=\"+\", c=Y, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a105fe08",
   "metadata": {},
   "source": [
    "También como antes, extraeremos un conjunto de entrenamiento y un conjunto de prueba para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf55e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = X[:-1000,:], Y[:-1000]\n",
    "x_test, y_test = X[-1000:,:], Y[-1000:]\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], marker=\"+\", c=y_train, cmap='bwr');\n",
    "plt.title('Training set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], marker=\"+\", c=y_test, cmap='bwr');\n",
    "plt.title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841bb91",
   "metadata": {},
   "source": [
    "#### _Softmax_ y _log-softmax_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ae133b",
   "metadata": {},
   "source": [
    "Antes de crear el modelo, tenemos que echar un vistazo a los datos de salida. En la sección anterior, teníamos dos clases, y una sola neurona de salida era suficiente para discriminar entre una y otra ($0$ para una clase y $1$ para la otra).\n",
    "\n",
    "Ahora, sin embargo, tenemos cuatro salidas en total. ¿Cómo podemos hacer que el modelo sea capaz de clasificar entre todas ellas? La respuesta es que no podemos. Necesitamos una neurona para cada clase, y cada neurona tendrá que tener un valor de salida entre $0$ y $1$. La neurona con el valor más alto será la que se active, y por tanto será la clase que se prediga.\n",
    "\n",
    "En este punto es donde entran en juego las funciones _softmax_ y _log-softmax_. Podríamos poner una capa de activación sigmoide o de cualquier otro tipo, pero estas actúan de manera independiente (una función de activación por neurona. La _softmax_ y derivadas actúan sobre todas las neuronas a la vez, creando una distribución de probabilidades en la que la suma de todos los valores es 1. Esto hace que el entrenamiento sea más eficiente, porque permite que cada salida se interprete como la probabilidad de que la entrada pertenezca a esa clase, y la clase con mayor probabilidad es la que se elige.\n",
    "\n",
    "La función softmax se utiliza en la clasificación multiclase para convertir un vector de valores reales en probabilidades. Esto significa que, para cada ejemplo, la salida de la red se transforma en un conjunto de valores entre 0 y 1 que suman 1, de forma que cada valor representa la probabilidad de pertenecer a una determinada clase. La fórmula de la _softmax_ es:\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e11b8-1bdf-4e09-a95d-b4cf63b104b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "classes = np.arange(len(logits))\n",
    "\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / np.sum(exp_z)\n",
    "\n",
    "softmax_probs = softmax(logits)\n",
    "plt.bar(classes, softmax_probs)\n",
    "plt.title(\"Softmax (probabilitites)\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922fdd7b-4eee-4f25-adb0-7fed1e7290be",
   "metadata": {},
   "source": [
    "Por otro lado, la _log-softmax_ aplica el logaritmo a los valores obtenidos con softmax, es decir, devuelve _log-probabilidades_. Esto resulta **especialmente útil para evitar problemas numéricos cuando se trabajan con valores muy pequeños**, y es la función que esperan algunas pérdidas, como la `NLLLoss` en _PyTorch_. La fórmula de la _log-softmax_ es:\n",
    "\n",
    "$$\n",
    "\\operatorname{log-softmax}(z)_j = z_j - \\log\\left(\\sum_{k=1}^{K} e^{z_k}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9da70d-0770-460c-b274-148cfceb03c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([1.0, 2.0, 3.0, 4.0])\n",
    "classes = np.arange(len(logits))\n",
    "\n",
    "def log_softmax(z):\n",
    "    return z - np.log(np.sum(np.exp(z)))\n",
    "\n",
    "log_softmax_values = log_softmax(logits)\n",
    "log_softmax_probs = np.exp(log_softmax_values)\n",
    "\n",
    "plt.bar(classes, log_softmax_probs)\n",
    "plt.title(\"Log-Softmax (Converted to probabilities)\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671259b-77c7-42fb-be88-eb9cd5e03f34",
   "metadata": {},
   "source": [
    "Sí salen los mismo resultados. Esto es porque estamos convirtiendo las salidas de _log-softmax_ a probabilidades (aplicando la exponencial), lo que hace que se obtenga exactamente el mismo vector de probabilidadesque el producido por _softmax_.\n",
    "\n",
    "Usar _softmax_ tiene la ventaja de que genera directamente probabilidades, por lo que es muy sencillo interpretar qué quiere decir una salida concreta. El problema viene cuando los valores de entrada son muy altos o muy bajos, porque pueden provocar un desbordamiento debido a la precisión. La función _log-softmax_ por otro lado es mucho más estable numéricamente, aunque no nos da directamente una distribución de probabilidades (aunque ya hemos visto que se soluciona rápidamente aplicando la exponencial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a9a76-3e69-42ab-89d3-2f656f02be5c",
   "metadata": {},
   "source": [
    "#### El modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a0a23-1334-4558-9e63-4a60cb8dc0a3",
   "metadata": {},
   "source": [
    "Vamos a crear el modelo de clasificación. Usaremos como capa de activación una _log-softmax_, lo que nos predispone a usar la función de _loss_ que se muestra en el ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89388001",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.LazyLinear(out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=4),\n",
    "    nn.LogSoftmax(dim=1)  # LogSoftmax para obtener probabilidades logarítmicas\n",
    ")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.NLLLoss() # \"Pérdida de log-verosimilitud negativa\"\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c111ee00",
   "metadata": {},
   "source": [
    "Si recordamos en un ejemplo anterior, teníamos `CrossEntropyLoss` como función de _loss_, e indicamos que no hacía falta especificar la capa final puesto que la propia función de pérdida realizaba la activación _softmax_. Pues bien, en realidad internamente implementa `LogSoftmax` + `NLLLoss`, así que no haría falta especificar nada más que el loss. Concretamente nuestro modelo quedaría como sigue:\n",
    "\n",
    "```python\n",
    "model = nn.Sequential(\n",
    "# ...\n",
    "    nn.LazyLinear(out_features=4),\n",
    ")\n",
    "#...\n",
    "criterion = nn.CrossEntropyLoss() # Pérdida de entropía cruzada\n",
    "#...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dca6e1-8599-4806-8602-42372cf8f6e0",
   "metadata": {},
   "source": [
    "Ahora entrenemos el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d90718-3795-40e4-88f5-449a2474bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "n_val = int(len(x_train) * val_split)\n",
    "indices = torch.randperm(len(x_train))\n",
    "val_indices = indices[:n_val]\n",
    "train_indices = indices[n_val:]\n",
    "\n",
    "x_train_final = x_train[train_indices]\n",
    "y_train_final = y_train[train_indices]\n",
    "x_val = x_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "\n",
    "epochs = 5000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train_final)\n",
    "    loss = criterion(y_pred, y_train_final)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(x_val)\n",
    "        val_loss = criterion(y_val_pred, y_val)\n",
    "    val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58776f96-9d1f-4612-8b2a-756c3af69ce7",
   "metadata": {},
   "source": [
    "Veamos cómo ha evolucionado el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9b7029-fb9f-4d65-aeec-bb37f5f1e950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d716db98-f9e1-4dfd-8dee-b8550db1b913",
   "metadata": {},
   "source": [
    "Recorramos algunas de las diferentes métricas que existen para evaluar un modelo de clasificación. Para ello, extraeremos las predicciones de nuestro modelo sobre el conjunto de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39785e9-83cd-4e36-bd79-26ff498f0555",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_logits = model(x_test)\n",
    "    test_loss = criterion(y_logits, y_test)\n",
    "    ŷ_test = torch.argmax(y_logits, dim=1)\n",
    "print(\"Test negative log-likelihood loss:\", test_loss.item())\n",
    "\n",
    "results = torch.cat((y_test.view(-1, 1), ŷ_test.view(-1, 1)), dim=1).numpy()\n",
    "print(\"Ground truth vs predicción (first 10 rows):\")\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37cf04c-b2bc-45ca-af10-e2f25b0944e2",
   "metadata": {},
   "source": [
    "En adelante nos conviene trabajar con las respuestas como array y no como tensores, así que extraeremos las respuestas a arrays de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba64a6b-6769-4c3f-9ca4-ef3e438506f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.numpy()\n",
    "ŷ_test = ŷ_test.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51135694",
   "metadata": {},
   "source": [
    "Y ahora sí, hablemos un poco de las métricas de evaluación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be6c75",
   "metadata": {},
   "source": [
    "#### Métricas de evaluación para problemas de clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1d1bf",
   "metadata": {},
   "source": [
    "En realidad, las métricas utilizadas en este tipo de problemas son las mismas que hemos visto para la clasificación binaria. Precisión, sensibilidad, exactitud, etc., son medidas que evalúan el acierto frente al error.\n",
    "\n",
    "Sin embargo, merece la pena revisar la matriz de confusión para los casos en los que tenemos más de una clase. Vamos a ello."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25e4e13",
   "metadata": {},
   "source": [
    "##### Comprender una matriz de confusión multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b1c63f",
   "metadata": {},
   "source": [
    "Mientras que una matriz de confusión de $2$ por $2$ es intuitiva y fácil de entender, las matrices de confusión más grandes pueden llegar a ser bastantes confusas.\n",
    "\n",
    "Explorémoslas suponiendo que, en el ejemplo anterior, las cuatro clases corresponden a cuatro tipos distintos de pociones de vida resultantes de mezclar dos hierbas en distintas proporciones: excelente ($3$), grande ($2$), pequeña ($1$) y diminuta ($0$). La evaluación de cualquier clasificador sobre estos datos de diamante producirá una matriz de $4$ por $4$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b0c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.ConfusionMatrixDisplay.from_predictions(y_test, ŷ_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19cceb",
   "metadata": {},
   "source": [
    "Aunque la interpretación de la matriz de confusión se vuelve más y más complicada a medida que aumenta el número de clases, existe un método para simplificar conceptualmente el entendimieto de cualquier tipo de matriz, independientemente de su tamaño.\n",
    "\n",
    "Lo primero que hay que hacer es identificar qué clases son positivas y cuáles son negativas. Esto depende de la tarea a resolver, y no siempre es obvio. Si la clasificación es equilibrada, es decir, si cada clase es igual de importante, puede que no haya clases positivas ni negativas, pero este no suele ser el caso, y siempre suele haber clases que son más o menos interesantes para nuestra tarea.\n",
    "\n",
    "En nuestro ejemplo, como aventureros acérrimos que somos, nos interesa que la red neuronal implementada en nuestra varita de identificación de pociones clasifique mejor las pociones excelente y grandes que las pequeñas y diminutas, principalmente porque el tamaño de la mochila es limitado y son las que nos mantendrán vivo más tiempo. En este caso, las etiquetas excelente y grande constituirán una clase positiva, y las demás etiquetas se considerarán negativas.\n",
    "\n",
    "Después de eso, podemos pasar a definir los términos de la matriz de confusión:\n",
    "\n",
    "1. **Verdaderos positivos**: Aquellos casos donde la poción real y la predicha son _excelente_ (tipo 1) o _grande_ (tipo 2),\n",
    "2. **Verdaderos negativos**: Aquellos casos donde cualquier etiqueta considerada negativa ha sido correctamente predicha,\n",
    "3. **Falsos positivos**: La poción es _pequeña_ o _minúscula_, pero ha sido predicha como _grande_ o _excelente_, y\n",
    "4. **Falsos negativos**: La poción es _grande_ o _excelente_, pero ha sido predicha como _pequeña_ o _diminuta_.\n",
    "\n",
    "Habiendo definido los 4 términos, encontrar cada uno de ellos en la matriz debería ser fácil. Y a partir de ahí, se pueden definir las medidas que hemos discutido previamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e82cf8c",
   "metadata": {},
   "source": [
    "##### Una nota sobre diferentes estrategias de clasificación multiclase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc655fad",
   "metadata": {},
   "source": [
    "Aquí hemos resuelto el problema de clasificación multiclase de una manera muy específica: estableciendo una correspondencia entre cada una de las clases con cada una de las neuronas de salida, denotando estas la probabilidad de que un ejemplo pertenezca o no a la clase que representan. Esta estrategia es un tipo dentro de lo que se conoce como _transformación binaria_ de la salida y vamos a hablar un poco sobre ello.\n",
    "\n",
    "Hay algoritmos que están específicamente diseñados para resolver problemas de clasificación binaria, como las redes neuronales. Cuando tenemos una única clase y queremos clasificar si un ejemplo pertenece o no a una clase determinada, basta con una neurona. Sin embargo, si tenemos más clases, existen principalmente Una neurona si tenemos dos clases y varias neuronas si tenemos más. La estrategia de transformación binaria se subdivide en dos, dependiendo del número de neuronas que usamos para la salida.\n",
    "\n",
    "###### Estrategia uno contra todos (OVR, del inglés _one-vs-rest_)\n",
    "\n",
    "También conociada como uno contra todos (OVA, del inglés _one-vs-all_). En este caso, tenemos **una neurona para cada clase**, y cada neurona indica la probabilidad de que un ejemplo pertenezca a esa clase. Por ejemplo, si en un problema de clasificación de imágenes tenemos las clases `Perro`, `Gato`, `Caballo` y `Vaca`, tendremos cuatro neuronas de salida:\n",
    "\n",
    "- Neurona 1: `Perro` vs. (`Gato`, `Caballo`, `Vaca`),\n",
    "- Neurona 2: `Gato` vs. (`Perro`, `Caballo`, `Vaca`),\n",
    "- Neurona 3: `Caballo` vs. (`Perro`, `Gato`, `Vaca`), y\n",
    "- Neurona 4: `Vaca` vs. (`Perro`, `Gato`, `Caballo`).\n",
    "\n",
    "Esta es la estrategia que hemos utilizado y se llama así porque una clase vence al resto en cada clasificación. Suele ser la más extendida.\n",
    "\n",
    "###### Estrategia uno contra uno (OVO, del inglés _one-vs-one_)\n",
    "\n",
    "En este caso, tenemos **una neurona para cada par de clases**. En este caso, tendremos $\\frac{N(N-1)}{2}$ neuronas de salida, y cada neurona indica la probabilidad de que un ejemplo pertenezca a una clase frente a otra. Con esta estrategia tendremos un total de 6 neuronas para el mismo ejemplo de antes.\n",
    "\n",
    "In this case, we have one neuron for each pair of classes, i.e. $\\frac{N(N-1)}{2}$, and each neuron indicates the probability that an example belongs to one class versus the other. With this strategy we get a total of 6 neurons for the same example.\n",
    "\n",
    "- Neurona 1: `Perro` vs. `Gato`\n",
    "- Neurona 2: `Perro` vs. `Caballo`\n",
    "- Neurona 3: `Perro` vs. `Vaca`\n",
    "- Neurona 4: `Gato` vs. `Caballo`\n",
    "- Neurona 5: `Gato` vs. `Vaca`\n",
    "- Neurona 6: `Caballo` vs. `Vaca`\n",
    "\n",
    "Esta estrategia se usa a menudo en [SVM](https://es.wikipedia.org/wiki/M%C3%A1quinas_de_vectores_de_soporte), otra técnica de clasificación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a07053",
   "metadata": {},
   "source": [
    "### Clasificación multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8ea81",
   "metadata": {},
   "source": [
    "En la clasificación multietiqueta, el objetivo es predecir múltiples etiquetas binarias o atributos para una instancia de entrada dada. A diferencia de la clasificación multiclase, donde una instancia se asigna a una sola clase, **la clasificación multietiqueta asigna una instancia a una o más clases**.\n",
    "\n",
    "Por ejemplo, en un problema de clasificación multietiqueta de reconocimiento de imágenes, una imagen puede tener múltiples objetos o atributos que pueden ser reconocidos, como `gato`, `perro`, `interior`, `exterior`, `diurno` y `nocturno`. Así, una imagen puede ser asignada múltiples etiquetas al mismo tiempo.\n",
    "\n",
    "En la clasificación multietiqueta, cada ejemplo puede estar asociado con múltiples etiquetas binarias, y el modelo tiene como objetivo aprender un mapeo entre las características de entrada y el conjunto correcto de etiquetas. Hay varios algoritmos que se pueden utilizar para la clasificación multietiqueta, como $k$-vecinos más cercanos, árboles de decisión, _random forest_ y redes neuronales.\n",
    "\n",
    "El problema a resolver será similar al anterior sólo que cada ejemplo podrá pertenecer a varias clases de las existentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f9515",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = sklearn.datasets.make_multilabel_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=2,\n",
    "    n_classes=4,\n",
    "    n_labels=2,\n",
    ")\n",
    "Y_colors = Y[:,0] + 2*Y[:,1] + 4*Y[:,2] + 8*Y[:,3]\n",
    "plt.scatter(X[:,0], X[:,1], marker=\"+\", c=Y_colors, cmap='bwr');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377660a0",
   "metadata": {},
   "source": [
    "Al igual que antes, extraeremos un conjunto de entrenamiento y un conjunto de prueba para evaluar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d6fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = X[:-1000,:], Y[:-1000]\n",
    "y_train_colors = y_train[:,0] + 2*y_train[:,1] + 4*y_train[:,2] + 8*y_train[:,3]\n",
    "x_test, y_test = X[-1000:,:], Y[-1000:]\n",
    "y_test_colors = y_test[:,0] + 2*y_test[:,1] + 4*y_test[:,2] + 8*y_test[:,3]\n",
    "\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)  # Etiquetas multilabel, 4 columnas\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(x_train[:,0], x_train[:,1], marker=\"+\", c=y_train_colors, cmap='bwr');\n",
    "plt.title('Training set')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_test[:,0], x_test[:,1], marker=\"+\", c=y_test_colors, cmap='bwr');\n",
    "plt.title('Test set');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeff87b6",
   "metadata": {},
   "source": [
    "#### Modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b71c1d",
   "metadata": {},
   "source": [
    "Implementar un modelo de clasificación multietiqueta es muy similar a un problema de clasificación multiclase. Sin embargo en este caso, una capa _softmax_ en la salida y una entropía cruzada como _loss__ en la salida única no son válidas porque, por definición, puede haber más de una salida.\n",
    "\n",
    "En este caso **se aplica una entropía cruzada binaria en cada salida**, lo que hace que cada salida sea independiente de las demás. Esto se logra, o bien con una salida sigmoidal y on  objeto de la clase `BCELoss` como función de pérdida, o bien sin capa sigmoidal y un objeto de la clase `BCEWithLogitsLoss` como pérdida (que ya él solito incluye la capa sigmoidal).\n",
    "\n",
    "Sabiendo esto, vamos a crear el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fc1853",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.LazyLinear(out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=16),\n",
    "    nn.ReLU(),\n",
    "    nn.LazyLinear(out_features=4),\n",
    "    nn.Sigmoid()  # Ahora sí, sigmoide porque cada salida es independiente\n",
    ")\n",
    "print(model)\n",
    "\n",
    "criterion = nn.BCELoss()  # Ya hemos puesto una salida sigmoide, así que BCELoss\n",
    "optimizer = torch.optim.SGD(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c06f9b0",
   "metadata": {},
   "source": [
    "Hemos hecho un poco de trampa porque este problema es un poco más complejo y requiere un modelo más potente.\n",
    "\n",
    "Estas cuatro salidas corresponderán a la codificación de nuestras cuatro clases, y cada una de ellas tendrá una salida independiente. En este caso, la salida será un número entre 0 y 1 que indica la probabilidad de que el ejemplo pertenezca a esa clase:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bb954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[:10,:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869c8d71",
   "metadata": {},
   "source": [
    "La codificación es similar a la que hemos hecho antes (_one-hot encoding_), pero en este caso, cada fila puede tener más de un 1. En ocasiones es nombrada _multi-hot encoding_.\n",
    "\n",
    "Ahora sólo nos queda entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c505158",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_split = 0.1\n",
    "n_val = int(len(x_train) * val_split)\n",
    "indices = torch.randperm(len(x_train))\n",
    "val_indices = indices[:n_val]\n",
    "train_indices = indices[n_val:]\n",
    "\n",
    "x_train_final = x_train[train_indices]\n",
    "y_train_final = y_train[train_indices]\n",
    "x_val = x_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "\n",
    "epochs = 5000\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(x_train_final)\n",
    "    loss = criterion(y_pred, y_train_final)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_val_pred = model(x_val)\n",
    "        val_loss = criterion(y_val_pred, y_val)\n",
    "    val_losses.append(val_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f45e941",
   "metadata": {},
   "source": [
    "Vamos a ver cómo ha evolucionado el proceso de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81832dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3148bf26",
   "metadata": {},
   "source": [
    "Por lo visto parece que el modelo ha aprendido algo, aunque la evolución del _loss_ no es para emocionarse. Aún así, por ejemplo, es suficiente para nosotros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273c4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_logits = model(x_test)\n",
    "    test_loss = criterion(y_logits, y_test)\n",
    "    ŷ_test = (y_logits >= 0.5).float()\n",
    "print(\"Test binary cross entropy loss:\", test_loss.item())\n",
    "\n",
    "results = torch.cat((y_test.T, ŷ_test.T), dim=1).numpy()\n",
    "print(\"Ground truth vs predicción (first 10 rows):\")\n",
    "print(results[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6a8c9-cc81-4cda-b62a-bd1dfd3ffcbc",
   "metadata": {},
   "source": [
    "En adelante nos conviene trabajar con las respuestas como array y no como tensores, así que extraeremos las respuestas a arrays de numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd6ff21-e803-4289-a21c-1fa0b6f91336",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.numpy()\n",
    "ŷ_test = ŷ_test.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517e9c9d",
   "metadata": {},
   "source": [
    "#### Métricas de evaluación para problemas de clasificación multietiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb536a",
   "metadata": {},
   "source": [
    "A diferencia de los problemas de clasificación binaria y multiclase, en los que cada instancia está asociada a una única etiqueta de clase, en la clasificación multietiqueta, cada instancia puede estar asociada a múltiples etiquetas binarias. Por lo tanto, las métricas de evaluación utilizadas en la clasificación multietiqueta son diferentes de las utilizadas en los problemas de clasificación binaria y multiclase.\n",
    "\n",
    "Generalmente, esto se hace utilizando métricas como la precisión, la exactitud, el _recall_ y el F1 _score_, que pueden calcularse para cada etiqueta por separado o para el conjunto total de etiquetas. Sin embargo, hay otras métricas que pueden darnos algunas indicaciones de cómo los modelos están realizando su tarea.\n",
    "\n",
    "Estas métricas pueden calcularse por separado para cada etiqueta o para el conjunto total de etiquetas. Dependiendo del problema específico y del dominio, diferentes métricas pueden ser más apropiadas. Por lo tanto, es importante elegir cuidadosamente las métricas de clasificación multietiqueta apropiadas para evaluar el rendimiento de un modelo dado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eec65e",
   "metadata": {},
   "source": [
    "##### Macropromedio\n",
    "\n",
    "Consiste en calcular una métrica (por ejemplo, precisión, _recall_ o F1 _score_) para cada etiqueta por separado, y luego tomar la media de todas las métricas. Esta métrica da el mismo peso a cada etiqueta, independientemente de su frecuencia en el conjunto de datos.\n",
    "\n",
    "$$\n",
    "macro(f, y, \\hat{y}) = \\sum_{l \\in L} f(y, \\hat{y}, l)\n",
    "$$\n",
    "\n",
    "Siendo $L$ el conjunto de etiquetas o clases y $l$ la métrica. Por ejemplo, dado que queremos calcular el promedio macro para la precisión, tenemos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4871ec85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y, ŷ, label=1):\n",
    "    TP = np.sum((ŷ_test == label) & (y_test == label))\n",
    "    FP = np.sum((ŷ_test == label) & (y_test != label))\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def macro(metric, y, ŷ, labels=None):\n",
    "    labels = labels or np.unique(y)\n",
    "    return np.mean([metric(y, ŷ, label) for label in labels])\n",
    "\n",
    "print(f'Macro precision = {macro(precision, y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dcc3e7f",
   "metadata": {},
   "source": [
    "Especial atención a que hemos reescrito la función de precisión para que ahora tenga en cuenta la etiqueta. Dado que estamos tratando con un problema de más de una clase, es necesario indicar sobre qué etiqueta queremos trabajar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f14b4e",
   "metadata": {},
   "source": [
    "##### Micropromedio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d0ceba",
   "metadata": {},
   "source": [
    "En esta medida se calcula la métrica considerando todos los ejemplos y etiquetas juntas. Esta métrica da el mismo peso a cada ejemplo y a cada etiqueta, independientemente de su frecuencia en el conjunto de datos.\n",
    "\n",
    "Por supuesto depende de la métrica a calcular, por lo que por ejemplo, para la precisión, tendríamos lo siguiente:\n",
    "\n",
    "$$\n",
    "\\mu Pre = \\frac{\\sum_{l in L}TP_l}{\\sum_{l in L}TP_l + \\sum_{l in L}FP_l}\n",
    "$$\n",
    "\n",
    "Lo cual se puede implementar de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425e262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def micro_precision(y, ŷ, labels=None):\n",
    "    labels = labels or np.unique(y)\n",
    "    TP = np.sum([(ŷ_test == label) & (y_test == label) for label in labels])\n",
    "    FP = np.sum([(ŷ_test == label) & (y_test != label) for label in labels])\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "print(f'Micro precision = {micro_precision(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd003bf",
   "metadata": {},
   "source": [
    "##### Ratio de coincidencia exacta (EMR del inglés _exact match ratio_) y 1/0 _loss_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac0f721",
   "metadata": {},
   "source": [
    "Esta métrica es bastante simple y es una extensión del concepto de precisión. Se define como:\n",
    "\n",
    "$$\n",
    "EMR(y, \\hat{y}) = \\frac{1}{n} \\sum_{i = 0}^n I(y_i = \\hat{y_i})\n",
    "$$\n",
    "\n",
    "Donde $I$ es una función (denominada «indicador») que devuelve $1$ o $0$ si el argumento es `true` o `false` respectivamente, $n$ es el número de ejemplos en el conjunto de entrenamiento, $y$ es el vector de etiquetas verdaderas e $\\hat{y}$ es el vector de etiquetas predichas.\n",
    "\n",
    "En otras palabras, el ratio de coincidencia exacta es la proporción de ejemplos en los que todas las etiquetas verdaderas coinciden con todas las etiquetas predichas. Por ejemplo, si tenemos un conjunto de datos con $100$ ejemplos, y en $80$ de ellos todas las etiquetas verdaderas coinciden con todas las etiquetas predichas, el ratio de coincidencia exacta sería de $0,8$.\n",
    "\n",
    "Una posible implementación de esta métrica sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b38653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emr(y, ŷ):\n",
    "    return np.sum(y == ŷ) / len(y)\n",
    "\n",
    "print(f'EMR = {emr(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41c375f",
   "metadata": {},
   "source": [
    "El ratio de coincidencia exacta es una métrica útil para evaluar el rendimiento de un modelo de clasificación multietiqueta, ya que proporciona una medida de cuántos ejemplos se clasifican correctamente en términos de todas las etiquetas.\n",
    "\n",
    "El principal inconveniente de esta medida es que no tiene en cuenta las etiquetas parcialmente correctas. Es decir, o es 100% correcta o no es correcta en absoluto. Por ejemplo, si estamos clasificando imágenes de perros y gatos, y el sistema predice que hay un perro y un gato en la imagen, pero en realidad hay un perro y un caballo, la medida EMR dará un 0.\n",
    "\n",
    "Por ello, a partir de esta métrica se define el _loss_ 1/0, que se define como:\n",
    "\n",
    "$$\n",
    "L_{01}(y, \\hat{y}) = 1 - EMR(y, \\hat{y})\n",
    "$$\n",
    "\n",
    "Dicho de otro modo, el _loss_ 1/0 es simplemente el complemento de la medida EMR. Es una medida útil para evaluar el rendimiento de un modelo de clasificación multietiqueta, ya que proporciona una medida de cuántos ejemplos se clasifican incorrectamente en términos de todas las etiquetas.\n",
    "\n",
    "Sin embargo, esta función de _loss_ tiene el problema de que, como no es diferenciable, no se puede utilizar en la mayoría de los algoritmos de aprendizaje automático. Pero la ponemos aquí para conocer de su existencia, no vaya a ser que nos la encontremos en algún sitio y no sepamos qué significa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0f7bb",
   "metadata": {},
   "source": [
    "##### _Hamming loss_ (HL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d173d9b",
   "metadata": {},
   "source": [
    "Se define como la fracción de etiquetas que se predicen incorrectamente, es decir, la fracción de etiquetas incorrectas con respecto al número total de etiquetas:\n",
    "\n",
    "$$\n",
    "HL(y, ŷ) = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{L} \\sum_{j=1}^{L} I(y_{ij} \\neq \\hat{y}_{ij})\n",
    "$$\n",
    "\n",
    "Una posible implementación de esta métrica sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11be1bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_loss(y, ŷ):\n",
    "    return np.sum(y != ŷ) / np.prod(y.shape)\n",
    "\n",
    "print(f'Hamming loss = {hamming_loss(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e73a70",
   "metadata": {},
   "source": [
    "Se puede observar que se trata de una extensión de la métrica 1/0 _loss_, pero teniendo en cuenta esta vez las etiquetas parcialmente correctas. Esto es, si el sistema predice que hay un perro y un gato en la imagen, pero en realidad hay un perro y un caballo, la métrica HL dará un 0.5 y no un 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65235f2a",
   "metadata": {},
   "source": [
    "##### Coeficiente de similitud de Jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd51de21",
   "metadata": {},
   "source": [
    "Se usa para cómo de parecido son dos conjuntos. En el contexto de la clasificación multietiqueta, se utiliza para medir cuán similares son las etiquetas verdaderas y las predichas. Se define como:\n",
    "\n",
    "$$\n",
    "J(y, \\hat{y}) = \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|}\n",
    "$$\n",
    "\n",
    "Dicho de otro modo, el coeficiente de similitud de Jaccard es la proporción de la intersección de las etiquetas verdaderas y las predichas con respecto a la unión de las etiquetas verdaderas y las predichas. Por ejemplo, si tenemos un conjunto de etiquetas verdaderas y predichas, y la intersección de las dos es de 10 y la unión de las dos es de 20, el coeficiente de similitud de Jaccard sería de 0,5.\n",
    "\n",
    "Una posible implementación de esta métrica sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7a27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_score(y_true, y_pred):\n",
    "    y_true_bool = y_true.astype(bool)\n",
    "    y_pred_bool = y_pred.astype(bool)\n",
    "\n",
    "    inter = np.sum(np.logical_and(y_true_bool, y_pred_bool), axis=1)\n",
    "    union = np.sum(np.logical_or(y_true_bool, y_pred_bool), axis=1)\n",
    "    \n",
    "    jaccard = np.divide(\n",
    "        inter,\n",
    "        union,\n",
    "        out=np.zeros_like(inter),\n",
    "        where=union!=0,\n",
    "        casting='unsafe'\n",
    "    )\n",
    "    \n",
    "    return np.mean(jaccard)\n",
    "\n",
    "print(f'Jaccard score = {jaccard_score(y_test, ŷ_test):.02}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f910ef8a",
   "metadata": {},
   "source": [
    "El coeficiente de similitud de Jaccard toma valores entre $0$ y $1$, donde $1$ indica una similitud perfecta entre las etiquetas verdaderas y las predichas, y $0$ indica que no hay similitud entre las etiquetas verdaderas y las predichas. Un valor de $0.5$ indica, por tanto, que la mitad de las etiquetas verdaderas y predichas coinciden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "Este _notebook_ ha sido intensito, pero en él hemos visto las principales diferencias de los dos tipos de problemas con los que nos encontraremos en problemas de aprendizaje profundo: clasificación y regresión. Los modelos desarrollados para estos son muy similares, variando básicamente en la salida y su cálculo de error.\n",
    "\n",
    "Además, para la evaluación de estos modelos hemos presentado algunas medidas, algunas específicas para la clasificación y otras para la regresión. Lo bueno es que prácticamente todos los _frameworks_ incluyen estas implementaciones, probablemente mucho mejor de lo que nosotros mismos podríamos implementarlas. Sin embargo, es muy importante saber cómo estamos midiendo y qué significan esas mediciones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
