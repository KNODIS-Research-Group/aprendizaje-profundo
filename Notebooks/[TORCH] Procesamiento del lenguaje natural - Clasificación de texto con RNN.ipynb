{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Clasificación de texto con RNN<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2023-05-11</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "En un _notebook_ anterior exploramos la clasificación de textos con CNN, las cuales son adecuadas para capturar dependencias locales en datos de texto. Sin embargo, a veces necesitamos un modelo más potente que pueda capturar dependencias a largo plazo en los datos. Aquí es donde entran en juego las RNN.\n",
    "\n",
    "Las RNN están diseñadas específicamente para modelar datos secuenciales, lo que las hace ideales para tareas de clasificación de texto. A diferencia de las redes neuronales tradicionales, que procesan las entradas independientemente unas de otras, las RNN mantienen una memoria de las entradas anteriores y utilizan esta información para hacer predicciones sobre la entrada actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a explorar cómo utilizar RNN para la clasificación de texto en el mismo problema que en el _notebook_ donde clasificábamos las reseñas de Amazon que los usuarios hicieron sobre los productos mediante CNN, pero esta vez con RNN.\n",
    "\n",
    "Veremos que en realidad los cambios son mínimos, ya que es poco más que cambiar una capa por otra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from shutil import unpack_archive\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76e288",
   "metadata": {},
   "source": [
    "Y crearemos los directorios necesarios en caso de que no se hayan creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f0c7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Parámetros comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "Mantendremos los mismos parámetros globales para poder comparar ambos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_VOCAB_SIZE = 16384\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'tmp/glove.6B.zip'\n",
    "GLOVE_PATH = \"tmp/glove.6B.300d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd945b",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa0426",
   "metadata": {},
   "source": [
    "El proceso que llevaremos a cabo será el mismo que hicimos en el _notebook_ anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5df57",
   "metadata": {},
   "source": [
    "### Descarga del _dataset_ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>overall</th>\n",
       "      <th>vote</th>\n",
       "      <th>verified</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>style</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>True</td>\n",
       "      <td>06 3, 2013</td>\n",
       "      <td>A2TYZ821XXK2YZ</td>\n",
       "      <td>3426958910</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>Garrett</td>\n",
       "      <td>This is awesome to listen to, A must-have for ...</td>\n",
       "      <td>Slayer Rules!</td>\n",
       "      <td>1370217600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>10 11, 2014</td>\n",
       "      <td>A3OFSREZADFUDY</td>\n",
       "      <td>3426958910</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>Ad</td>\n",
       "      <td>bien</td>\n",
       "      <td>Five Stars</td>\n",
       "      <td>1412985600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>02 11, 2014</td>\n",
       "      <td>A2VAMODP8M77NG</td>\n",
       "      <td>3426958910</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>JTGabq</td>\n",
       "      <td>It was great to hear the old stuff again and I...</td>\n",
       "      <td>SLAYER!!!!!!!!!!!!!!!!!!!!!</td>\n",
       "      <td>1392076800</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "      <td>12 7, 2013</td>\n",
       "      <td>AAKSLZ9IDTEH0</td>\n",
       "      <td>3426958910</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>john F&amp;#039;n doe</td>\n",
       "      <td>well best of's are a bit poison normally but t...</td>\n",
       "      <td>slayer greatest hits! you mean everything righ...</td>\n",
       "      <td>1386374400</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>06 12, 2016</td>\n",
       "      <td>A3OH43OZJLKI09</td>\n",
       "      <td>5557706259</td>\n",
       "      <td>{'Format:': ' Audio CD'}</td>\n",
       "      <td>melinda a goodman</td>\n",
       "      <td>What can I say? This is Casting Crowns!!!This ...</td>\n",
       "      <td>This is a good, blessing filled</td>\n",
       "      <td>1465689600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   overall  vote  verified   reviewTime      reviewerID        asin  \\\n",
       "0        5   3.0      True   06 3, 2013  A2TYZ821XXK2YZ  3426958910   \n",
       "1        5   NaN      True  10 11, 2014  A3OFSREZADFUDY  3426958910   \n",
       "2        5   NaN      True  02 11, 2014  A2VAMODP8M77NG  3426958910   \n",
       "3        4   3.0     False   12 7, 2013   AAKSLZ9IDTEH0  3426958910   \n",
       "4        5   NaN      True  06 12, 2016  A3OH43OZJLKI09  5557706259   \n",
       "\n",
       "                      style       reviewerName  \\\n",
       "0  {'Format:': ' Audio CD'}            Garrett   \n",
       "1  {'Format:': ' Audio CD'}                 Ad   \n",
       "2  {'Format:': ' Audio CD'}             JTGabq   \n",
       "3  {'Format:': ' Audio CD'}  john F&#039;n doe   \n",
       "4  {'Format:': ' Audio CD'}  melinda a goodman   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  This is awesome to listen to, A must-have for ...   \n",
       "1                                               bien   \n",
       "2  It was great to hear the old stuff again and I...   \n",
       "3  well best of's are a bit poison normally but t...   \n",
       "4  What can I say? This is Casting Crowns!!!This ...   \n",
       "\n",
       "                                             summary  unixReviewTime image  \n",
       "0                                      Slayer Rules!      1370217600   NaN  \n",
       "1                                         Five Stars      1412985600   NaN  \n",
       "2                        SLAYER!!!!!!!!!!!!!!!!!!!!!      1392076800   NaN  \n",
       "3  slayer greatest hits! you mean everything righ...      1386374400   NaN  \n",
       "4                    This is a good, blessing filled      1465689600   NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_URL = 'https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Digital_Music_5.json.gz'\n",
    "DATASET_ZIP = 'tmp/Digital_Music_5.json.gz'\n",
    "\n",
    "# Download the remote file if it does not exist\n",
    "if not os.path.exists(DATASET_ZIP):\n",
    "    with open(DATASET_ZIP, 'wb') as f:\n",
    "        print(f'Downloading {DATASET_ZIP}...')\n",
    "        r = requests.get(DATASET_URL, verify=False)\n",
    "        f.write(r.content)\n",
    "        print('OK')\n",
    "\n",
    "corpus = pd.read_json(DATASET_ZIP, lines=True)\n",
    "corpus.dropna(subset=['overall', 'reviewText'], inplace=True)\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcdc82",
   "metadata": {},
   "source": [
    "### ... preparación de las entradas ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape: (169623,)\n"
     ]
    }
   ],
   "source": [
    "x_train = corpus['reviewText'].astype(str).str.strip()\n",
    "print(f'Training input shape: {x_train.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f933c217",
   "metadata": {},
   "source": [
    "### ... de las salidas ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232f8283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training output shape: (169623,)\n"
     ]
    }
   ],
   "source": [
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "print(f'Training output shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca78ee6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dac35d",
   "metadata": {},
   "source": [
    "### los _embeddings_ preentrenados de GloVe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738d3835",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking ...OK\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print('Downloading ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Unzip in the directory 'glove'.\n",
    "print('Unpacking ...', end='')\n",
    "unpack_archive(GLOVE_FILE, 'tmp')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "### ... cogiendo los pesos de las `MAX_VOCAB_SIZE` palabras más comunes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def yield_tokens(corpus):\n",
    "    \"\"\"Iterador que genera listas de tokens a partir del corpus\"\"\"\n",
    "    for text in corpus:\n",
    "        yield text.split()  # tenemos el corpus tokenizado por espacios\n",
    "\n",
    "def load_glove_embeddings(glove_path, corpus, embedding_dim, max_vocab_size):\n",
    "    print(\"Building vocabulary... \", end=\"\")\n",
    "    \n",
    "    # vocabulario a partir del corpus\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(corpus), specials=[\"<unk>\", \"<pad>\"], max_tokens=max_vocab_size+2)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    \n",
    "    print(f\"done ({len(vocab)} words).\")\n",
    "    \n",
    "    print(\"Loading GloVe embeddings... \", end=\"\")\n",
    "    word2vec = {}\n",
    "    \n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vector\n",
    "    \n",
    "    print(\"done.\")\n",
    "\n",
    "    print(\"Creating embedding matrix with GloVe vectors... \", end=\"\")\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    unassigned_words = 0\n",
    "\n",
    "    for word, idx in vocab.get_stoi().items():\n",
    "        if idx >= max_vocab_size + 2:\n",
    "            continue  # saltamos si el índice supera el tamaño máximo del vocabulario\n",
    "\n",
    "        if word == \"<unk>\":\n",
    "            word = \"unk\"  # compatibilidad con GloVe >:(\n",
    "\n",
    "        word_vector = word2vec.get(word)\n",
    "        if word_vector is not None:\n",
    "            embedding_matrix[idx] = word_vector\n",
    "        else:\n",
    "            unassigned_words += 1\n",
    "\n",
    "    print(f\"Number of words in GloVe: {len(word2vec)}\")\n",
    "    print(f\"Done ({unassigned_words} words unassigned).\")\n",
    "    print(f\"Vocab length: {len(vocab)}\")\n",
    "\n",
    "    return vocab, torch.tensor(embedding_matrix, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f10a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_index, max_len):\n",
    "        self.texts = [[word_index.get(word, 1) for word in text.split()] for text in texts]\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.texts[idx]\n",
    "        x = x[:self.max_len] + [0] * (self.max_len - len(x))  # Padding\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79d06b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe embeddings... done.\n",
      "Creating embedding matrix with GloVe vectors... Number of words in GloVe: 400000\n",
      "Done (168190 words unassigned).\n"
     ]
    }
   ],
   "source": [
    "texts = x_train.values\n",
    "labels = y_train.values\n",
    "\n",
    "word_index = {word: i+2 for i, word in enumerate({word for text in texts for word in text.split()})}\n",
    "# Make sure that no index exceeds MAX_VOCAB_SIZE\n",
    "word_index = {word: min(i + 2, MAX_VOCAB_SIZE + 1) for word, i in word_index.items()}\n",
    "\n",
    "embedding_matrix = load_glove_embeddings(GLOVE_PATH, word_index, EMBEDDING_DIM, MAX_VOCAB_SIZE)\n",
    "\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "train_dataset = TextDataset(train_texts, train_labels, word_index, MAX_SEQUENCE_LENGTH)\n",
    "val_dataset = TextDataset(val_texts, val_labels, word_index, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbce887",
   "metadata": {},
   "source": [
    "### ... y cargándolos dentro de la capa de `Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bb5e776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/mmq5zm5n1rg71ck5xqfyp8hh0000gn/T/ipykernel_77958/1180853728.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Clasificación basada en redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "Y ahora, en lugar de utilizar CNN, utilizaremos RNN. En este caso, el conjunto de dimensiones lo realizan la capa `TextVectorization` (que convierte el texto en secuencias de enteros de longitud $T$) y la capa Embedding (que convierte cada entero en un vector de dimensiones $D$), convirtiendo la entrada en un tensor con la forma $N \\times T \\times D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5392767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationRNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, embedding_dim, num_classes):\n",
    "        super(TextClassificationRNN, self).__init__()\n",
    "        # Crear la capa de embedding usando la matriz preentrenada\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
    "        self.gru = nn.GRU(embedding_dim, 64, batch_first=True)  # GRU con 64 unidades ocultas\n",
    "        self.fc = nn.Linear(64, num_classes)  # Capa de salida para clasificación\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # [batch_size, seq_len, embedding_dim]\n",
    "        _, hidden = self.gru(embedded)  # hidden: [1, batch_size, hidden_dim]\n",
    "        return self.fc(hidden.squeeze(0))  # [batch_size, num_classes]\n",
    "\n",
    "    def fit(self, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "        self.to(device)\n",
    "        history = {\n",
    "            \"train_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "            \"val_report\": []\n",
    "        }\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Entrenamiento\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "            for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "            history[\"train_loss\"].append(avg_loss)\n",
    "\n",
    "            # Evaluación\n",
    "            val_acc, val_report = self.evaluate(val_loader, device)\n",
    "            history[\"val_acc\"].append(val_acc)\n",
    "            history[\"val_report\"].append(val_report)\n",
    "\n",
    "            print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "            print(f\"Train Loss: {avg_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "            print(val_report)\n",
    "\n",
    "        return history\n",
    "\n",
    "    def evaluate(self, val_loader, device):\n",
    "        self.eval()\n",
    "        predictions, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = self(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                predictions.extend(preds.cpu().numpy())\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        report = classification_report(true_labels, predictions, zero_division=0)\n",
    "        return accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee23e3",
   "metadata": {},
   "source": [
    "Aunque el número de parámetros es similar, aumentar el número de unidades en una unidad recurrente no aumenta mucho el número de parámetros en nuestro modelo. Sin embargo, sí que aumentará mucho el tiempo de entrenamiento. Por lo tanto, nuestro modelo no podrá obtener resultados tan buenos como los anteriores.\n",
    "\n",
    "Entrenemos el modelo y esperemos que todo vaya bien (otra vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40bd1f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gc/mmq5zm5n1rg71ck5xqfyp8hh0000gn/T/ipykernel_77958/3936320419.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.embedding = nn.Embedding.from_pretrained(torch.tensor(embedding_matrix, dtype=torch.float32), freeze=False)\n",
      "Epoch 1/2 - Training: 100%|██████████| 4241/4241 [03:19<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "Train Loss: 0.2614, Val Accuracy: 0.9371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.04      0.07       782\n",
      "           1       0.45      0.04      0.07      1365\n",
      "           2       0.94      1.00      0.97     31778\n",
      "\n",
      "    accuracy                           0.94     33925\n",
      "   macro avg       0.66      0.36      0.37     33925\n",
      "weighted avg       0.91      0.94      0.91     33925\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 - Training: 100%|██████████| 4241/4241 [04:07<00:00, 17.13it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/2\n",
      "Train Loss: 0.2365, Val Accuracy: 0.9369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.12      0.19       782\n",
      "           1       0.59      0.03      0.05      1365\n",
      "           2       0.94      1.00      0.97     31778\n",
      "\n",
      "    accuracy                           0.94     33925\n",
      "   macro avg       0.65      0.38      0.40     33925\n",
      "weighted avg       0.91      0.94      0.91     33925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(set(labels))\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Crear modelo\n",
    "model = TextClassificationRNN(embedding_matrix, EMBEDDING_DIM, NUM_CLASSES)\n",
    "\n",
    "# Optimizer y pérdida\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(train_loader, val_loader, criterion, optimizer, DEVICE, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade383c",
   "metadata": {},
   "source": [
    "Echemos un vistazo al progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "103f8c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABBgAAAFVCAYAAABFH1x8AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAJ2AAACdgBx6C5rQAAJUdJREFUeJzt3X+QXXV9//HX3R9sNpvwjQvyla0mIASNoatFpIwkSgWxVdEBosBoKQOk1UIHgkkt449kqAEZAqVYxlAittWUIoKi8x1tRooWEEppSYMpI4SQBIEkSAzJZjebzd37/QOyErLZ3XA2ubt3H48Zhtx7zj33fTcfduY+OffcUqVSqQQAAACggLpqDwAAAACMfgIDAAAAUJjAAAAAABQmMAAAAACFCQwAAABAYQIDAAAAUJjAAAAAABTWUO0BBrJmzZr8/Oc/T09PT04//fS0trZWeyQAAACgHwc8MHR3d+fKK6/Ms88+m9mzZ+ekk05Kktxzzz259957UyqVMnv27EyePDk//elP86lPfSovvPBCHnrooXz4wx8+0OMCAAAAQ3DAA0NjY2PmzZuXZcuW9d3X0dGRZcuWZeHChdm4cWNuueWWzJ8/P6VSacjHrVQq6e3t3R8j7zelUimVSqXaY8CwsJ6pJdYztcaappZYz9Sa0bim6+vr+73/gAeGurq6TJo0abf7Vq1alenTp6ehoSFtbW3ZunVrent78/73vz933HFHdu7cmY9+9KN7HOu+++7L/fffnySZM2fOgRh/WDU2Nqanp6faY8CwsJ6pJdYztcaappZYz9Sa0bimR0xg6E9HR0daWlr6bjc3N6ezszNHHHFEjjjiiL0+bubMmZk5c2aSpFwuZ8OGDft71GHV2tqaTZs2VXsMGBbWM7XEeqbWWNPUEuuZWjMa13RbW1u/94+Ib5FoaWnJtm3b+m53dXVl/PjxVZwIAAAA2BcjIjBMnTo1jz/+eMrlctavX5+JEyemrm5EjAYAAAAMQVU+IrFo0aKsWbMmTU1NefLJJ3P++efnlFNO6buw44UXXliNsQAAAIDXqSqBYe7cuXvcd+qpp+bUU0+twjQAAABAUT6HAAAAABQmMAAAAACFCQwAAABAYQIDAAAAUJjAAAAAABRWlW+RINmyfWdKnTuyefvOao/C61Sq9gAjTKmrJy+NsvXs77AGlPbP32J9V0+2dJf3y7HZnf8OD4yG7T3Zup/WtL/DGjDK/hIbu3emY4ff0a82yv4KeY2DeyvVHmHYCAxV8hf/7+ls3u4XIwAAwFh245kHZUpztacYHgJDlfzV+34n41smZsvWLdUehdehUjuRcdgcfPDB2bLFeubA2Z//GU6cODFbt27dj89A4nfpgXTwxInZYk3Tj9H4n6Hf0bur+GU66h11yPjs7KyNNS0wVMm0N45Pa+v/yaZNzmKgNljP1JLW1knZtKm32mPAsLGmqSXWM7Xm4HGN2dRZ7SmGh4s8AgAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIWN+sCwYsWKLF26ND09PdUeBQAAAMasUf81le3t7Wlvb0+57OvxAAAAoFpG/RkMAAAAQPUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIWN+sCwYsWKLF26ND09PdUeBQAAAMashmoPUFR7e3va29tTLperPQoAAACMWaP+DAYAAACg+gQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwkZ9YFixYkWWLl2anp6eao8CAAAAY1ZDtQcoqr29Pe3t7SmXy9UeBQAAAMasUX8GAwAAAFB9AgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhAgMAAABQmMAAAAAAFCYwAAAAAIUJDAAAAEBhIz4wrFu3LjfddFMeeOCBao8CAAAA7EXDYDusXr06//zP/5xyuZy3ve1tOeecc4Z88O7u7lx55ZV59tlnM3v27Jx00kl92+65557ce++9KZVKmT17diZPntzvMSZPnpyTTz45mzdvHvLzAgAAAAfWgIFh586due222zJ37tyMGzduj+2bN2/OpEmT9nq7sbEx8+bNy7Jly3Z7XEdHR5YtW5aFCxdm48aNueWWWzJr1qz8+Mc/7tvnhBNOyMyZM1/nywIAAAAOpAEDwxNPPJGmpqbccMMN2bFjR84555wcc8wxfdvvu+++vPTSS/n0pz+dtWvX5tZbb80VV1zRFyPq6up2Cw67rFq1KtOnT09DQ0Pa2tqydevWTJs2LdOnT99j3xdffDEPPfRQurq6cuSRR6atrW237StWrMhjjz2Ws8466/W8fgAAAGAYDBgYNm3alHXr1uWaa67Jtm3bcvXVV+e6667r23766afn9ttvz+LFi/Pcc8/l8ssv7/dMh9fq6OhIS0tL3+3m5uZ0dnZmwoQJe+x7yCGH5MILL9zrsdrb29Pe3p5yuTzo8wIAAAD7x4AXeZwwYULe9ra3pbm5OYceemjGjRuXzs7O3fY58cQTs3z58kyZMqXfsxX609LSkm3btvXd7urqyvjx4/d9egAAAGBEGDAwTJ06Nc8//3zK5XI6OzvT2dm5Wwh45plnsmTJklx11VVpaWnJbbfdNqQnnTp1ah5//PGUy+WsX78+EydOTF3diP9CCwAAAGAvBvyIREtLSz74wQ9mwYIFKZfL+eM//uPdtj/11FOZM2dOWltbc8455+RHP/pRtm/fvtvHJBYtWpQ1a9akqakpTz75ZM4///xMmDAhp5xySubPn59SqTTgRyAAAACAka9UqVQq1R5iOJTL5WzYsKHaY+yT1tbWbNq0qdpjwLCwnqkl1jO1xpqmlljP1JrRuKZf++ULu/hcAgAAAFCYwAAAAAAUJjAAAAAAhQkMAAAAQGECAwAAAFCYwAAAAAAUJjAAAAAAhQkMAAAAQGECAwAAAFCYwAAAAAAUJjAAAAAAhQkMAAAAQGECAwAAAFCYwAAAAAAUJjAAAAAAhQkMAAAAQGECAwAAAFCYwAAAAAAUJjAAAAAAhQkMAAAAQGECAwAAAFCYwAAAAAAUJjAAAAAAhQkMAAAAQGECAwAAAFBYQ7UHAAAAgAOtUqmkXC6nUqlUdY7t27enp6enqjP0p1Qqpb6+PqVSaciPcQYDAAAAY0qlUklXV1fK5XK1R8nWrVurPUK/yuVyurq69inAOIMBAACAMaVcLqe+vj5NTU3VHiX19fUjInT0p7u7O+VyOQ0NQ0sHzmAAAABgTKlUKqmr83Z4MHV1dft0BsOo/4muWLEiS5cuHZGfWQEAAICxYtR/RKK9vT3t7e0j9pQSAAAAGAtG/RkMAAAAMNK99NJLufPOO4e8/+233577779/n57jfe97376ONawEBgAAANjPtmzZkrvuumuP+3t7e/vd/+yzz86MGTP291jDatR/RAIAAABer0rPjmTj+uE74GFvSqnxoD3uXrJkSZYvX55Zs2bl6aefzmmnnZZf/epXmTt3br7xjW/kueeey7Zt2zJ//vyceOKJue6663L00UfnuOOOy2c+85kceeSReeKJJ/KpT30qf/InfzLgCC+88ELmzJmTrq6utLS05IYbbkhzc3MuuuiibN++PaVSKddcc002bNiQv/7rv05LS0smT56c66+/vtBLFxgAAAAYuzauT++CS4btcHUL/i75ncl73H/RRRdl1apVWbp0aS677LJMmTIlV199derr6zN16tSMHz8+zzzzTC699NI9znTYsGFD7rzzzlQqlZx22mmDBoavfe1rOeuss3LGGWfkX/7lX3LzzTfnox/9aJqbm7N06dIkL5858Q//8A+ZM2dOTjvttL2eSbEvBAYAAADGrsPelLoFfzesxxuKd7/73UlefqN/7bXX5tFHH01DQ0PWr9/zbIpjjjkm48aNS5KUSqVBj7169erMnj07SXL88cfnJz/5SY499tgcf/zxueSSS9La2pq5c+fmM5/5TG666abcfffdmTlzZs4555yhvsp+CQwAAACMWaXGg/o942C4NTY2ZufOnX236+peviTiL37xi6xevTrf//73s27dunzyk5/cc8YhRIVXe+tb35pHHnkkb3nLW/LII4/krW99a7q7u/Nnf/ZnKZVKueGGG3LXXXfl7LPPzlVXXZVKpZKZM2fm4x//eJqbm1/3axQYAAAAYD877LDDctBBB2X27Nl56aWX+u4/+uijs23btsyaNSvvfve709jYWPi5Lrnkklx22WX59re/nebm5tx444158skn86UvfSkNDQ2pVCr527/929x8883593//9/T29ubkk08uFBeSpFSpVCqFpx8ByuVyNmzYUO0x9klra2s2bdpU7TFgWFjP1BLrmVpjTVNLrGeGQ09PT5IMy5v5ourr61Mul6s9Rr/29nNqa2vrd39nMAAAAMAocv/99+eGG27Y7b4vfOEL+b3f+73qDPQKgQEAAABGkRkzZmTGjBnVHmMPddUeAAAAABj9BAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAGCEuO6663L33XdXe4zXxbdIAAAAMGbtKPdm/daeYTvemyY25qD6sfn/8gUGAAAAxqz1W3vyF//v6WE73tc+cmQmT2ra4/4FCxZk5syZOeWUU7J58+acd955OeKII/L888+no6Mj8+fPz4knnjjgsb/3ve9l6dKl6erqynvf+9584QtfSJJ85StfycMPP5yDDjooc+fOzXHHHZfPf/7zWbt2berq6nLNNdfkqKOOGrbXuDcCAwAAAGPWmyY25msfOXJYj9efWbNm5etf/3pOOeWU/PCHP8zpp5+eT33qU5k4cWLWrFmTSy+9NHfdddeAx/7Qhz6UM844I0nyiU98ImvWrMlTTz2VjRs35gc/+EGSpFwu51vf+lba2tryN3/zN333HQgCAwAAAGPWQfV1/Z5xMNyOPfbYrFmzJtu2bcvdd9+dr3/967n22muzfPny1NfXZ/369YMe4+c//3kWL16cSqWSp59+Os8//3x++ctf5qSTTurbp76+Pr/85S/z0Y9+dLf7DoSx+cEQAAAAOMA+/OEP5+abb05zc3PWr1+f1atX5wc/+EGuv/769Pb2Dvr4r371q7n55pvz3e9+N29961tTqVRyzDHH5MEHH+zbp7e3N29729v2uO9AEBgAAADgADjjjDNy44035swzz8zRRx+dbdu25YwzzsjSpUvT2Nj/Ryte7cwzz8zZZ5+dz372s2lqevmsi1NPPTVveMMbcvrpp+cTn/hE/uM//iPnnntu1q1blzPOOCOzZs3K6tWr9/dLS5KUKpVK5YA8035WLpezYcOGao+xT1pbW7Np06ZqjwHDwnqmlljP1BprmlpiPTMcenpe/taIobyp39/q6+sP2DUS9tXefk5tbW397u8aDAAAADDCnHvuuX1v8JPkpJNOypw5c6o40eAEBgAAAMaUUqk0Ys8a2OW2226r9gjp7e3dpwtECgwAAACMKfX19dmxY0e6u7tTV1fdSxOWy+UDdhHGfdHb25tyuZyDDjpoyI9xkUcAAADGlFKplObm5gP29Y0DmThxYrVH6Fd9fX2am5tTKpWG/BhnMAAAADDmlEqlNDRU/y3xuHHj0tnZWe0xhoUzGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAob8YFh3bp1uemmm/LAAw9UexQAAABgL4YUGJ577rmce+65eeKJJ/bp4N3d3fnCF76Q888/f49AcM899+SLX/xivvSlL2XdunV7PcbkyZNz8skn79PzAgAAAAdWw1B2uvPOO/OOd7xjj/s3b96cSZMm7fV2Y2Nj5s2bl2XLlu32uI6OjixbtiwLFy7Mxo0bc8stt2TWrFn58Y9/3LfPCSeckJkzZ+7jywEAAACqYdDA8OSTT2bSpEmpq9vzZIf77rsvL730Uj796U9n7dq1ufXWW3PFFVdk3LhxSZK6urrdgsMuq1atyvTp09PQ0JC2trZs3bo106ZNy/Tp0/fY98UXX8xDDz2Urq6uHHnkkWlra9tt+4oVK/LYY4/lrLPOGuprBgAAAIbZoIHhrrvuyp//+Z/nn/7pn/bYdvrpp+f222/P4sWL89xzz+Xyyy/viwsD6ejoSEtLS9/t5ubmdHZ2ZsKECXvse8ghh+TCCy/c67Ha29vT3t6ecrk86PMCAAAA+8eA12D47//+7xx11FGZOHHiXvc58cQTs3z58kyZMqXfsxX609LSkm3btvXd7urqyvjx44c2MQAAADDiDBgY1qxZk5UrV2bhwoVZsWJF/vEf/zG/+c1v+rY/88wzWbJkSa666qq0tLTktttuG9KTTp06NY8//njK5XLWr1+fiRMn9vsRDAAAAGB0GPAjEmeeeWbOPPPMJMlNN92UD37wg3nDG97Qt/2pp57KnDlz0tramnPOOSc/+tGPsn379t0+JrFo0aKsWbMmTU1NefLJJ3P++ednwoQJOeWUUzJ//vyUSqUBPwIBAAAAjHylSqVSqfYQw6FcLmfDhg3VHmOftLa2ZtOmTdUeA4aF9UwtsZ6pNdY0tcR6ptaMxjX92i9f2MXnEgAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKGzEB4Z169blpptuygMPPFDtUQAAAIC9aBho48aNG3PjjTemvr4+vb29ueiiizJlypQhH7y7uztXXnllnn322cyePTsnnXRS37Z77rkn9957b0qlUmbPnp3Jkyf3e4zJkyfn5JNPzubNm4f8vAAAAMCBNWBgOOSQQ3LllVemrq4uv/jFL/K9730vl112Wd/2zZs3Z9KkSXu93djYmHnz5mXZsmW7HbejoyPLli3LwoULs3Hjxtxyyy2ZNWtWfvzjH/ftc8IJJ2TmzJnFXh0AAABwQAwYGOrr6/v+3NnZucfZC/fdd19eeumlfPrTn87atWtz66235oorrsi4ceOSJHV1dbsFh11WrVqV6dOnp6GhIW1tbdm6dWumTZuW6dOn77Hviy++mIceeihdXV058sgj09bWttv2FStW5LHHHstZZ5015BcNAAAADK8BA0OSrFmzJrfccktefPHFzJ07d7dtp59+em6//fYsXrw4zz33XC6//PK+uDCQjo6OtLS09N1ubm5OZ2dnJkyYsMe+hxxySC688MK9Hqu9vT3t7e0pl8uDPi8AAACwfwx6kccjjjgiCxcuzF/+5V/mG9/4xh7bTzzxxCxfvjxTpkzp92yF/rS0tGTbtm19t7u6ujJ+/PihTw0AAACMKAMGhp6enr4/jx8/Pk1NTbttf+aZZ7JkyZJcddVVaWlpyW233TakJ506dWoef/zxlMvlrF+/PhMnTkxd3Yj/QgsAAABgLwb8iMQvf/nL3HHHHamrq0ulUsl555232/annnoqc+bMSWtra84555z86Ec/yvbt23f7mMSiRYuyZs2aNDU15cknn8z555+fCRMm5JRTTsn8+fNTKpUG/AgEAAAAMPKVKpVKpdpDDIdyuZwNGzZUe4x90tramk2bNlV7DBgW1jO1xHqm1ljT1BLrmVozGtf0a798YRefSwAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAAChMYAAAAAAKExgAAACAwgQGAAAAoDCBAQAAACisodoDjFXlKy/Ni9u70luqS+rrk4aGpL7ht//u+3N9Sg2NL+/z6vtf2Zb6xn4e/8r9DQ0pvXrbkB+/65/6lOo0KAAAAAYnMFRJ6X1/mOZSJZ1btyblnS//s3NnUi6/8u+evj9XdvYk27tevl3emezsedV+u/55ze1X/lzp7S02aF1d/4HilQCxz2Gk7zH7I4z89n5hBAAA4MASGKqk7uQ/yvjW1mzftGm/Pk+lt/eVMPGqKLFbmHht2HhNoNi1bciPHyCM7NpXGBFGAACAmiMw1LhSXd3Lb7YbG1/f44d5nqHqN4yUd/YTI/YhjAzx8a8rjOx6rDACAACMUQIDI9KYCyN9cWM/hJHdHr//wsgL/QYOYQQAAMYKgQGGUW2GkVfuHySMTBg3Lh1bNu/j48tVDyP7fMbIK/fvlzAylMcLIwAAjFACAzAsYWRca2s69/M1RV6rcBjp59ohwxdGyq+JJMKIMAIAUNsEBmDUqukzRga4qGrhMNK9vd9rlwgjv93W2zwulZ4eYQQAYB8IDAAH2OgPI3sLE0MNI/v6+H0II/1cs+T1hJEXX31jFIURZ4wAANUkMAAwJGMijLxy/4Tm5nS89JtRF0Z2s9cwUr97EBnOMFJfn1LDa48pjADAWCEwAFDTXk8YaWptzbZXrikyYsLIbl+ZO/i3zewRRob8+NcTRn770RxhBADGLoEBAEagmjljZEhh41WBYrjDSLmc7OwZ82Gk3NuTytYOYQSA/UpgAACGTc2FkSFeVPX1hZGXn+91h5Fy+eXnHYK9fsfPfgsjDf083hkjALVOYAAAxrxaDyMHjx+fLb/ZNPQwMmhYGSSM9Hvtkn0PI3sljACMSAIDAMAoNdQw0tjamtKmPc9jGLlhZOCLqlb28m01Qwoj5Z3CiDAC7CcCAwAAB9SoPmOkdy8RYqhhpJ+Lsg7+eGHk1fv2bGpNpXObMAIjkMAAAABD0BdGGoSRfQ4jr7koa5EwsnmoL7xIGHnt/cN6xsggjxdGGMUEBgAAqGG1EUZ+e+2QSRMmZPOLL+5jGPnt4/cIG/1cVLVS3pl0dg8tjLzmozlVPWNEGKHKBAYAAGDE2VsYqW9tTemg5sEfv78GG8Tewshez/h4baAY1jDyyrVOhBFh5AARGAAAAIZJbZ0xUn5NDBnGMPKq+19fGNn12P0cRnbd3h9h5JX7e4/7/eH5SxwBBAYAAIAxbtSGkUqln7BRMIzs2uf1hpFdt4cYRnZe+XfJ4ZOr9BMcXgIDAAAAo1KpVHo5ioziMNL4f/9v8tKWKk0yvAQGAAAAOIBeHUZK9bXzttwVLQAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKExgAAAAAAoTGAAAAIDCBAYAAACgMIEBAAAAKKxUqVQq1R5irOru7k5TU1O1x4BhYT1TS6xnao01TS2xnqk1tbSmncFQRddff321R4BhYz1TS6xnao01TS2xnqk1tbSmBQYAAACgMIGhimbMmFHtEWDYWM/UEuuZWmNNU0usZ2pNLa1p12AAAAAACnMGAwAAAFCYwAAAAAAU1lDtAcaCe+65J/fee29KpVJmz56dyZMn923bsGFDFi9enJ07d+Y973lPPvaxj1VxUhiagdb0jTfemI0bN6a3tzennXZaTj755OoNCkMw0HreZcGCBWlra8uf/umfVmFCGLqB1nN3d3e++c1v9v2O/qu/+quMGzeuitPC4AZa048++mi+853vpL6+PkceeWQuvPDCKk4Kg+vu7s6VV16ZZ599NrNnz85JJ5202/ZaeG8oMOxnHR0dWbZsWRYuXJiNGzfmlltuyfz58/u2L126NOeee26mTp2aBQsW5MQTT8xhhx1WxYlhYIOt6U984hM5/PDD09PTk7lz52bGjBlpaPCrhpFpsPWcJP/1X//lTRijwmDr+Y477siMGTNy7LHHVnFKGLqhrOnPfe5zOfTQQ3PVVVdl7dq1mTJlShUnhoE1NjZm3rx5WbZsWb/ba+G9oY9I7GerVq3K9OnT09DQkLa2tmzdujW9vb1925999tkcc8wxKZVKOe644/K///u/VZwWBjfYmj788MOTJA0NDSmVStUaE4ZksPXc29ubf/3Xf80f/uEfVnFKGJrB1vPKlSvzyCOPZMGCBbnzzjurOCkMzWBr+i1veUs6OzvT29ubHTt2pKWlpYrTwuDq6uoyadKkvW6vhfeGAsN+1tHRsdsvu+bm5nR2dvbdfvUvyZaWlnR0dBzQ+WBfDbamd7n77rvz+7//+85eYEQbbD3/7Gc/ywknnJDGxsZqjAf7ZLD1vHbt2rzrXe/Kl7/85Tz99NNZuXJlNcaEIRtsTc+YMSMLFy7MZZddlra2thx66KHVGBOGTS28NxQY9rOWlpZs27at73ZXV1fGjx/fd/vV/4e3s7MzEyZMOKDzwb4abE0nyQMPPJCnn346Z5999oEeD/bJQOt5x44duf/++/MHf/AH1RoP9slgv58nTpyY9vb21NXVpb29PWvXrq3GmDBkg63pJUuW5Oqrr86NN96YJHn44YcP+IwwnGrhvaHAsJ9NnTo1jz/+eMrlctavX5+JEyemru63P/Y3v/nNWbVqVSqVSh599NFMmzatitPC4AZb08uXL8+//du/5ZJLLtntfhiJBlrPGzduzLZt2/LVr3413/72t/Poo4/mZz/7WZUnhr0b7PfztGnT8vTTTydJVq9enTe96U3VGhWGZLA1XVdX1xccDj744FH5f3vh1WrhvWGpUqlUqj1ErfvJT36Sn/70pymVSrnwwguzefPmdHR0ZMaMGVm/fn0WL16ccrmc448/Ph//+MerPS4MaqA1PXv27LS2tqa5uTlJctlllw34WTOotoHW8y4rV67MAw884FskGPEGWs8vvPBCFi9enJ6enrz5zW/O7NmzXSuHEW+gNf3ggw/mhz/8YRobG9PS0pJLL700TU1N1R4ZBrRo0aKsWbMmTU1N+d3f/d28613vqqn3hgIDAAAAUJjzlwEAAIDCBAYAAACgMIEBAAAAKExgAAAAAAprqPYAAEB1ffKTn8yUKVP6bh966KH5/Oc/v1+e5zvf+c6wHxcAGBkEBgAg1157bbVHAABGOYEBAOjXT3/60zz00EMpl8v59a9/nba2tlx88cUZP358Ojs7s2TJkqxduzZJ8pGPfCQf+MAHkiTr1q3LN7/5zXR0dCRJLrjggkybNi1J8v3vfz8PPvhguru7c/HFF2fq1Kl7PO/FF1+c97///Vm+fHm2bNmS888/P8cff3xWrlyZO+64IwsWLOibb+XKlbn44ov7Zq2rq8uvfvWrHHXUUfmjP/qjLF26NL/+9a/zsY99LB/60IcOwE8NAMYugQEAyLx58/r+PG3atFxwwQVJkscffzyLFi3KG9/4xtx666357ne/m/POOy/f/e53M378+Fx33XXZsmVLrrjiihx99NFpa2vLtddem4suuijvfOc7Uy6X093d3XfsSZMm5Zprrsn999+f22+/PV/84hf7nadUKuWqq67KE088kZtuuinHH3/8oK9h9erVWbRoUSZMmJDPf/7z+cEPfpAvf/nL+c1vfpPPfe5zOfXUU1NfX1/wJwUA7I3AAADs9SMSxx57bN74xjcmST7wgQ/k61//epJk5cqV+exnP5skOfjgg/Oe97wnK1euTJI0NTXlne98Z5Kkvr4+48eP7zvee9/73iTJ0Ucfndtvv32v87x6vw0bNgzpNbzjHe/IwQcfnCSZPHly3v72t6e+vj6HHnpompqasnnz5hxyyCFDOhYAsO98iwQAcMAcdNBBSZK6urr09vbudb/GxsY99quvr0+lUunbZ8eOHf0+ZtfjXnu7XC4XfwEAwF4JDADAXv3iF7/Ir3/96yQvX/Ng+vTpSZLp06fnnnvuSZJs3bo1//mf/5np06enra0t3d3d+Z//+Z8kSW9vbzo7O4dllsMOOyzPPfdctm/fnp07d+bhhx8eluMCAMPDRyQAgN2uwdDU1JSvfOUrSV6+HsPf//3f54UXXsjhhx+eSy65JEkya9asLFmyJJ/73OeSJGeddVYmT56cJJk7d25uvfXWfOtb30pdXV0uuOCCvP3tby88Y2traz74wQ9m3rx5mTRpUqZMmbLb9R2G6uqrr84nP/nJHHXUUYVnAgB+q1R59bmGAACvePW3NAAADMZHJAAAAIDCnMEAAAAAFOYMBgAAAKAwgQEAAAAoTGAAAAAAChMYAAAAgMIEBgAAAKCw/w9zt1ZQYWE/RwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1280x384 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e83a",
   "metadata": {},
   "source": [
    "Veamos ahora cómo interpreta el sentimiento de una reseña buena, regular y mala extraída del sitio web de amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "915a270e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Results:\n",
      "Sample 1 classified as: 2\n",
      "Sample 2 classified as: 2\n",
      "Sample 3 classified as: 2\n"
     ]
    }
   ],
   "source": [
    "good = \"My nephew is on the autism spectrum and likes to fidget with things so I knew this toy would be a hit. Was concerned that it may not be \\\"complex\\\" enough for his very advanced brain but he really took to it. Both him (14 yrs) and his little brother (8 yrs) both enjoyed playing with it throughout Christmas morning. I'm always happy when I can find them something unique and engaging.\"\n",
    "poor = \"I wasn't sure about this as it's really small. I bought it for my 9 year old grandson. I was ready to send it back but my daughter decided it was a good gift so I'm hoping he likes it. Seems expensive for the price though to me.\"\n",
    "evil = \"I just wanted to follow up to say that I reported this directly to the company and had no response. I have not gotten any response from my review. The level of customer service goes a long way when an item you purchase is defective and this company didn’t care to respond. No I am even more Leary about ordering anything from this company. I never asked for a refund or replacement since I am not able to return it. I’m just wanted to let them know that this was a high dollar item and I expected it to be a quality item. Very disappointed! I bought this for my grandson for Christmas. He loved it and played with it a lot. My daughter called to say that the stickers were peeling on the corners. I am not able to take it from my grandson because he is autistic and wouldn’t understand. I just wanted to warn others who are wanting to get this. Please know that this is a cool toy and it may not happen to yours so it is up to you.\"\n",
    "\n",
    "test_texts = [good, poor, evil]\n",
    "test_labels = [2, 1, 0]  # good, poor, evil\n",
    "test_dataset = TextDataset(test_texts, test_labels, word_index, MAX_SEQUENCE_LENGTH)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "print(\"\\nTest Results:\")\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, _) in enumerate(test_loader):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        outputs = model(inputs)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        print(f\"Sample {i + 1} classified as: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos estudiado cómo utilizar RNN para clasificar textos y las hemos comparado con las CNN. Aunque ambas arquitecturas pueden ser eficaces para la clasificación de textos, presentan algunas diferencias clave.\n",
    "\n",
    "La principal es las RNN son más adecuadas para captar las dependencias a largo plazo en los datos de texto, mientras que las CNN son más adecuadas para captar las dependencias locales. Esto hace que las RNN sean una buena opción para tareas como el análisis de sentimientos o la traducción de idiomas, donde el contexto de una palabra o frase puede ser crucial para determinar su significado.\n",
    "\n",
    "Sin embargo, el entrenamiento de las RNN puede ser más costoso que el de las CNN (ya hemos visto el tiempo que tarda una red pequeña en ser entrenada durante 5 _epochs_). Por otro lado, las CNN son más rápidas de entrenar y se adaptan mejor a conjuntos de datos más grandes. Son especialmente adecuadas para tareas como la clasificación de textos o el reconocimiento de imágenes, en las que las características locales son importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
