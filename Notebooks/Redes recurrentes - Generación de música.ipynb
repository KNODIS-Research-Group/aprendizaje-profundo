{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "# Generando música<a id=\"top\"></a>\n",
    "\n",
    "<i>Última actualización: 2025-03-05</small></i></div>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En este _notebook_ vamos a crear un modelo que aprenderá a «tocar música». El entrecomillado es porque, para tocar música bien, se necesitan modelos y técnicas muy complejas que quedan un poco fuera del alcance de un tutorial como este.\n",
    "\n",
    "Sin embargo, en este ejercicio tocaremos los fundamentos de la generación basada en notas y acordes y, junto con los modelos que veremos más adelante en este tema más conceptos como redes bidireccionales e incrustaciones de la parte de procesamiento del lenguaje natural podremos generar música con un poco más de sentido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un modelo de predicción de notas basado en una secuencia de notas anteriores. Al final habremos aprendido a:\n",
    "\n",
    "- Leer y escribir ficheros midi,\n",
    "- Generar secuencias siguiendo una tipología de red recurrente _one-to-many_, y\n",
    "- Guardar modelos entrenados en disco para entrenarlos en diferentes momentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A continuación importaremos las librerías que se utilizarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import music21\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También vamos a configurar algunos parámetros para adaptar la presentación grafica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos los directorios necesarios para almacenar los ficheros de datos y los modelos generados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y terminamos con constantes que se utilizarán a lo largo del _notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1024\n",
    "TRAIN_EPOCHS = 2\n",
    "SEQUENCE_LEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Empezaremos cargando todas las notas de los archivos `.mid` ubicados en la ruta relativa `datasets/Music`. Estas notas se almacenarán en una lista llamada `notes`. Algunos detalles de implementación:\n",
    "\n",
    "- Para parsear un fichero midi utilizaremos la función `parse(file)` del módulo `converter` de la librería `music21`,\n",
    "- Las notas que queremos obtener están en el atributo `.flat.notes` del midi analizado. Sin embargo, tenemos que tener en cuenta que contiene dos tipos de datos:\n",
    "  - Notas normales, que son del tipo `note.Note`. Si la nota es `note`, almacenaremos directamente en la lista de notas la representación en cadena de `note.pitch`.\n",
    "  - Acordes, que son del tipo `chord.Chord`. Son una lista de notas, y lo que almacenaremos será la lista de sus notas (si el acorde es `chord`, la lista será `chord.normalOrder`) como una cadena de texto donde cada nota irá separada por un punto (`'.'`).\n",
    "\n",
    "Esto no es por capricho; es una forma de representar las notas que facilitará la conversión posterior de las notas generadas en una nueva pista de audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_files = glob.glob(\"../Datasets/music/doom/*.mid\")\n",
    "\n",
    "notes = []  # Almacenará todas las notas y acordes de los ficheros\n",
    "for file in midi_files:\n",
    "    print(f'Parsing {file}')\n",
    "\n",
    "    midi = music21.converter.parse(file)\n",
    "    \n",
    "    for note_or_chord in midi.flat.notes:\n",
    "        if isinstance(note_or_chord, music21.note.Note):\n",
    "            notes.append(str(note_or_chord.pitch))\n",
    "        elif isinstance(note_or_chord, music21.chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in note_or_chord.normalOrder))\n",
    "    \n",
    "    notes.append('EOS')  # Añadimos un token de fin de canción\n",
    "\n",
    "print(f\"Some notes: {notes[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestro siguiente paso será crear dos diccionarios: `note_to_int` y `int_to_note`. ¿Cuál será su significado? Pues que tenemos la lista de todas las notas de todas las pistas de sonido. Como las redes trabajan con números, no con símbolos, lo que haremos será asignar un valor único a cada nota diferente. Con estos dos diccionarios sabremos traducir de nota a número y de un número a su nota para poder traducir dentro y fuera de la red.\n",
    "\n",
    "Por lo tanto, crearemos estos dos diccionarios con las diferentes notas en orden ascendente, desde 0 hasta el número de notas menos una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_notes = sorted(set(notes))\n",
    "note_to_int = {note: i for i, note in enumerate(unique_notes)}\n",
    "int_to_note = {i: note for note, i in note_to_int.items()}\n",
    "\n",
    "print(f\"Unique notes: {len(unique_notes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuamos con la preparación de los conjuntos de datos. En `notas` tenemos la lista ordenada de notas. Se espera que las notas estén determinadas por la secuencia anterior. Por facilitar la implementación, no hemos creado un nuevo token para indicar que una canción ha terminado, por lo que habrá ciertas pausas que no se correspondan con un compás real. Si te apetece modificarlo, ¡adelante!\n",
    "\n",
    "Lo que crearemos ahora será el conjunto de entrenamiento, las variables `x_train` y `y_train`. `y_train` estará formado por las secuencias de entrada, que tendrán una longitud de 50 (la variable SEQUENCE_LEN, ya creada), mientras que `y_train` tendrá la nota correspondiente a continuación de esa secuencia. Construiremos este conjunto a partir de la lista `notas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_song = []\n",
    "x_data, y_data = [], []\n",
    "for note in notes:\n",
    "    current_song.append(note)\n",
    "    if note == \"EOS\":  # Fin de canción, así que procesamos la secuencia\n",
    "        if len(current_song) > SEQUENCE_LEN:  # Si hay suficientes notas\n",
    "            for i in range(SEQUENCE_LEN, len(current_song)):\n",
    "                seq_in = current_song[i - SEQUENCE_LEN:i]\n",
    "                seq_out = current_song[i]\n",
    "                x_data.append([note_to_int[n] for n in seq_in])\n",
    "                y_data.append(note_to_int[seq_out])\n",
    "        current_song = []\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f'{x_data[i]} -> {y_data[i]}')\n",
    "\n",
    "print(f\"X data shape: {x_data.shape}\")\n",
    "print(f\"Y data shape: {y_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizaremos los valores de entrada para que estén en el rango [0, 1]. Para ello, dividiremos entre el número de notas diferentes. Ojo, tendría más sentido usar la aproximación de usar _embeddings_ para representar las notas en lugar de usar el índice como tal, pero como todavía no sabemos qué es esto, nos conformamos con normalizarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = x_data / len(unique_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Como vamos a alimentar una red recurrente, las dimensiones de entrada deben ser $M \\times T \\times C$, siendo:\n",
    "\n",
    "- $M$: El número de ejemplos, es decir, el número total de secuencias.\n",
    "- $T$: El tamaño de la secuencia, que hemos definido en la constante anterior.\n",
    "- $C$: El número de parámetros que tiene cada elemento de la secuencia, que en nuestro caso es $1$ (cada nota es un único entero).\n",
    "\n",
    "El problema es que las dimensiones de nuestro conjunto de entrenamiento no son esas, por lo que tendremos que remodelar el conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.expand_dims(x_data, axis=2)\n",
    "\n",
    "print(f'X data shape: {x_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último crearemos un Dataset para este conjunto de datos y su `DataLoader` correspondiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.x[idx], dtype=torch.float32),\n",
    "            torch.tensor(self.y[idx], dtype=torch.long),\n",
    "        )\n",
    "\n",
    "dataset = MusicDataset(x_data, y_data)\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset=dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y con los conjuntos creados y listos para entrenar, pasamos a trabajar con el modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Implementando y entrenando el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora crearemos un modelo para que aprenda a predecir la siguiente nota dada una secuencia de notas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGeneratorModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_classes,\n",
    "        num_layers,\n",
    "        dropout=0.2,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.rnn_layers = torch.nn.ModuleList()\n",
    "        self.dpo_layers = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            in_size = input_size if i == 0 else hidden_size\n",
    "            self.rnn_layers.append(\n",
    "                torch.nn.GRU(in_size, hidden_size, batch_first=True)\n",
    "            )\n",
    "            self.dpo_layers.append(\n",
    "                torch.nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        self.fc = torch.nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for gru, dropout in zip(self.rnn_layers, self.dpo_layers):\n",
    "            x, _ = gru(x)\n",
    "            x = dropout(x)\n",
    "        x = x[:, -1, :]\n",
    "        logits = self.fc(x)\n",
    "        return logits\n",
    "\n",
    "music_generator = MusicGeneratorModel(\n",
    "    input_size=1,\n",
    "    hidden_size=128,\n",
    "    num_classes=len(unique_notes),\n",
    "    num_layers=3,\n",
    "    dropout=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora entrenaremos el modelo con nuestro conjunto de datos durante 25 épocas; no son muchas, pero la carga computacional derivada del entrenamiento de este tipo de modelos es bastante pesada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = utils.train(\n",
    "    model=music_generator,\n",
    "    train_loader=data_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(music_generator.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=len(unique_notes)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de ver la evolución del error, podríamos modificar la creación del modelo para que cargue el mejor punto de control si existe y si queremos (mediante una variable, por ejemplo `LOAD_PREVIOUS`).\n",
    "\n",
    "Ahora, veamos cómo han evolucionado error y exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A generar música"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos un modelo entrenado para generar música. Ahora procederemos a generar una canción. Para hacerlo sencillo, generaremos una canción de N notas (digamos 100, y veremos cómo se comporta. Para ello haremos lo siguiente\n",
    "\n",
    "1. Crear una secuencia aleatoria de inicio del tamaño de secuencia esperado, lo que constituirá nuestra primera entrada,\n",
    "2. Pasar esa secuencia al modelo y recoger la siguiente nota que predice,\n",
    "3. Eliminar la primera nota de la secuencia y añadir la nueva nota al final, lo que constituirá la siguiente secuencia, y\n",
    "4. Continuar así hasta terminar de generar notas.\n",
    "\n",
    "El resultado será una lista con la secuencia y todas las notas generadas. La lista con las notas generadas se llamará `new_song`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_index = eos_index = note_to_int[\"EOS\"]\n",
    "while start_index == eos_index:\n",
    "    start_index = np.random.randint(0, len(dataset))\n",
    "\n",
    "pattern, _ = dataset[start_index]\n",
    "pattern = pattern.unsqueeze(0)  # (SEQUENCE_LEN, 1) -> (1, SEQUENCE_LEN, 1)\n",
    "pattern.shape\n",
    "\n",
    "generated_song = []\n",
    "max_generated = 256\n",
    "\n",
    "music_generator.eval()\n",
    "for _ in range(max_generated):\n",
    "    with torch.no_grad():\n",
    "        output = music_generator(pattern)\n",
    "        probabilities = torch.softmax(output, dim=1).cpu().numpy().flatten()\n",
    "        next_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "    \n",
    "    if next_index == eos_index:\n",
    "        break\n",
    "    \n",
    "    generated_song.append(int_to_note[next_index])\n",
    "    \n",
    "    next_value = next_index / float(len(unique_notes))\n",
    "    pattern_np = pattern.cpu().numpy()\n",
    "    pattern_np = np.roll(pattern_np, -1, axis=1)\n",
    "    pattern_np[0, -1, 0] = next_value\n",
    "    pattern = torch.tensor(pattern_np)\n",
    "\n",
    "print(f\"Generated song: {generated_song}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente fragmento de código transforma la lista de notas en un midi, separando cada nota por medio segundo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "output_notes = []\n",
    "\n",
    "for pattern in generated_song:\n",
    "    if ('.' in pattern) or pattern.isdigit():\n",
    "        # Acorde, así que lo partimos en sus notas\n",
    "        notes = []\n",
    "        for current_note in pattern.split('.'):\n",
    "            new_note = music21.note.Note(int(current_note))\n",
    "            new_note.storedInstrument = music21.instrument.Violin()\n",
    "            notes.append(new_note)\n",
    "        new_chord = music21.chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    else:\n",
    "        # Nota, así que la creamos directamente\n",
    "        new_note = music21.note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = music21.instrument.Violin()\n",
    "        output_notes.append(new_note)\n",
    "\n",
    "    offset += 0.5\n",
    "\n",
    "midi_stream = music21.stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp='tmp/test_output.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, el modelo ha generado una canción. Sí, no respetamos los tiempos, hay secuencias que no tienen sentido (los cortes entre canciones), etc., pero nos ha servido como experimento para ver el desarrollo de un proyecto de principio a fin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hemos implementado un modelo recurrente que aprende de muchos datos para resolver un problema _de_uno_a_muchos_: generar música a partir de una semilla inicial.\n",
    "\n",
    "Os animamos a modificar la arquitectura para ver si encontráis una que genere canciones que tengan algún sentido, y a probar a añadir entradas aleatorias durante el entrenamiento para que durante la inferencia se puedan añadir dichas entradas para alterar la generación de melodías."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
