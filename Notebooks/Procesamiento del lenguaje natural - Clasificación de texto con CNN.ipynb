{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Clasificación de texto con CNN<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-13</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "En el procesamiento del lenguaje natural (NLP, del inglés natural language processing_), una tarea muy típica es la clasificación de textos. En esta tarea, un texto dado se clasifica según su significado. A menudo se utiliza, por ejemplo, para el problema del análisis de sentimientos.\n",
    "\n",
    "Se trata de un problema denominado _many-to-one_, es decir, uno en el que el tamaño de la secuencia de entrada es $T_X = 1$, pero el tamaño de la secuencia de salida es $T_Y = 1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a hacer un experimento en el que utilizaremos el conjunto de datos de reseñas de amazon para una tarea de análisis de sentimiento. A partir de los datos de reseñas y valoraciones, identificaremos si una reseña es positiva, neutra o negativa, y para ello utilizaremos una primera aproximación utilizando un modelo de redes neuronales convolucionales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ea7481e",
   "metadata": {},
   "source": [
    "Y crearemos los directorios necesarios en caso de que no se hayan creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64979a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_PATH = pathlib.Path('tmp')\n",
    "TEMP_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d22a2c",
   "metadata": {},
   "source": [
    "## Parámetros globales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "231d1a4e",
   "metadata": {},
   "source": [
    "Comenzaremos definiendo los parámetros que utilizaremos a lo largo del cuaderno, que consistirán en la longitud máxima de las secuencias (recuerda que deben tener una longitud fija) y la dimensión del vector de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuántas dimensiones tienen nuestros vectores de palabras (usaremos\n",
    "#  GloVe, así que 50, 100, 200 o 300)\n",
    "EMBEDDING_DIM = 300\n",
    "# Tamaño máximo de nuestro vocabulario (se elegirán los token más\n",
    "#  frecuentes)\n",
    "MAX_VOCAB_SIZE = 16384\n",
    "# Longitud máxima de las secuencias de palabras\n",
    "MAX_SEQUENCE_LEN = 32"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e86b03b1",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b1ff85c",
   "metadata": {},
   "source": [
    "Vamos a cargar los datos de entrenamiento, que consistirán en los datos de reseñas de Amazon de la categoría «Música digital» (https://nijianmo.github.io/amazon/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "564710a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to /home/blazaid/Projects/aprendizaje-profundo/Notebooks/tmp/Digital_Music_5.json\n",
      "Loading text corpus\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "DATASET_URL = 'https://github.com/blazaid/aprendizaje-profundo/raw/refs/heads/gh-pages/Datasets/Digital_Music_5.json.gz'\n",
    "DATASET = pathlib.Path('tmp/Digital_Music_5.json')\n",
    "\n",
    "print(f\"Downloading dataset to {DATASET.resolve()}\")\n",
    "if not DATASET.exists():\n",
    "    with requests.get(DATASET_URL, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        with gzip.GzipFile(fileobj=response.raw) as f_gz:\n",
    "            with DATASET.open(\"wb\") as f:\n",
    "                shutil.copyfileobj(f_gz, f)\n",
    "else:\n",
    "    print(\"File already exists! Nice\")\n",
    "\n",
    "print(\"Loading text corpus\")\n",
    "corpus = pd.read_json(DATASET, lines=True)\n",
    "corpus.dropna(subset=['overall', 'reviewText'], inplace=True)\n",
    "corpus.head()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bace8799",
   "metadata": {},
   "source": [
    "El proceso que realizaremos en este _notebook_ será ver cómo construir un modelo convolucional para trabajar con este tipo de conjuntos de datos. Por lo tanto no entraremos en el detalle de obtener un conjunto de datos de prueba.\n",
    "\n",
    "Sin embargo, hay que tener en cuenta que en un problema real sería necesario entrenar con validación y contrastar con un conjunto de prueba antes de poner nuestro modelo en producción."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5719ffd0",
   "metadata": {},
   "source": [
    "### Preparando la entrada a nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "074e95f9",
   "metadata": {},
   "source": [
    "La entrada de nuestro modelo serán las reseñas como tales; por lo tanto, tomaremos la columna `reviewText` como nuestro conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f62d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = corpus['reviewText'].astype(str).str.strip()\n",
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "\n",
    "print(f'Training input shape:  {x_train.shape}')\n",
    "print(f'Training output shape: {y_train.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6464895c",
   "metadata": {},
   "source": [
    "Ahora crearemos una capa [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization), que se encargará de:\n",
    "\n",
    "1. Convertir una reseña completa en una secuencia de enteros (palabras), asignando a cada palabra un valor único.\n",
    "2. Truncar o rellenar las sencuencias para que estas mantengan una longitud fija previamente establecida (en nuestro caso `MAX_SEQUENCE_LEN`).\n",
    "\n",
    "El vocabulario se extraerá de nuestra entrada, tomando las `MAX_VOCAB_SIZE` palabras más comunes. Hemos añadido `+ 2` a la longitud ya que hay dos tokens preasignados: Padding (`''`) y palabras fuera del vocabulario (`'[UNK]'`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=MAX_VOCAB_SIZE + 2,\n",
    "    output_sequence_length=MAX_SEQUENCE_LEN,\n",
    ")\n",
    "vectorize_layer.adapt(x_train.to_numpy())\n",
    "\n",
    "print(f'Vocabulary length: {len(vectorize_layer.get_vocabulary())}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88b782d0",
   "metadata": {},
   "source": [
    "### Preparando la salida de nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b394b67f",
   "metadata": {},
   "source": [
    "Partiendo de la base de que las reseñas con valoraciones altas serán positivas y las que tengan valoraciones bajas serán negativas, conservaremos sólo la reseña y la valoración del producto del conjunto de datos.\n",
    "\n",
    "En concreto, convertiremos las valoraciones de las reseñas en 0 si son malas (1 o 2 estrellas), 1 si son mediocres (3 estrellas) y 2 si son buenas (4 o más estrellas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "print(f'Training output shape: {y_train.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Uso de _embeddings_ preentrenados en nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "Ya hemos visto que, a la hora de construir un modelo lingüístico, un aspecto importante es la representación de las palabras. Para captar el significado semántico de las palabras, utilizamos _embeddings_ de palabras, que son representaciones vectoriales de palabras en un espacio donde cada una de las muchas dimensiones representan una característica semántica.\n",
    "\n",
    "Esta vez, en lugar de entrenar nuestros _embeddings_ desde cero, aprovecharemos uno preentrenado, _Global Vectors for Word Representation_ (GLoVe), entrenado con un conjunto de datos de más de 6.000 millones de tokens. Cuenta con varios vectores de palabras preentrenados, por lo que los utilizaremos en <http://nlp.stanford.edu/data/glove.6B.zip>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'tmp/glove.6B.zip'\n",
    "\n",
    "# Download the compressed GloVe dataset (if you don't already have it)\n",
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print('Downloading ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Unzip it in the directory 'glove'.\n",
    "print('Unpacking ...', end='')\n",
    "unpack_archive(GLOVE_FILE, 'tmp')\n",
    "print('OK')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39fc8ea3",
   "metadata": {},
   "source": [
    "Mediante su uso podemos aprovechar la gran cantidad de conocimiento codificado en estos _embeddings_, lo que seguramente mejore (y mucho)el rendimiento de nuestro modelo lingüístico.\n",
    "\n",
    "Ahora carguemos el _embedding_ de la dimensión especificada en la configuración. El archivo se compone de líneas de tuplas, donde el primer elemento es la palabra (en texto) y el segundo es ese vector de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading GloVe {EMBEDDING_DIM}-d embedding... ', end='')\n",
    "word2vec = {}\n",
    "with open(f'tmp/glove.6B.{EMBEDDING_DIM}d.txt') as f:\n",
    "    for line in f:\n",
    "        word, vector = line.split(maxsplit=1)\n",
    "        word2vec[word] = np.fromstring(vector,'f', sep=' ')\n",
    "print(f'done ({len(word2vec)} word vectors loaded)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "Bueno, $400.000$ _tokens_ son bastantes. Como nuestro vocabulario es menor, vamos a crear una capa de incrustación más pequeña, del tamaño de nuestro vocabulario. Para ello, incluiremos en ésta sólo los vectores de las palabras que nos devolverá la capa `TextVectorization`.\n",
    "\n",
    "Comenzaremos creando la matriz de incrustación con los vectores del guante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3344464",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating embedding matrix with GloVe vectors... ', end='')\n",
    "\n",
    "# Our newly created embedding: a matrix of zeros\n",
    "embedding_matrix = np.zeros((MAX_VOCAB_SIZE + 2, EMBEDDING_DIM))\n",
    "\n",
    "ko_words = 0\n",
    "for i, word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    if word == '[UNK]':\n",
    "        # The second word is for an unknown token, in glove is 'unk'\n",
    "        word = 'unk'\n",
    "\n",
    "    # Get the word vector and overwrite the row in its corresponding position\n",
    "    word_vector = word2vec.get(word)\n",
    "    if word_vector is not None:\n",
    "        embedding_matrix[i] = word_vector\n",
    "    else:\n",
    "        ko_words += 1\n",
    "\n",
    "print(f'done ({ko_words} words unassigned)')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9f9bae0",
   "metadata": {},
   "source": [
    "Bueno, al parecer hay muchas palabras que no tienen correspondencia en el _embedding_ descargada. Después de todo $400000$ _tokens_ igual no eran tantos después de todo.\n",
    "\n",
    "Una vez hecho esto, podemos crear una capa de incrustación con la matriz de pesos precargada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=embedding_matrix.shape[0],\n",
    "    output_dim=embedding_matrix.shape[1],\n",
    "    weights=[embedding_matrix],\n",
    "    input_length=MAX_SEQUENCE_LEN,\n",
    "    trainable=False,\n",
    "    name='Embedding',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Clasificación basada en redes neuronales convolucionales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "Haremos una aproximación al problema utilizando redes convolucionales. En este caso, nuestras frases estarán representadas por «imágenes» de una sola fila, con tantas columnas como la longitud de la secuencia especificada y tantos canales como la dimensión de cada valabra de la secuencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50554551",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(1,), dtype=tf.string),\n",
    "    vectorize_layer,  # [[1, 2, 3, 4, 5], [6, 7, 8, 9, 0]]\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Conv1D(32, kernel_size=3, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(3, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['sparse_categorical_accuracy'],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4550806",
   "metadata": {},
   "source": [
    "Entrenemos al modelo y esperemos que todo salga bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda9aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2d17311",
   "metadata": {},
   "source": [
    "Echemos un vistazo al progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16b83b4",
   "metadata": {},
   "source": [
    "Veamos ahora cómo interpreta el sentimiento de una reseña buena, regular y mala extraída del sitio web de amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"My nephew is on the autism spectrum and likes to fidget with things so I knew this toy would be a hit. Was concerned that it may not be \\\"complex\\\" enough for his very advanced brain but he really took to it. Both him (14 yrs) and his little brother (8 yrs) both enjoyed playing with it throughout Christmas morning. I'm always happy when I can find them something unique and engaging.\"\n",
    "poor = \"I wasn't sure about this as it's really small. I bought it for my 9 year old grandson. I was ready to send it back but my daughter decided it was a good gift so I'm hoping he likes it. Seems expensive for the price though to me.\"\n",
    "evil = \"I just wanted to follow up to say that I reported this directly to the company and had no response. I have not gotten any response from my review. The level of customer service goes a long way when an item you purchase is defective and this company didn’t care to respond. No I am even more Leary about ordering anything from this company. I never asked for a refund or replacement since I am not able to return it. I’m just wanted to let them know that this was a high dollar item and I expected it to be a quality item. Very disappointed! I bought this for my grandson for Christmas. He loved it and played with it a lot. My daughter called to say that the stickers were peeling on the corners. I am not able to take it from my grandson because he is autistic and wouldn’t understand. I just wanted to warn others who are wanting to get this. Please know that this is a cool toy and it may not happen to yours so it is up to you.\"\n",
    "\n",
    "probabilities = model.predict([good, poor, evil], verbose=0)\n",
    "print(f'Good was classified as {np.argmax(probabilities[0])}')\n",
    "print(f'Poor was classified as {np.argmax(probabilities[1])}')\n",
    "print(f'Evil was classified as {np.argmax(probabilities[2])}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos demostrado cómo utilizar una red neuronal convolucional para clasificar texto, en concreto, reseñas de productos como buenas, medias o malas. También hemos analizado las ventajas de utilizar incrustaciones de palabras preentrenadas, que pueden ayudar a mejorar el rendimiento del modelo aprovechando las relaciones semánticas entre las palabras del corpus preentrenado. También exploramos la capa TextVectorization, que proporciona una forma flexible de preprocesar datos de texto y convertirlos en vectores numéricos.\n",
    "\n",
    "Se trata de un ejemplo que puede servir de punto de partida para multitud de proyectos de NLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
