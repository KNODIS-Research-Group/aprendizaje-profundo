{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><a href=\"https://knodis-research-group.github.io/\"><img style=\"float: right; width: 128px; vertical-align:middle\" src=\"https://knodis-research-group.github.io/knodis-logo_horizontal.png\" alt=\"KNODIS logo\" /></a>\n",
    "\n",
    "# Clasificación de texto con RNN<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-14</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "En un _notebook_ anterior exploramos la clasificación de textos con CNN, las cuales son adecuadas para capturar dependencias locales en datos de texto. Sin embargo, a veces necesitamos un modelo más potente que pueda capturar dependencias a largo plazo en los datos. Aquí es donde entran en juego las RNN.\n",
    "\n",
    "Las RNN están diseñadas específicamente para modelar datos secuenciales, lo que las hace ideales para tareas de clasificación de texto. A diferencia de las redes neuronales tradicionales, que procesan las entradas independientemente unas de otras, las RNN mantienen una memoria de las entradas anteriores y utilizan esta información para hacer predicciones sobre la entrada actual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a explorar cómo utilizar RNN para la clasificación de texto en el mismo problema que en el _notebook_ donde clasificábamos las reseñas de Amazon que los usuarios hicieron sobre los productos mediante CNN, pero esta vez con RNN.\n",
    "\n",
    "Veremos que en realidad los cambios son mínimos, ya que es poco más que cambiar una capa por otra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import re\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e76e288",
   "metadata": {},
   "source": [
    "Y crearemos los directorios necesarios en caso de que no se hayan creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c7f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_PATH = pathlib.Path('tmp')\n",
    "TEMP_PATH.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Parámetros comunes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "Mantendremos los mismos parámetros globales para poder comparar ambos métodos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 20           # Número de iteraciones de entrenamiento\n",
    "BATCH_SIZE = 32768      # Número de ejemplos por batch\n",
    "EMBEDDING_DIM = 50      # Dimensiones de nuestro embedding (50, 100, 200 o 300)\n",
    "MAX_VOCAB_SIZE = 20000  # Tamaño máximo de nuestro vocabulario\n",
    "MAX_SEQUENCE_LEN = 128  # Longitud máxima de las secuencias de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd945b",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aa0426",
   "metadata": {},
   "source": [
    "El proceso que llevaremos a cabo será el mismo que hicimos en el _notebook_ anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e5df57",
   "metadata": {},
   "source": [
    "### Descarga del _dataset_ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://github.com/blazaid/aprendizaje-profundo/raw/refs/heads/gh-pages/Datasets/Digital_Music_5.json.gz'\n",
    "DATASET = pathlib.Path('tmp/Digital_Music_5.json')\n",
    "\n",
    "print(f\"Downloading dataset to {DATASET.resolve()}\")\n",
    "if not DATASET.exists():\n",
    "    with requests.get(DATASET_URL, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        with gzip.GzipFile(fileobj=response.raw) as f_gz:\n",
    "            with DATASET.open(\"wb\") as f:\n",
    "                shutil.copyfileobj(f_gz, f)\n",
    "else:\n",
    "    print(\"File already exists! Nice\")\n",
    "\n",
    "print(\"Loading text corpus\")\n",
    "corpus = pd.read_json(DATASET, lines=True)\n",
    "corpus.dropna(subset=['overall', 'reviewText'], inplace=True)\n",
    "corpus.head()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcdc82",
   "metadata": {},
   "source": [
    "### ... preparación de las entradas y las salidas ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = corpus['reviewText'].astype(str).str.strip()\n",
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "num_classes = len(set(y_train))\n",
    "\n",
    "print(f\"Training input shape:  {x_train.shape}\")\n",
    "print(f\"Training output shape: {y_train.shape}\")\n",
    "print(f\"No. of classes:        {num_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0413fefc",
   "metadata": {},
   "source": [
    "### ... _tokenización_, _datasets_ y _dataloaders_ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3221f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokens = re.findall(r'\\b\\w+\\b|[^\\w\\s]', text, re.UNICODE)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "counter = collections.Counter()\n",
    "for text in x_train:\n",
    "    counter.update(tokenizer(text))\n",
    "\n",
    "\n",
    "special_tokens = ['<PAD>', '<UNK>']\n",
    "\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE - len(special_tokens))\n",
    "vocab_words = [word for word, _ in most_common]\n",
    "\n",
    "vocab = {token: i for i, token in enumerate(special_tokens)}\n",
    "for i, word in enumerate(vocab_words, start=len(special_tokens)):\n",
    "    vocab[word] = i\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f'Tamaño del vocabulario: {vocab_size}')\n",
    "\n",
    "\n",
    "def text_to_sequence(text, vocab, max_len=MAX_SEQUENCE_LEN):\n",
    "    tokens = tokenizer(text)\n",
    "    seq = [vocab.get(token, vocab['<UNK>']) for token in tokens]\n",
    "    if len(seq) > max_len:\n",
    "        seq = seq[:max_len]\n",
    "    else:\n",
    "        seq = seq + [vocab['<PAD>']] * (max_len - len(seq))\n",
    "    return seq\n",
    "\n",
    "\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len=MAX_SEQUENCE_LEN):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = text_to_sequence(self.texts[idx], self.vocab, self.max_len)\n",
    "        return torch.tensor(seq, dtype=torch.long), torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "\n",
    "\n",
    "dataset = TextDataset(x_train, y_train, vocab, MAX_SEQUENCE_LEN)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dac35d",
   "metadata": {},
   "source": [
    "### ... los _embeddings_ preentrenados de GloVe ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738d3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = pathlib.Path('tmp/glove.6B.zip')\n",
    "\n",
    "# Download the compressed GloVe dataset (if you don't already have it)\n",
    "if not GLOVE_FILE.exists():\n",
    "    print('Downloading GloVe ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Unzip it in the directory 'glove'.\n",
    "print('Unpacking ...', end='')\n",
    "shutil.unpack_archive(GLOVE_FILE, 'tmp')\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "### ... cogiendo los pesos de las `MAX_VOCAB_SIZE` palabras más comunes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Loading GloVe {EMBEDDING_DIM}-d embedding... ', end='')\n",
    "word2vec = {}\n",
    "with open(f'tmp/glove.6B.{EMBEDDING_DIM}d.txt') as f:\n",
    "    for line in f:\n",
    "        word, vector = line.split(maxsplit=1)\n",
    "        word2vec[word] = np.fromstring(vector,'f', sep=' ')\n",
    "print(f'done ({len(word2vec)} word vectors loaded)')\n",
    "\n",
    "print('Creating embedding matrix with GloVe vectors... ', end='')\n",
    "\n",
    "# Our newly created embedding: a matrix of zeros\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "\n",
    "ko_words = 0\n",
    "for word, idx in vocab.items():\n",
    "    word_glove = 'unk' if word == '<UNK>' else word\n",
    "    vector = word2vec.get(word_glove)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[idx] = vector\n",
    "    else:\n",
    "        ko_words += 1\n",
    "\n",
    "print(f'done ({ko_words} out of {len(vocab)} words unassigned)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Clasificación basada en redes neuronales recurrentes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "Y ahora, en lugar de utilizar CNN, utilizaremos RNN. En este caso, el conjunto de dimensiones lo realizan la capa `TextVectorization` (que convierte el texto en secuencias de enteros de longitud $T$) y la capa Embedding (que convierte cada entero en un vector de dimensiones $D$), convirtiendo la entrada en un tensor con la forma $N \\times T \\times D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5392767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifier(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_classes, embedding_matrix, bidirectional=True, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = torch.nn.Parameter(\n",
    "            torch.tensor(embedding_matrix),\n",
    "            requires_grad=False,\n",
    "        )\n",
    "        \n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,  # Si usamos bidireccional o no\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.fc = torch.nn.Linear(\n",
    "            hidden_dim * (2 if bidirectional else 1),   # ¡Ojo! Bidireccional\n",
    "                                                        #  dobla las entradas\n",
    "            num_classes,\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (B, seq_len, embedding_dim)\n",
    "        x = x.float()\n",
    "        out, _ = self.lstm(x)  # (B, seq_len, hidden_dim * embedding_dim)\n",
    "        out = out.transpose(1, 2)  # (B, hidden_dim * num_directions, seq_len)\n",
    "        # Max pooling adaptativo a lo largo del tiempo para un vector/muestra\n",
    "        pooled = torch.nn.functional.adaptive_max_pool1d(out, 1).squeeze(2)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits = self.fc(pooled)\n",
    "        return logits\n",
    "\n",
    "\n",
    "rnn_model = RNNClassifier(\n",
    "    vocab_size,\n",
    "    EMBEDDING_DIM,\n",
    "    hidden_dim=32,\n",
    "    num_layers=2,\n",
    "    num_classes=num_classes,\n",
    "    embedding_matrix=embedding_matrix,\n",
    "    bidirectional=True,\n",
    "    dropout=0.5,\n",
    ")\n",
    "print(rnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ee23e3",
   "metadata": {},
   "source": [
    "Aunque el número de parámetros es similar, aumentar el número de unidades en una unidad recurrente no aumenta mucho el número de parámetros en nuestro modelo. Sin embargo, sí que aumentará mucho el tiempo de entrenamiento. Por lo tanto, nuestro modelo no podrá obtener resultados tan buenos como los anteriores.\n",
    "\n",
    "Entrenemos el modelo y esperemos que todo vaya bien (otra vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd1f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = utils.train(\n",
    "    rnn_model,\n",
    "    train_loader,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(rnn_model.parameters()),\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=num_classes),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dade383c",
   "metadata": {},
   "source": [
    "Echemos un vistazo al progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103f8c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e83a",
   "metadata": {},
   "source": [
    "Veamos ahora cómo interpreta el sentimiento de una reseña buena, regular y mala extraída del sitio web de amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915a270e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = (\"My nephew is on the autism spectrum and likes to fidget with things so I knew this toy would be a hit. \"\n",
    "        \"Was concerned that it may not be \\\"complex\\\" enough for his very advanced brain but he really took to it. \"\n",
    "        \"Both him (14 yrs) and his little brother (8 yrs) both enjoyed playing with it throughout Christmas morning. \"\n",
    "        \"I'm always happy when I can find them something unique and engaging.\")\n",
    "\n",
    "poor = (\"I wasn't sure about this as it's really small. I bought it for my 9 year old grandson. \"\n",
    "        \"I was ready to send it back but my daughter decided it was a good gift so I'm hoping he likes it. \"\n",
    "        \"Seems expensive for the price though to me.\")\n",
    "\n",
    "evil = (\"I just wanted to follow up to say that I reported this directly to the company and had no response. \"\n",
    "        \"I have not gotten any response from my review. The level of customer service goes a long way when an item \"\n",
    "        \"you purchase is defective and this company didn’t care to respond. No I am even more Leary about ordering \"\n",
    "        \"anything from this company. I never asked for a refund or replacement since I am not able to return it. \"\n",
    "        \"I’m just wanted to let them know that this was a high dollar item and I expected it to be a quality item. \"\n",
    "        \"Very disappointed! I bought this for my grandson for Christmas. He loved it and played with it a lot. \"\n",
    "        \"My daughter called to say that the stickers were peeling on the corners. I am not able to take it from my \"\n",
    "        \"grandson because he is autistic and wouldn’t understand. I just wanted to warn others who are wanting to get \"\n",
    "        \"this. Please know that this is a cool toy and it may not happen to yours so it is up to you.\")\n",
    "\n",
    "def predict_text(text, model, vocab):\n",
    "    model.eval()\n",
    "    seq = text_to_sequence(text, vocab, MAX_SEQUENCE_LEN)\n",
    "    seq_tensor = torch.tensor(seq, dtype=torch.long).unsqueeze(0)  # crear batch de 1\n",
    "    with torch.no_grad():\n",
    "        logits = model(seq_tensor)\n",
    "        pred = logits.argmax(dim=1).item()\n",
    "    return pred\n",
    "\n",
    "print(f'Good was classified as {predict_text(good, cnn_model, vocab)}')\n",
    "print(f'Poor was classified as {predict_text(poor, cnn_model, vocab)}')\n",
    "print(f'Evil was classified as {predict_text(evil, cnn_model, vocab)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos estudiado cómo utilizar RNN para clasificar textos y las hemos comparado con las CNN. Aunque ambas arquitecturas pueden ser eficaces para la clasificación de textos, presentan algunas diferencias clave.\n",
    "\n",
    "La principal es las RNN son más adecuadas para captar las dependencias a largo plazo en los datos de texto, mientras que las CNN son más adecuadas para captar las dependencias locales. Esto hace que las RNN sean una buena opción para tareas como el análisis de sentimientos o la traducción de idiomas, donde el contexto de una palabra o frase puede ser crucial para determinar su significado.\n",
    "\n",
    "Sin embargo, el entrenamiento de las RNN puede ser más costoso que el de las CNN (ya hemos visto el tiempo que tarda una red pequeña en ser entrenada durante 5 _epochs_). Por otro lado, las CNN son más rápidas de entrenar y se adaptan mejor a conjuntos de datos más grandes. Son especialmente adecuadas para tareas como la clasificación de textos o el reconocimiento de imágenes, en las que las características locales son importantes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
