{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Creación y visualización de _embeddings_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-12</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los _embeddings_ son una técnica utilizada para representar datos de forma más compacta y significativa. En concreto, se utilizan para representar vectores de dimensiones muy grandes (por ejemplo, representaciones de palabras o imágenes) en espacios de mucha menor dimensión (normalmente unas pocas decenas o centenares de ellas).\n",
    "\n",
    "Son muy útiles en aplicaciones de aprendizaje automático relacionadas con el procesamiento del lenguaje natural. Por ejemplo, pueden utilizarse para representar palabras en un espacio vectorial, de modo que las palabras que tienen un significado similar se representen cerca unas de otras. Esto es útil en tareas como la traducción automática, en la que se busca la palabra en la lengua de destino que tenga el significado más parecido a la palabra en la lengua de origen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "En este cuaderno crearemos un Embedding y lo proyectaremos en un espacio bidimensional para probar cómo se pueden relacionar las palabras en un espacio vectorial en un contexto dado (problema).\n",
    "\n",
    "Entrenaremos este embedding para una tarea de clasificación típicamente utilizada para empezar a aprender NLP, la de análisis de sentimiento de los comentarios de IMDb. Sin embargo, nos quedaremos en el punto en el que se entrena el _embedding_, dejando el análisis de sentimiento para más adelante."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "\n",
    "plt.style.use('bmh')\n",
    "plt.rcParams.update({'figure.figsize': (16, 9),'figure.dpi': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, establecemos las constantes de los recursos comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8192\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQUENCE_LEN = 128\n",
    "TRAIN_EPOCHS = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "## Preparando nuestro conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el dataset [_Large Movie Review Dataset_](https://ai.stanford.edu/~amaas/data/sentiment/) para _sentiment analysis_ como fuente de datos. En este caso vamos a hacer uso de la biblioteca `datasets` de Hugging Face, que ofrece un interfaz muy cómodo para descargar conjuntos de datos del sitio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('imdb')\n",
    "# Primera frase de ejemplo\n",
    "text = dataset['train'][0]['text']\n",
    "label = dataset['train'][0]['label']\n",
    "print(f\"Train example: {text}--- -> {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ser los ejemplos de entrada cadenas de texto, tenemos que definir alguna forma de identificar los diferentes _tokens_ que los conforman. Para ello nos definiremos una función que convertirá las cadenas en sus _tokens_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_tokenizer(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "text_sentence = \"The best sentence of the world.\"\n",
    "tokens_sentence = text_tokenizer(text_sentence)\n",
    "print(f\"Sentence: {text_sentence} -> Tokens {tokens_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es el mejor _tokenizador_, pero nos hace el apaño. Ahora usaremos esta función para extraer todos los tokens que conforman todos los comentarios y su frecuencia absoluta. De esta manera, podremos determinar qué palabras son las más comunes y por tanto cuáles son las que más nos interesa conservar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = collections.Counter()\n",
    "for example in dataset['train']:\n",
    "    list_of_tokens = text_tokenizer(example['text'])\n",
    "    counter.update(list_of_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la lista de tokens se les añade normalmente una serie de tokens especiales que se usan para manejar situaciones particulares durante el procesamiento de texto. Nosotros añadiremos los típicos:\n",
    "\n",
    "- `<PAD>`: Se utiliza para _padding_, esto es, para rellenar secuencias de diferente longitud y poder procesarlas en batches de tamaño fijo, agregando este token hasta alcanzar una longitud deseada.\n",
    "- `<SOS>`: _Start Of Sentence_ (comienzo de la oración), el cual se usa para indicar explícitamente el comienzo de una secuencia. Es útil en la generación de texto o modelos de secuencia a secuencia (_seq2seq_).\n",
    "- `<UNK>`: Utilizado para los casos en los que encontramos una palabra que no está en nuestro vocabulario (_unknown_). Así, cualquier palabra no reconocida se mapea a este token.\n",
    "- `<UNU>`: Esta no es tan común, y sirve para reservar un índice en el vocabulario para posibles usos futuros o para mantener compatibilidad con algunos modelos o datasets que lo requieran (_unused_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_tokens = ['<PAD>', '<SOS>', '<UNK>', '<UNU>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, con todos los _tokens_ definidos, pasaremos a quedarnos con los que más nos interesen (en principio los más comunes). A estos tokens se les denominará «vocabulario», y será la lista de tokens que nuestros modelos usarán. Además, cada uno de los tokens llevará asociado un índice que represetará a dicho token en las entradas y salidas del modelo qué los usen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tomamos la lista de tokens más comunes, guardando sitio para los especiales\n",
    "most_common = counter.most_common(MAX_VOCAB_SIZE - len(special_tokens))\n",
    "\n",
    "# Metemos primero los tokens especiales y después los tokens más comunes\n",
    "vocabulary = {token: idx for idx, token in enumerate(special_tokens)}\n",
    "for idx, (token, count) in enumerate(most_common, start=len(special_tokens)):\n",
    "    vocabulary[token] = idx\n",
    "\n",
    "# Creamos también el diccionario inverso para mapear índices a palabras\n",
    "index_to_token = {idx: word for word, idx in vocabulary.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nuestras listas de tokens de entrada están compuestas de tokens, no de números, y los modelos requieren entradas numéricas (en nuestro caso enteros porque serán índices a vectores). Vamos a crear una función que pase de texto a lista de índices, usando para ello nuestra función previa, `text_tokenizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta función será usada por nuestra siguiente función, preprocess_dataset, que tomará el conjunto de entrada descargado y devolverá las listas de índices a tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vale, esto ya es otra cosa. Ahora, vamos a por el primer problema de todos. Para obtener una entrada de longitud fija, podemos simplemente truncar las reseñas a un número fijo de palabras, digamos 64. Para las reseñas de más de 64 palabras, conservaremos sólo las primeras 64 palabras. En el caso de las opiniones de más de 64 palabras, sólo conservaremos las 64 primeras. Para las reseñas más cortas, rellenaremos los huecos de palabras no utilizadas con el valor que hayamos asignado al relleno (el 0). Con keras, esto es fácil de hacer usando la función `pad_sequences`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(dataset, fn_tokenizer, vocabulary, max_len, with_sos=False):\n",
    "    UNK = vocabulary['<UNK>']\n",
    "    \n",
    "    data = []\n",
    "    for entry in dataset:\n",
    "        # 1. Tokenizamos el comentario, añadiendo <SOS> si es necesario\n",
    "        sequence = fn_tokenizer(entry['text'])\n",
    "        if with_sos:\n",
    "            sequence = ['<SOS>'] + sequence\n",
    "\n",
    "        # 2. Truncamos la secuencia si es larga o la rellenamos si es corta\n",
    "        sequence = sequence[:max_len]\n",
    "        if len(sequence) < max_len:\n",
    "            sequence = ['<PAD>'] * (max_len - len(sequence)) + sequence\n",
    "\n",
    "        # 3. Ya podemos pasar a índices. Los codificamos como long para ello\n",
    "        sequence = torch.tensor(\n",
    "            [vocabulary.get(token, UNK) for token in sequence],\n",
    "            dtype=torch.long,\n",
    "        )\n",
    "\n",
    "        # 4. Procesamos las salidas que, afortunadamente, ya vienen preparaditas\n",
    "        #    como 0 (negative) o 1 (positive). Las codificamos como float para\n",
    "        #    la pérdida\n",
    "        label = torch.tensor(example['label'], dtype=torch.float)\n",
    "        data.append((sequence, label))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos ahora los dataloaders a partir de un dataset personalizado, tal y como hemos hecho hasta el momento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data  # Lista de tuplas (secuencia, etiqueta)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = SentimentDataset(prepare_dataset(\n",
    "    dataset=dataset['train'],\n",
    "    fn_tokenizer=text_tokenizer,\n",
    "    vocabulary=vocabulary,\n",
    "    max_len=MAX_SEQUENCE_LEN,\n",
    "))\n",
    "test_dataset  = SentimentDataset(prepare_dataset(\n",
    "    dataset=dataset['test'],\n",
    "    fn_tokenizer=text_tokenizer,\n",
    "    vocabulary=vocabulary,\n",
    "    max_len=MAX_SEQUENCE_LEN,\n",
    "))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "## Usando la capa `Embedding`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch facilita **mucho** el trabajo con _embeddings_ gracias a la capa `torch.nn.Embedding`, que puede entenderse como una capa de tipo `Linear` que mapea desde índices enteros (los índices de las palabras concretas) a vectores de palabras (sus correspondientes _embeddings_).\n",
    "\n",
    "Los `Embeddings` requieren al menos dos parámetros; el primero (`num_embeddings`), el número de _embeddings_, indica cuántas palabras tendrá en cuenta nuestra capa. Cuanto mayor sea este parámetro, más palabras se podrán representar, pero más memoria ocupará y, sobre todo, más parámetros habrá que entrenar.\n",
    "\n",
    "El segundo parámetro (`embedding_dim`) es la dimensión de los vectores resultantes. Este parámetro indica el número de características que se almacenarán para cada palabra del _embedding_. Este es el parámetro con el que más se juega cuando se crea una incrustación desde 0 para resolver un problema, del mismo modo que se experimenta con el número de neuronas de una capa `Linear`.\n",
    "\n",
    "Por ejemplo, vamos a crear un _embedding_ con unas cuantas palabras y una dimensión concreta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": [
    "embedding_layer = torch.nn.Embedding(\n",
    "    num_embeddings=1000,  # Número máximo de tokens a representar\n",
    "    embedding_dim=5,      # Cuántas dimensiones representa cada ítem\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2dKKV1L2Rk7e"
   },
   "source": [
    "Cuando se crea una capa `Embedding`, sus pesos se inicializan aleatoriamente, como con cualquier otra capa. Y también como cualquier otra capa, dichos pesos se ajustarán gradualmente durante el entrenamiento.\n",
    "\n",
    "Una vez entrenados, cabe esperar que los vectores de palabras aprendidos codifiquen aproximadamente las similitudes entre palabras, ya que, después de todo, se aprendieron para el problema específico sobre el que se entrena el modelo.\n",
    "\n",
    "Podemos darle un tensor de varios _tokens_ (o un tensor de los mismos) y nos devolverá un tensor, esta vez con $n$ dimensiones más, correspondientes a las $n$ características que codifican los vectores de los embeddings de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "vwSYepRjyRGy",
    "outputId": "ccc67727-3756-42b3-bd5e-f35fdc26234d"
   },
   "outputs": [],
   "source": [
    "result = embedding_layer(torch.LongTensor([\n",
    "    [0, 1, 2, 3],\n",
    "    [4, 5, 6, 7],\n",
    "]))\n",
    "print(f'Result (shape = {result.shape}):\\n{result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "Podemos ver que si damos un _batch_ de secuencias como entrada, una capa `torch.nn.Embedding` devuelve un tensor 3D de tipo `float`, de la forma `(num_sequences, items_per_sequence, embedding_dim)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "## Entrenamiento del modelo de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos un modelo secuencial formado por un embedding, un par de capas recurrentes (para ver si son capaces de capturar alguna características, y una capa densa. Este modelo se entrenará para intentar dar respuesta a qué comentarios son negativos y cuáles positivos (clasificación binaria).\n",
    "\n",
    "Entre todos los pesos que se entrenen, estarán los del `Embedding`. Una vez finalizado el entrenamiento, nuestro `Embedding` tendrá una comprensión de las relaciones que existen entre nuestras palabras, al menos dentro de nuestro contexto de comentarios positivos o negativos a nuestras películas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx,  # El índice para el <PAD>. Por defecto 0.\n",
    "        )\n",
    "        # Capas recurrentes\n",
    "        self.lstm = torch.nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=3,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Capa lineal para la salida final\n",
    "        self.fc = torch.nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len)\n",
    "        embedded = self.embedding(x)  # (B, SEQ_LEN) -> (B, SEQ_LEN, EMBED_DIM)\n",
    "        out, _ = self.lstm(embedded)\n",
    "        out = out[:,-1,:]\n",
    "        out = self.fc(out)\n",
    "        out = torch.squeeze(out, 1)  # # (B, 1) -> (B,)\n",
    "        return out\n",
    "\n",
    "# Instanciar el modelo\n",
    "model = SentimentRNN(\n",
    "    vocab_size=len(vocabulary),\n",
    "    embed_dim=2,  # Así podemos visualizarlo en 2D\n",
    "    hidden_dim=8,\n",
    "    pad_idx=vocabulary['<PAD>'],\n",
    ")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "source": [
    "Y ahora entrenamos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "id": "5mQehiQyv8rP",
    "outputId": "2b0a8bac-739f-489c-cdd3-50e4b73770ed"
   },
   "outputs": [],
   "source": [
    "history = utils.train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.BCEWithLogitsLoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters()),\n",
    "    validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos la evolución del entrenamiento para comprobar que el modelo está aprendiendo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.xlabel('Loss and accuracy (log scale)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización del embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": [
    "Vamos a ver la representación que hace de ciertas palabras nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "n-j0db7ipF6k",
    "outputId": "90e34c4e-bc23-4828-e496-1c354c2bf1c7"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_seq, _ = test_dataset[0]\n",
    "    embedding_output = model.embedding(sample_seq)\n",
    "    embedding_output = embedding_output.squeeze(0).numpy()\n",
    "\n",
    "print(f\"Input shape:     {sample_seq.numpy().shape}\")\n",
    "print(f\"Embedding shape: {embedding_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que, dada una secuencia, la salida de nuestro _embedding_ es una lista de un único tensor con la secuencia de salida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(embedding_output[:, 0], embedding_output[:, 1])\n",
    "plt.xlabel('Unknown dimension 1')\n",
    "plt.ylabel('Unknown dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un ojo a los vectores de los pesos asociados a determinadas palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = model.embedding.weight.detach().numpy()\n",
    "df_weights = pd.DataFrame(\n",
    "    embedding_weights,\n",
    "    index=[index_to_token.get(i, '<UNK>') for i in range(len(vocabulary))]\n",
    ")\n",
    "\n",
    "df_weights.loc[['reeves', 'seagal', 'curtis', 'megan', 'horrible', 'man', 'woman'],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "Vamos a probar una cosa. Crearemos una reseña ficticia y veremos cómo se distribuyen las palabras en el espacio bidimensional que hemos creado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "GsjempweP9Lq",
    "outputId": "a5ba6d7a-df50-4e02-d321-303c73f85cc5"
   },
   "outputs": [],
   "source": [
    "review = ['cage', 'reeves', 'seagal', 'cavill', 'good', 'matrix', 'bad', 'crap',\n",
    "          'affleck', 'man', 'woman', 'king', 'queen', 'curtis', 'megan']\n",
    "\n",
    "encoded_review = [vocabulary.get(word, vocabulary['<UNK>']) for word in review]\n",
    "encoded_review_tensor = torch.tensor(encoded_review, dtype=torch.long)\n",
    "print(\"Reseña codificada:\", encoded_review_tensor.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora veamos la salida de `Embedding` si le damos a revisar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los vectores de embedding para cada palabra de la reseña\n",
    "with torch.no_grad():\n",
    "    review_embeddings = model.embedding(encoded_review_tensor.unsqueeze(0))  # (1, len(review), embed_dim)\n",
    "    review_embeddings = review_embeddings.squeeze(0).numpy()  # (len(review), embed_dim)\n",
    "\n",
    "# Graficar los embeddings de la reseña y anotar cada punto con su palabra\n",
    "plt.scatter(review_embeddings[:, 0], review_embeddings[:, 1])\n",
    "for i, word in enumerate(review):\n",
    "    plt.text(review_embeddings[i, 0], review_embeddings[i, 1], word)\n",
    "plt.xlabel('Unknown dimension 1')\n",
    "plt.ylabel('Unknown dimension 2')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval = utils.evaluate(\n",
    "    model=model,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.BCEWithLogitsLoss(),\n",
    "    metric_fn=torchmetrics.classification.BinaryAccuracy(),\n",
    ")\n",
    "print(f'Results -> Loss: {eval.get(\"loss\")}, Acc: {eval.get(\"metric\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que, en principio, nuestro `embedding` ha localizado espacialmente las palabras que están más relacionadas. De este modo, podemos identificar qué actores están más cerca de qué calificadores en función de su proximidad a ellos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyMZWyxYjMr"
   },
   "source": [
    "Hemos creado una incrustación a partir de ciertos datos y demostrado cómo puede crear una representación en un espacio vectorial de las palabras incluidas en ella. Esta representación permite agrupar en el espacio vectorial palabras que tienen significados similares, lo que puede ser útil para tareas de análisis de textos como la clasificación de documentos o la búsqueda de información. También hemos mostrado cómo la incrustación puede utilizarse para realizar tareas como la identificación de palabras desconocidas y la detección de palabras mal escritas.\n",
    "\n",
    "En general, el uso de incrustaciones para representar datos de texto es una técnica valiosa en el aprendizaje automático y el procesamiento del lenguaje natural. Permiten representar las palabras de forma más significativa en un espacio vectorial, lo que facilita la identificación de patrones y la realización de tareas de clasificación y predicción. Además, su capacidad para agrupar palabras similares en un espacio vectorial puede mejorar significativamente la precisión y la eficacia de los modelos de análisis de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
