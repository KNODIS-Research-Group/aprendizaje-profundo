{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div><a href=\"https://knodis-research-group.github.io/\"><img style=\"float: right; width: 128px; vertical-align:middle\" src=\"https://knodis-research-group.github.io/knodis-logo_horizontal.png\" alt=\"KNODIS logo\" /></a>\n",
    "\n",
    "# Predicción de casos diarios de COVID-19<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-14</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Nuevo Coronavirus (COVID-19)\n",
    "\n",
    "La Covid-19 fue una enfermedad respiratoria muy infecciosa que se extendió por todo el planeta desde finales de 2019. A mediados de 2023, la el número adcumulado de casos de casos de coronavirus es una enfermedad se ha extendido rápidamente por todo el mundo. En el periodo de estudio del 22 de enero de 2020 al 2 de agosto de 2023 [se registraron en el mundo alrededor de 769 millones de casos de SARS-CoV-2](https://es.statista.com/estadisticas/1104227/numero-acumulado-de-casos-de-coronavirus-covid-19-en-el-mundo-enero-marzo/).\n",
    "\n",
    "### ¿Cómo de peligroso era?\n",
    "\n",
    "Pues era una enfermedad muy contagiosa, pero no era especialmente letal. En [worldometers](https://www.worldometers.info/coronavirus/worldwide-graphs/) existe un listado de indicadores con la progresión desde primcippios de 2020 hasta el 13 de abril de 2024 que dejaron de actualizarlo. En general, las personas mayores y las personas con enfermedades crónicas graves tenían más probabilidades de morir a causa de la Covid-19, por ello era importante proteger a estos grupos de riesgo, [aunque en algunas capitales de España no se pensase lo mismo](https://www.imdb.com/es/title/tt35097841/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un modelo de regresión para predecir el número de casos diarios de Covid-19 a partir de datos históricos, utilizando redes neuronales recurrentes. Concretamente:\n",
    "\n",
    "- Manipularemos datos de series temporales para adecuarlos a nuestros modelos.\n",
    "- Implementaremos y entrenaremos modelos de regresión basados en `RNN`, `LSTM` y `GRU`.\n",
    "- Los compararemos y evaluaremos sy desempeño utilizando métricas específicas para series temporales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación importaremos las librerías que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import torch\n",
    "import torchmetrics\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último, establecemos las constantes de los recursos comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8192\n",
    "TRAIN_EPOCHS = 256\n",
    "SEQUENCE_LEN = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descargaremos todo el dataset, que se encuentra alojado en el repositorio oficial del CSSE de Johns Hopkins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19\"\n",
    "dataset = \"csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
    "df = pd.read_csv(f\"{repo}/refs/heads/master/{dataset}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo vamos a prescindir de la información relacionada con la localización (las cuatro primeras columnas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:, 4:]  # Off with the first 4 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " También, dado que lo que queremos predecir es el número de casos diarios y que la información que aparece es el acumulado, debemos calcular la diferencia entre días consecutivos. Para ello primero debemos asegurarnos de que no hay valores nulos en el dataset. Así que comprobaremos que no los hay y sacaremos los valores acumulados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df.isnull().sum().sum() == 0  # No null values, please\n",
    "\n",
    "serie = df.sum(axis=0)\n",
    "serie.index = pd.to_datetime(serie.index, format='mixed')\n",
    "print(serie.head())\n",
    "serie.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora, este conjunto de datos acumulados por día se lo restaremos a nuestro conjunto de datos para que nos quede la diferencia entre días consecutivos, que es lo que queremos predecir. Nos quedamos con el primer valor de la  secuencia porque no tenemos el valor anterior para calcular la diferencia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serie = serie.diff().fillna(serie[0]).astype(np.int64)\n",
    "print(serie.head())\n",
    "serie.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ya tenemos la serie temporal con la que vamos a trabajar. Ahora vamos a dividir los datos en entrenamiento y test. Para ello vamos a utilizar los datos de los primeros $80\\%$ días para entrenar y el resto para test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(len(serie) * .2)\n",
    "\n",
    "train_data = serie[:-test_size]\n",
    "test_data  = serie[-test_size:]\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape:  {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos datos los estandarizaremos para que tengan media $0$ y desviación típica $1$. Para ello utilizaremos `StandardScaler` de `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "scaler = scaler.fit(train_data.to_numpy().reshape(-1, 1))\n",
    "\n",
    "train_data = scaler.transform(train_data.to_numpy().reshape(-1, 1))\n",
    "test_data = scaler.transform(test_data.to_numpy().reshape(-1, 1))\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Test data shape:  {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, toda la secuencia será transformada en secuencias de longitud `n_steps` para que podamos entrenar nuestros modelos. Para ello utilizaremos la función `split_sequence` que definiremos a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequence(data, n_steps):\n",
    "    X, y = [], []\n",
    "\n",
    "    for i in range(len(data) - n_steps):\n",
    "        X.append(data[i:i + n_steps])\n",
    "        y.append(data[i + n_steps])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "x_train, y_train = split_sequence(data=train_data, n_steps=SEQUENCE_LEN)\n",
    "x_test, y_test = split_sequence(data=test_data, n_steps=SEQUENCE_LEN)\n",
    "\n",
    "print(f\"Train data shape -> input: {x_train.shape}, output: {y_train.shape}\")\n",
    "print(f\"Test data shape  -> input: {x_test.shape}, output: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para finalizar, trabajaremos similar a  como trabajábamos en el resto de ejemplos, así que crearemos una subclase de `torch.utils.data.Dataset` iterar sobre los datos de entrenamiento y test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]\n",
    "\n",
    "train_dataset = TimeSeriesDataset(x_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de los modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Ahora pasaremos a definir nuestros modelos. Concretamente, implementaremos tres modelos basados en redes neuronales recurrentes: `RNN`, `LSTM` y `GRU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(torch.nn.Module):\n",
    "    def __init__(self, n_in, n_hid, n_layers, n_out, rnn_cls, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.rnn = rnn_cls(\n",
    "            input_size=n_in,\n",
    "            hidden_size=n_hid,\n",
    "            num_layers=n_layers,  # Esto es otra forma de apilar varias RNN\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc = torch.nn.Linear(in_features=n_hid, out_features=n_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "input_features = 1\n",
    "hidden_units = 16\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "rnn_model = RNNModel(\n",
    "    n_in=input_features,\n",
    "    n_hid=hidden_units,\n",
    "    n_layers=num_layers,\n",
    "    n_out=output_size,\n",
    "    rnn_cls=torch.nn.RNN,\n",
    ")\n",
    "\n",
    "gru_model = RNNModel(\n",
    "    n_in=input_features,\n",
    "    n_hid=hidden_units,\n",
    "    n_layers=num_layers,\n",
    "    n_out=output_size,\n",
    "    rnn_cls=torch.nn.GRU,\n",
    ")\n",
    "\n",
    "lstm_model = RNNModel(\n",
    "    n_in=input_features,\n",
    "    n_hid=hidden_units,\n",
    "    n_layers=num_layers,\n",
    "    n_out=output_size,\n",
    "    rnn_cls=torch.nn.LSTM,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora, entrenaremos el modelo con los datos de entrenamiento que hemos preparado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training RNN model ... \", end=\"\")\n",
    "history_rnn = utils.train(\n",
    "    model=rnn_model,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam(rnn_model.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.MeanAbsoluteError(),\n",
    "    verbose=False,\n",
    ")\n",
    "print(\"Results:\")\n",
    "avg_trn_loss = np.mean(history_rnn.get(\"train_loss\"))\n",
    "avg_val_loss = np.mean(history_rnn.get(\"val_loss\"))\n",
    "avg_trn_metr = scaler.inverse_transform([[    # Los datos vienen estandarizados,\n",
    "    np.mean(history_rnn.get(\"train_metric\"))  #  así que tenemos que pasarlos a\n",
    "]])                                           #  su escala original\n",
    "avg_val_metr = scaler.inverse_transform([[\n",
    "    np.mean(history_rnn.get(\"val_metric\"))\n",
    "]])\n",
    "print(f\"- Train loss: {avg_trn_loss} - Validation loss: {avg_val_loss}\")\n",
    "print(f\"- Train metric: {avg_trn_metr} - Validation metric: {avg_val_metr}\")\n",
    "\n",
    "print(\"Training GRU model ...\", end=\"\")\n",
    "history_gru = utils.train(\n",
    "    model=gru_model,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam(gru_model.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.MeanAbsoluteError(),\n",
    "    verbose=False,\n",
    ")\n",
    "print(\"Results:\")\n",
    "avg_trn_loss = np.mean(history_gru.get(\"train_loss\"))\n",
    "avg_val_loss = np.mean(history_gru.get(\"val_loss\"))\n",
    "avg_trn_metr = scaler.inverse_transform([[\n",
    "    np.mean(history_gru.get(\"train_metric\"))\n",
    "]])\n",
    "avg_val_metr = scaler.inverse_transform([[\n",
    "    np.mean(history_gru.get(\"val_metric\"))\n",
    "]])\n",
    "print(f\"- Train loss: {avg_trn_loss} - Validation loss: {avg_val_loss}\")\n",
    "print(f\"- Train metric: {avg_trn_metr} - Validation metric: {avg_val_metr}\")\n",
    "\n",
    "print(\"Training LSTM model ...\", end=\"\")\n",
    "history_lstm = utils.train(\n",
    "    model=lstm_model,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    optimizer=torch.optim.Adam(lstm_model.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.MeanAbsoluteError(),\n",
    "    verbose=False,\n",
    ")\n",
    "print(\" OK\")\n",
    "print(\"Results for RNN:\")\n",
    "avg_trn_loss = np.mean(history_lstm.get(\"train_loss\"))\n",
    "avg_val_loss = np.mean(history_lstm.get(\"val_loss\"))\n",
    "avg_trn_metr = scaler.inverse_transform([[\n",
    "    np.mean(history_lstm.get(\"train_metric\"))\n",
    "]])\n",
    "avg_val_metr = scaler.inverse_transform([[\n",
    "    np.mean(history_lstm.get(\"val_metric\"))\n",
    "]])\n",
    "print(f\"- Train loss: {avg_trn_loss} - Validation loss: {avg_val_loss}\")\n",
    "print(f\"- Train metric: {avg_trn_metr} - Validation metric: {avg_val_metr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a la evolución del entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_rnn).plot(\n",
    "    title='RNN model',\n",
    "    subplots=[('train_loss', 'val_loss'), ('train_metric', 'val_metric')],\n",
    "    layout=(1, 2),\n",
    ")\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(history_gru).plot(\n",
    "    title='GRU model',\n",
    "    subplots=[('train_loss', 'val_loss'), ('train_metric', 'val_metric')],\n",
    "    layout=(1, 2),\n",
    ")\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()\n",
    "\n",
    "pd.DataFrame(history_lstm).plot(\n",
    "    title='LSTM model',\n",
    "    subplots=[('train_loss', 'val_loss'), ('train_metric', 'val_metric')],\n",
    "    layout=(1, 2),\n",
    ")\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras el entrenamiento, ya podemos evaluar el rendimiento de cada modelo sobre el conjunto de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating RNN model ...\", end=\"\")\n",
    "eval_rnn = utils.evaluate(\n",
    "    model=rnn_model,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    metric_fn=torchmetrics.MeanAbsoluteError(),\n",
    ")\n",
    "print(\" OK\")\n",
    "print(\"Evaluating GRU model ...\", end=\"\")\n",
    "eval_gru = utils.evaluate(\n",
    "    model=gru_model,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    metric_fn=torchmetrics.MeanAbsoluteError(),\n",
    ")\n",
    "print(\" OK\")\n",
    "print(\"Evaluating LSTM model ...\", end=\"\")\n",
    "eval_lstm = utils.evaluate(\n",
    "    model=lstm_model,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.MSELoss(),\n",
    "    metric_fn=torchmetrics.MeanAbsoluteError(),\n",
    ")\n",
    "print(\" OK\")\n",
    "\n",
    "print(f'Results for RNN  -> Loss: {eval_rnn.get(\"loss\")}, metric: {scaler.inverse_transform([[eval_rnn.get(\"metric\")]])}')\n",
    "print(f'Results for GRU  -> Loss: {eval_gru.get(\"loss\")}, metric: {scaler.inverse_transform([[eval_gru.get(\"metric\")]])}')\n",
    "print(f'Results for LSTM -> Loss: {eval_lstm.get(\"loss\")}, metric: {scaler.inverse_transform([[eval_lstm.get(\"metric\")]])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al menos no es un comportamiento como el anterior. Pero sigue siendo inútil. Aparentemente no vamos a hacernos ricos, al menos no así."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook hemos desarrollado y entrenado tres modelos de redes neuronales recurrentes utilizando PyTorch, con el objetivo de predecir el número diario de casos confirmados de COVID-19. Hemos visto cómo:\n",
    "\n",
    "- Se pueden preprocesar y transformar datos reales de series temporales para adaptarlos al modelo,\n",
    "- Es posible implementar distintos modelos en PyTorch con el mismo conjunto de datos, y\n",
    "\n",
    "Aunque la predicción de fenómenos epidemiológicos es un desafío debido a la complejidad y la naturaleza cambiante de los datos, al menos este ejercicio nos ha ayudado a la entender de qué manera se puede trabajar con este tipo de modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
