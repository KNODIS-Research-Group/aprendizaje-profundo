{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><a href=\"https://knodis-research-group.github.io/\"><img style=\"float: right; width: 128px; vertical-align:middle\" src=\"https://knodis-research-group.github.io/knodis-logo_horizontal.png\" alt=\"KNODIS logo\" /></a>\n",
    "\n",
    "# Regularización _dropout_<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-14</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7290d50",
   "metadata": {},
   "source": [
    "Vamos a reflexionar un poco sobre lo que se espera de un buen modelo predictivo: queremos que tenga un buen rendimiento con datos nunca vistos durante el entrenamientose desempeñe bien con datos no vistos. La teoría clásica de la generalización siempre ha sugerido que, para tener modelos más generalistas, debemos apuntar hacia modelos lo más simples posible.\n",
    "\n",
    "Sin embargo, en 2014, [Srivastava et al.](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf) propusieron un nuevo punto de vista para generalizar. Curiosamente, la analogía fue hecha por ellos con la reproducción sexual. Los autores argumentaron que el sobreajuste de las redes neuronales se caracteriza por un estado en el cual cada capa depende de un patrón específico de activaciones de la capa anterior, a lo cual llaman coadaptación. El _dropout_, según ellos, rompe esta coadaptación de la misma manera que la reproducción sexual rompe los genes coadaptados.\n",
    "\n",
    "El **_dropout_** es una técnica de regularización utilizada para prevenir el sobreajuste en las redes neuronales. Durante el entrenamiento, implica «apagar» algunos neuronas seleccionadas al azar en la red en cada iteración. Esto fuerza a la red a **que todas las neuronas aprendan de todos los ejemplos**, haciéndola más robusta y menos propensa al sobreajuste.\n",
    "\n",
    "<center>\n",
    "<figure class=\"image\">\n",
    "    <img src=\"https://blazaid.github.io/aprendizaje-profundo/Slides/images/dropout.png\" alt=\"Esquema de un MLP antes y después del Dropout\" />\n",
    "    <figcaption><em><strong>Figura 1.</strong>Esquema de un MLP antes y después del Dropout.</em></figcaption>\n",
    "</figure>\n",
    "</center>\n",
    "\n",
    "Lo normal es que una vez terminado el entrenamiento el _dropout_ se desactive, ya que la idea es que reparta el conocimiento a través de todas las neuronas durante el entrenamiento. No obstante, hay algunas excepciones: algunos autores utilizan el _dropout_ también durante el cálculo de la precisión con el conjunto de prueba como una heurística para la incertidumbre de las predicciones de la red neuronal. Es decir, si las predicciones coinciden en muchas diferentes permutaciones de _dropout_, entonces podríamos afirmar con cierta confianza que nuestro modelo es más robusto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe2634",
   "metadata": {},
   "source": [
    "El propósito será implementar un modelo de perceptrón multicapa bastante grande, que sea capaz de clasificar los datos en el conjunto de datos mnist, pero que haya sido regularizado con _dropout_ para intentar asegurar que su potencia no afecte su capacidad de generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bee9a8",
   "metadata": {},
   "source": [
    "A continuación, importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1919ce2-dcb8-4602-8715-4520afc778f6",
   "metadata": {},
   "source": [
    "Por último, establecemos las constantes de los recursos comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c75c6c-3a1e-49a9-a1ac-cb365f283b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = './tmp'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec0b7",
   "metadata": {},
   "source": [
    "## Preparación de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c37e7e",
   "metadata": {},
   "source": [
    "Usaremos el conjunto de datos `mnist` que hemos usado previamente y que nos viene de maravilla para este propósito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "validation_set = torchvision.datasets.MNIST(\n",
    "    root=DATASETS_DIR,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform = torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_set,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    ")\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1e618",
   "metadata": {},
   "source": [
    "## Usando dropout en PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7362f3",
   "metadata": {},
   "source": [
    "La verdad es que es bastante fácil. Todo lo que tenemos que hacer es añadir una capa `Dropout` después de la capa conectada a la cual queremos aplicar la regularización. El único parámetro que hay que pasar al constructor es la probabilidad de que una neurona sea o no desactivada.\n",
    "\n",
    "Durante el entrenamiento, esta capa eliminará aleatoriamente las salidas de la capa anterior (y por lo tanto las entradas de la siguiente capa) de acuerdo con la probabilidad especificada. Cuando no está en modo de entrenamiento,  simplemente pasa los datos sin modificar.\n",
    "\n",
    "Veamos primero cómo funcionarían dos modelos similares para el problema de `mnist`. Primero uno sin capa `Dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a5841c-92b5-4324-be22-19ec70544b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_dropout = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.LazyLinear(out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LazyLinear(out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.LazyLinear(out_features=10),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_no_dropout.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028c54e-12b1-4717-8be9-971dc3d130a3",
   "metadata": {},
   "source": [
    "Ahora entrenemos el modelo. Esto igual lleva un rato; después de todo es un modelo bastante grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675def2-5dc6-4bd8-be3c-7339c7a4e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "}\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Entrenamiento\n",
    "    model_no_dropout.train()  # Ponemos el modelo en modo entrenamiento\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_no_dropout(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  # accumulate loss\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "\n",
    "    # Validación\n",
    "    model_no_dropout.eval()  # Ponemos el modelo en modo evaluación\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            outputs = model_no_dropout(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(validation_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_accuracy'].append(train_accuracy)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84da043-8099-4994-bc21-2a3d21e3dadb",
   "metadata": {},
   "source": [
    "Veamos la evolución del entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a449a7",
   "metadata": {},
   "source": [
    "Y ahora uno con _dropout_. En Keras, implementar el dropout es sencillo. Se utiliza una capa especial llamada `Dropout`, que se inserta entre las capas consecutivas de la red neuronal. La capa Dropout toma un argumento llamado `rate`, que especifica la fracción de neuronas que se desactivarán en cada iteración. Aquí hay un ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3f8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.LazyLinear(out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.75),\n",
    "    torch.nn.LazyLinear(out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(0.75),\n",
    "    torch.nn.LazyLinear(out_features=10),\n",
    "    torch.nn.ReLU(),\n",
    ")\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model_dropout.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba6af6",
   "metadata": {},
   "source": [
    "En este ejemplo, hemos añadido dos capas de `Dropout` con una tasa del $50\\%$. Esto significa que la mitad de las neuronas se desactivarán aleatoriamente durante cada _batch_ (no _apoch). Esto es un valor bastante común, pero no es el único. En general, el valor de la tasa de _dropout_ es un hiperparámetro que se puede ajustar para obtener el mejor rendimiento del modelo.\n",
    "\n",
    "Entrenemos el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7162c-d1da-4936-ae4a-15e4d3e898ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_accuracy': [],\n",
    "    'val_loss': [],\n",
    "    'val_accuracy': [],\n",
    "}\n",
    "\n",
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Entrenamiento\n",
    "    model_dropout.train()  # Ponemos el modelo en modo entrenamiento\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_dropout(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  # accumulate loss\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "\n",
    "    # Validación\n",
    "    model_dropout.eval()  # Ponemos el modelo en modo evaluación\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in validation_loader:\n",
    "            outputs = model_dropout(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_loss /= len(validation_loader)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_accuracy'].append(train_accuracy)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_accuracy'].append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch} - \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f82ec-48d3-4ebd-8f25-b31c4109bb69",
   "metadata": {},
   "source": [
    "Y veamos también su evolución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4206a652-c72a-4a39-a559-00a95cd62a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760629b9",
   "metadata": {},
   "source": [
    "Comparemos los valores obtenidos de los conjuntos de ambos modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11176cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, batch_size=64):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == targets).sum().item()\n",
    "            total += inputs.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / total\n",
    "    accuracy = correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "print('Without dropout:')\n",
    "train_loss_1, train_accuracy_1 = evaluate_model(model_no_dropout, train_loader)\n",
    "test_loss_1, test_accuracy_1 = evaluate_model(model_no_dropout, validation_loader)\n",
    "print(f'\\tLoss     -> Train: {train_loss_1}, Test: {test_loss_1}')\n",
    "print(f'\\tAccuracy -> Train: {train_accuracy_1}, Test: {test_accuracy_1}')\n",
    "\n",
    "print('With dropout:')\n",
    "train_loss_2, train_accuracy_2 = evaluate_model(model_dropout, train_loader)\n",
    "test_loss_2, test_accuracy_2 = evaluate_model(model_dropout, validation_loader)\n",
    "print(f'\\tLoss     -> Train: {train_loss_2}, Test: {test_loss_2}')\n",
    "print(f'\\tAccuracy -> Train: {train_accuracy_2}, Test: {test_accuracy_2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a7d2f",
   "metadata": {},
   "source": [
    "El _dropout_ funciona forzando a la red neuronal a aprender características redundantes en distintos subconjuntos de neuronas. Durante el entrenamiento, la red neuronal se ve obligada a aprender características redundantes en diferentes subconjuntos de neuronas, lo que la hace más robusta y menos propensa al sobreajuste. La tendencia general observada es que en los modelos muy potentes el conocimiento tiende a repartirse entre todas las conexiones del modelo, lo que hace que se mitigue la sobreespecialización y aumente la generalización."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "Hemos realizado una comparación entre dos modelos, uno sin _dropout_ y otro con él, y hemos demostrado que el uso del _dropout_ puede mejorar significativamente la capacidad predictiva de los modelos. Además, hemos explicado cómo funciona el _dropout_ y cómo se puede ajustar su hiperparámetro para optimizar su rendimiento.\n",
    "\n",
    "El _dropout_ es una técnica muy eficaz para evitar el sobreajuste en las redes neuronales. Consiste en apagar aleatoriamente algunas neuronas de la red durante cada iteración de entrenamiento, forzando a la red a aprender características redundantes a través de diferentes subconjuntos de neuronas. En Keras, la implementación de dropout es sencilla utilizando la capa `Dropout`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
