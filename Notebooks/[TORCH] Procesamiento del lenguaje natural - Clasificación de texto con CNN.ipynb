{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4480ff39",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Clasificación de texto con CNN<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2024-03-11</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2489222",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e17030ba",
   "metadata": {},
   "source": [
    "En el procesamiento del lenguaje natural (NLP, del inglés natural language processing), una tarea muy típica es la clasificación de textos. En esta tarea, un texto dado se clasifica según su significado. A menudo se utiliza, por ejemplo, para el problema del análisis de sentimientos.\n",
    "\n",
    "Se trata de un problema denominado _many-to-one_, es decir, uno en el que el tamaño de la secuencia de entrada es $T_X = 1$, pero el tamaño de la secuencia de salida es $T_Y = 1$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc7d1e17",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0b9e324",
   "metadata": {},
   "source": [
    "Vamos a hacer un experimento en el que utilizaremos el conjunto de datos de reseñas de amazon para una tarea de análisis de sentimiento. A partir de los datos de reseñas y valoraciones, identificaremos si una reseña es positiva, neutra o negativa, y para ello utilizaremos una primera aproximación utilizando un modelo de redes neuronales convolucionales."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5d401eac",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa528aec",
   "metadata": {},
   "source": [
    "A continuación importaremos las bibliotecas que se utilizarán a lo largo del cuaderno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a8c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/blazaid/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n",
      "/home/blazaid/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/__init__.py:7: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\\n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \\n\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "/home/blazaid/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader, Dataset\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_vocab_from_iterator\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, classification_report\n",
      "File \u001b[0;32m~/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     _WARN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# the following import has to happen first in order to load the torchtext C++ library\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _extension  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     20\u001b[0m _TEXT_BUCKET \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://download.pytorch.org/models/text/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     22\u001b[0m _CACHE_DIR \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_get_torch_home(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/_extension.py:64\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m \u001b[43m_init_extension\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/_extension.py:58\u001b[0m, in \u001b[0;36m_init_extension\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _mod_utils\u001b[38;5;241m.\u001b[39mis_module_available(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext._torchtext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchtext C++ Extension is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[43m_load_lib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibtorchtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# This import is for initializing the methods registered via PyBind11\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# This has to happen after the base library is loaded\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _torchtext\n",
      "File \u001b[0;32m~/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/_extension.py:50\u001b[0m, in \u001b[0;36m_load_lib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torch/_ops.py:1357\u001b[0m, in \u001b[0;36m_Ops.load_library\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m   1352\u001b[0m path \u001b[38;5;241m=\u001b[39m _utils_internal\u001b[38;5;241m.\u001b[39mresolve_library_path(path)\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dl_open_guard():\n\u001b[1;32m   1354\u001b[0m     \u001b[38;5;66;03m# Import the shared library into the process, thus running its\u001b[39;00m\n\u001b[1;32m   1355\u001b[0m     \u001b[38;5;66;03m# static (global) initialization code in order to register custom\u001b[39;00m\n\u001b[1;32m   1356\u001b[0m     \u001b[38;5;66;03m# operators with the JIT.\u001b[39;00m\n\u001b[0;32m-> 1357\u001b[0m     \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloaded_libraries\u001b[38;5;241m.\u001b[39madd(path)\n",
      "File \u001b[0;32m/usr/lib/python3.12/ctypes/__init__.py:379\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[0;31mOSError\u001b[0m: /home/blazaid/Projects/aprendizaje-profundo/.venv/lib/python3.12/site-packages/torchtext/lib/libtorchtext.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSs"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from shutil import unpack_archive\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad1b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_device()\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ff3a5f84",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para adaptar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d522d503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ea7481e",
   "metadata": {},
   "source": [
    "Y crearemos los directorios necesarios en caso de que no se hayan creado previamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64979a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ee4fec89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6d22a2c",
   "metadata": {},
   "source": [
    "## Parámetros globales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "231d1a4e",
   "metadata": {},
   "source": [
    "Comenzaremos definiendo los parámetros que utilizaremos a lo largo del cuaderno, que consistirán en la longitud máxima de las secuencias (recuerda que deben tener una longitud fija) y la dimensión del vector de cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a1ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "MAX_VOCAB_SIZE = 16384\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2\n",
    "GLOVE_URL = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "GLOVE_FILE = 'tmp/glove.6B.zip'\n",
    "GLOVE_PATH = \"tmp/glove.6B.300d.txt\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e86b03b1",
   "metadata": {},
   "source": [
    "## Preprocesamiento de datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b1ff85c",
   "metadata": {},
   "source": [
    "Vamos a cargar los datos de entrenamiento, que consistirán en los datos de reseñas de Amazon de la categoría «Música digital» (https://nijianmo.github.io/amazon/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564710a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Digital_Music_5.json.gz'\n",
    "DATASET_ZIP = 'tmp/Digital_Music_5.json.gz'\n",
    "\n",
    "# Download the remote file if it does not exist\n",
    "if not os.path.exists(DATASET_ZIP):\n",
    "    with open(DATASET_ZIP, 'wb') as f:\n",
    "        print(f'Downloading {DATASET_ZIP}...')\n",
    "        r = requests.get(DATASET_URL, verify=False)\n",
    "        f.write(r.content)\n",
    "        print('OK')\n",
    "\n",
    "corpus = pd.read_json(DATASET_ZIP, lines=True)\n",
    "corpus.dropna(subset=['overall', 'reviewText'], inplace=True)\n",
    "corpus.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bace8799",
   "metadata": {},
   "source": [
    "El proceso que realizaremos en este _notebook_ será ver cómo construir un modelo convolucional para trabajar con este tipo de conjuntos de datos. Por lo tanto no entraremos en el detalle de obtener un conjunto de datos de prueba.\n",
    "\n",
    "Sin embargo, hay que tener en cuenta que en un problema real sería necesario entrenar con validación y contrastar con un conjunto de prueba antes de poner nuestro modelo en producción."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5719ffd0",
   "metadata": {},
   "source": [
    "### Preparando la entrada a nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "074e95f9",
   "metadata": {},
   "source": [
    "La entrada de nuestro modelo serán las reseñas como tales; por lo tanto, tomaremos la columna `reviewText` como nuestro conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f62d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = corpus['reviewText'].astype(str).str.strip()\n",
    "print(f'Training input shape: {x_train.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6464895c",
   "metadata": {},
   "source": [
    "Ahora crearemos una capa [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization), que se encargará de:\n",
    "\n",
    "1. Convertir una reseña completa en una secuencia de enteros (palabras), asignando a cada palabra un valor único.\n",
    "2. Truncar o rellenar las sencuencias para que estas mantengan una longitud fija previamente establecida (en nuestro caso `MAX_SEQUENCE_LEN`).\n",
    "\n",
    "El vocabulario se extraerá de nuestra entrada, tomando las `MAX_VOCAB_SIZE` palabras más comunes. Hemos añadido `+ 2` a la longitud ya que hay dos tokens preasignados: Padding (`''`) y palabras fuera del vocabulario (`'[UNK]'`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "88b782d0",
   "metadata": {},
   "source": [
    "### Preparando la salida de nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b394b67f",
   "metadata": {},
   "source": [
    "Partiendo de la base de que las reseñas con valoraciones altas serán positivas y las que tengan valoraciones bajas serán negativas, conservaremos sólo la reseña y la valoración del producto del conjunto de datos.\n",
    "\n",
    "En concreto, convertiremos las valoraciones de las reseñas en 0 si son malas (1 o 2 estrellas), 1 si son mediocres (3 estrellas) y 2 si son buenas (4 o más estrellas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d03f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = corpus['overall'].astype(int).replace({\n",
    "    1: 0,\n",
    "    2: 0,\n",
    "    3: 1,\n",
    "    4: 2,\n",
    "    5: 2,\n",
    "})\n",
    "print(f'Training output shape: {y_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578330b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7223928",
   "metadata": {},
   "source": [
    "## Uso de _embeddings_ preentrenados en nuestro modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9977840",
   "metadata": {},
   "source": [
    "Ya hemos visto que, a la hora de construir un modelo lingüístico, un aspecto importante es la representación de las palabras. Para captar el significado semántico de las palabras, utilizamos _embeddings_ de palabras, que son representaciones vectoriales de palabras en un espacio donde cada una de las muchas dimensiones representan una característica semántica.\n",
    "\n",
    "Esta vez, en lugar de entrenar nuestros _embeddings_ desde cero, aprovecharemos uno preentrenado, _Global Vectors for Word Representation_ (GLoVe), entrenado con un conjunto de datos de más de 6.000 millones de tokens. Cuenta con varios vectores de palabras preentrenados, por lo que los utilizaremos en <http://nlp.stanford.edu/data/glove.6B.zip>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b5594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(GLOVE_FILE):\n",
    "    print('Downloading ...', end='')\n",
    "    with open(GLOVE_FILE, 'wb') as f:\n",
    "        r = requests.get(GLOVE_URL, allow_redirects=True)\n",
    "        f.write(r.content)\n",
    "    print('OK')\n",
    "\n",
    "# Unzip in the directory 'glove'.\n",
    "print('Unpacking ...', end='')\n",
    "unpack_archive(GLOVE_FILE, 'tmp')\n",
    "print('OK')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "39fc8ea3",
   "metadata": {},
   "source": [
    "Mediante su uso podemos aprovechar la gran cantidad de conocimiento codificado en estos _embeddings_, lo que seguramente mejore (y mucho)el rendimiento de nuestro modelo lingüístico.\n",
    "\n",
    "Ahora carguemos el _embedding_ de la dimensión especificada en la configuración. El archivo se compone de líneas de tuplas, donde el primer elemento es la palabra (en texto) y el segundo es ese vector de palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b853ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def yield_tokens(corpus):\n",
    "    \"\"\"Iterador que genera listas de tokens a partir del corpus\"\"\"\n",
    "    for text in corpus:\n",
    "        yield text.split()  # Asume que el corpus ya está tokenizado por espacios\n",
    "\n",
    "def load_glove_embeddings(glove_path, corpus, embedding_dim, max_vocab_size):\n",
    "    print(\"Building vocabulary... \", end=\"\")\n",
    "    \n",
    "    # Construimos el vocabulario a partir del corpus\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(corpus), specials=[\"<unk>\", \"<pad>\"], max_tokens=max_vocab_size+2)\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    \n",
    "    print(f\"done ({len(vocab)} words).\")\n",
    "    \n",
    "    print(\"Loading GloVe embeddings... \", end=\"\")\n",
    "    word2vec = {}\n",
    "    \n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            word2vec[word] = vector\n",
    "    \n",
    "    print(\"done.\")\n",
    "\n",
    "    print(\"Creating embedding matrix with GloVe vectors... \", end=\"\")\n",
    "    embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "    unassigned_words = 0\n",
    "\n",
    "    for word, idx in vocab.get_stoi().items():\n",
    "        if idx >= max_vocab_size + 2:\n",
    "            continue  # Saltar si el índice supera el tamaño máximo del vocabulario\n",
    "\n",
    "        if word == \"<unk>\":\n",
    "            word = \"unk\"  # Mapear \"<unk>\" a \"unk\" para compatibilidad con GloVe\n",
    "\n",
    "        word_vector = word2vec.get(word)\n",
    "        if word_vector is not None:\n",
    "            embedding_matrix[idx] = word_vector\n",
    "        else:\n",
    "            unassigned_words += 1\n",
    "\n",
    "    print(f\"Number of words in GloVe: {len(word2vec)}\")\n",
    "    print(f\"Done ({unassigned_words} words unassigned).\")\n",
    "    print(f\"Vocab length: {len(vocab)}\")\n",
    "\n",
    "    return vocab, torch.tensor(embedding_matrix, dtype=torch.float)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c6ece6e",
   "metadata": {},
   "source": [
    "Bueno, $400.000$ _tokens_ son bastantes. Como nuestro vocabulario es menor, vamos a crear una capa de incrustación más pequeña, del tamaño de nuestro vocabulario. Para ello, incluiremos en ésta sólo los vectores de las palabras que nos devolverá la capa `TextVectorization`.\n",
    "\n",
    "Comenzaremos creando la matriz de incrustación con los vectores del guante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20216242",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, word_index, max_len):\n",
    "        self.texts = [[word_index.get(word, 1) for word in text.split()] for text in texts]\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.texts[idx]\n",
    "        x = x[:self.max_len] + [0] * (self.max_len - len(x))  # Padding\n",
    "        y = self.labels[idx]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3344464",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = x_train.values\n",
    "labels = y_train.values\n",
    "\n",
    "word_index = {word: i+2 for i, word in enumerate({word for text in texts for word in text.split()})}\n",
    "# Make sure that no index exceeds MAX_VOCAB_SIZE\n",
    "word_index = {word: min(i + 2, MAX_VOCAB_SIZE + 1) for word, i in word_index.items()}\n",
    "\n",
    "embedding_matrix = load_glove_embeddings(GLOVE_PATH, word_index, EMBEDDING_DIM, MAX_VOCAB_SIZE)[1]      #!!! ojo al [1], devuelve (vocab, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ae2fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "train_dataset = TextDataset(train_texts, train_labels, word_index, MAX_SEQUENCE_LENGTH)\n",
    "val_dataset = TextDataset(val_texts, val_labels, word_index, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f9f9bae0",
   "metadata": {},
   "source": [
    "Bueno, al parecer hay muchas palabras que no tienen correspondencia en el _embedding_ descargada. Después de todo $400000$ _tokens_ igual no eran tantos después de todo.\n",
    "\n",
    "Una vez hecho esto, podemos crear una capa de incrustación con la matriz de pesos precargada.\n",
    "\n",
    "```python\n",
    "self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b548d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, embedding_matrix, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        # haría un Sequential(), pero no consigo manejar bien la capa embedding :/\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
    "        self.conv1 = nn.Conv2d(1, 50, (3, EMBEDDING_DIM))\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(50, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # esto, a esto me refiero:\n",
    "        x = self.embedding(x).unsqueeze(1)  # dimensión de canal\n",
    "        x1 = torch.relu(self.conv1(x)).squeeze(3)\n",
    "        x1 = torch.max_pool1d(x1, x1.size(2)).squeeze(2)\n",
    "        x = self.dropout(x1)\n",
    "        return self.fc(x)\n",
    "\n",
    "# training evaluation \n",
    "def train_model(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    train_loss = total_loss / len(dataloader)\n",
    "    train_accuracy = correct_predictions / total_samples\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(true_labels, predictions)\n",
    "    val_report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    return val_acc, val_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c938b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextCNN(embedding_matrix, num_classes=3).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c377c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Sample texts: {texts[:3]}\")\n",
    "print(f\"Sample word_index: {list(word_index.items())[:10]}\")\n",
    "print(f\"Total words in word_index: {len(word_index)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b6eedd5",
   "metadata": {},
   "source": [
    "## Clasificación basada en redes neuronales convolucionales"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ee0758b",
   "metadata": {},
   "source": [
    "Haremos una aproximación al problema utilizando redes convolucionales. En este caso, nuestras frases estarán representadas por «imágenes» de una sola fila, con tantas columnas como la longitud de la secuencia especificada y tantos canales como la dimensión de cada valabra de la secuencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e243c252",
   "metadata": {},
   "source": [
    "Entrenemos el modelo y esperemos que salga bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f078c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_model(model, train_loader, criterion, optimizer, device)\n",
    "    val_acc, val_report = evaluate_model(model, val_loader, device)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'val_acc': val_acc,\n",
    "        'val_report': val_report\n",
    "    })"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b2d17311",
   "metadata": {},
   "source": [
    "Echemos un vistazo al progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e195930",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)\n",
    "history_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddec59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_df['epoch'], history_df['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history_df['epoch'], history_df['val_acc'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b16b83b4",
   "metadata": {},
   "source": [
    "Veamos ahora cómo interpreta el sentimiento de una reseña buena, regular y mala extraída del sitio web de amazon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fc59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "good = \"My nephew is on the autism spectrum and likes to fidget with things so I knew this toy would be a hit. Was concerned that it may not be \\\"complex\\\" enough for his very advanced brain but he really took to it. Both him (14 yrs) and his little brother (8 yrs) both enjoyed playing with it throughout Christmas morning. I'm always happy when I can find them something unique and engaging.\"\n",
    "poor = \"I wasn't sure about this as it's really small. I bought it for my 9 year old grandson. I was ready to send it back but my daughter decided it was a good gift so I'm hoping he likes it. Seems expensive for the price though to me.\"\n",
    "evil = \"I just wanted to follow up to say that I reported this directly to the company and had no response. I have not gotten any response from my review. The level of customer service goes a long way when an item you purchase is defective and this company didn’t care to respond. No I am even more Leary about ordering anything from this company. I never asked for a refund or replacement since I am not able to return it. I’m just wanted to let them know that this was a high dollar item and I expected it to be a quality item. Very disappointed! I bought this for my grandson for Christmas. He loved it and played with it a lot. My daughter called to say that the stickers were peeling on the corners. I am not able to take it from my grandson because he is autistic and wouldn’t understand. I just wanted to warn others who are wanting to get this. Please know that this is a cool toy and it may not happen to yours so it is up to you.\"\n",
    "\n",
    "test_texts = [good, poor, evil]\n",
    "test_labels = [2, 1, 0]  # good, poor, evil\n",
    "test_dataset = TextDataset(test_texts, test_labels, word_index, MAX_SEQUENCE_LENGTH)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "print(\"\\nTest Results:\")\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, _) in enumerate(test_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        predicted_class = torch.argmax(outputs, dim=1).item()\n",
    "        print(f\"Sample {i + 1} classified as: {predicted_class}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "43585490",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7defd88",
   "metadata": {},
   "source": [
    "Hemos demostrado cómo utilizar una red neuronal convolucional para clasificar texto, en concreto, reseñas de productos como buenas, medias o malas. También hemos analizado las ventajas de utilizar incrustaciones de palabras preentrenadas, que pueden ayudar a mejorar el rendimiento del modelo aprovechando las relaciones semánticas entre las palabras del corpus preentrenado. También exploramos la capa TextVectorization, que proporciona una forma flexible de preprocesar datos de texto y convertirlos en vectores numéricos.\n",
    "\n",
    "Se trata de un ejemplo que puede servir de punto de partida para multitud de proyectos de NLP."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33005bde",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
