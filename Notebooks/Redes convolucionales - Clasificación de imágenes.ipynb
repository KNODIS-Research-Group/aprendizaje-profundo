{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "010176d1",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" width=\"120px\" />\n",
    "\n",
    "\n",
    "# Clasificación de dígitos con redes convolucionales<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-1</small></i></div>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554fb61b",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "En este _notebook_ vamos a crear un clasificador para el problema del **CIFAR-100** usando para ello un modelo de red neuronal convolucional (CNN, del inglés _convolutional neural network_).\n",
    "\n",
    "Las CNN son un tipo de red neuronal que, aunque mantiene un comportamiento _feed-forward_, aprovecha la estructura de los datos de entrada (normalmente imágenes, pero en general cualquier conjunto de datos con relaciones espaciales) para reducir el tamaño del modelo y mejorar su rendimiento.\n",
    "\n",
    "¿Por qué es esto? Pues por varias razones, aunque las principales son las siguientes:\n",
    "\n",
    "### 1. Los MLP no escalan bien con imágenes grandes.\n",
    "\n",
    "Imaginemos una imagen a color de $32 \\times 32$ píxeles, como las de **CIFAR-100**. Si usáramos una red neuronal densa para clasificarla, necesitaríamos una capa de entrada con $32 \\times 32 \\times 3 = 3072$ neuronas. Si esta primera capa tuviera solo $256$ neuronas ocultas, el número total de pesos a entrenar sería:\n",
    "\n",
    "$$\n",
    "3072 \\times 256 + 256 = 786688\n",
    "$$\n",
    "\n",
    "A $32$ bits por pesos, esto ocuparía aproximadamente **3MiB** de memoria solo para la primera capa. Si trabajaramos con imágenes de tamaño mediano, como las del [conjunto _ImageNet_](https://www.image-net.org/) (que son de $224 \\times 224$ píxeles), este número superaría los **38 millones de pesos**, consumiendo unos $147$ MiB. Esto es claramente inviable a gran escala.\n",
    "\n",
    "En cambio, una CNN usa muchas menos conexiones. Por ejemplo, si en la primera capa aplicamos 32 filtros de $3 \\times 3$ sobre la misma imagen, el número de pesos sería:\n",
    "\n",
    "$$\n",
    "32 \\times 3 \\times 3 \\times + 32 = 896\n",
    "$$\n",
    "\n",
    "Esto ocuparía apenas $3.5$ KiB, independientemente del tamaño de la imagern, ya que los pesos pertenecen a los filtros, no a los píxeles individuales.\n",
    "\n",
    "### 2. Los MLP no aprovechan la estructura de los datos\n",
    "\n",
    "Las imágenes tienen una estructura espacial que los MLP. Por ejemplo, si un MLP ve una imagen de un gato y luego la misma imagen desplazada unos píxeles, no tiene por qué reconocerla como el mismo gato. Esto se denomina **invariancia a la traslación**, y los MLP no la poseen.\n",
    "\n",
    "Las CNN, en cambio, están diseñadas para captar la estructura espacial de los datos. Cada filtro de una capa de convolución aprende a detectar patrones que pueden aparecer en cualquier parte de la entrada. Esto les permite identificar características sin importar su posición exacta, lo que las hace ideales para tareas de visión por ordenador."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f243ba3",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d84b69f-c5f9-4411-af92-83c64f811369",
   "metadata": {},
   "source": [
    "Nuestro objetivo será implementar una CNN y compararla con el MLP que desarrollamos anteriormente, para analizar las diferencias entre ambos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67582fcc",
   "metadata": {},
   "source": [
    "## Bibliotecas y configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee9d46-b969-4cc6-ade2-2457a01dce37",
   "metadata": {},
   "source": [
    "A continuación, importaremos las bibliotecas necesarias para el desarrollo de este notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58374f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchmetrics\n",
    "import torchvision\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e97df",
   "metadata": {},
   "source": [
    "También configuraremos algunos parámetros para optimizar la presentación gráfica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca72f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (20, 6),'figure.dpi': 64})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122750d5-497f-4ece-b846-a7b73b7440a0",
   "metadata": {},
   "source": [
    "Por último, definiremos las constantes que utilizaremos como recursos comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff67957-d236-40cc-b1e4-e2c20b63cd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_DIR = './tmp'\n",
    "TRAIN_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06541e93",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aec0b7",
   "metadata": {},
   "source": [
    "## Preparación de los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c71335b-9733-48a7-8306-595d1e430db6",
   "metadata": {},
   "source": [
    "Antes de entrenar nuestro modelo, veamos en detalle los datos con los que vamos a trabajar. El [conjunto de datos **CIFAR-100**](https://www.cs.toronto.edu/~kriz/cifar.html) contiene $60000$ imágenes a color de $32 \\times 32$ píxeles, divididas en $100$ clases, con $600$ imágenes por clase. A diferencia de su versión más conocida (**CIFAR-10**), que solo tiene $10$ categorías generales (avión, coche, pájaro, etc.), CIFAR-100 es más desafiante, ya que cada clase representa un objeto o animal específico. El conjunto de datos está distribuido en $50000$ imágenes para entrenamiento y $10000$ para pruebas.\n",
    "\n",
    "![](images/CIFAR-100.png)\n",
    "\n",
    "Cada clase pertenece, además, a una categoría superior (por ejemplo, «_maple tree_» pertenece a «_trees_»). Esto nos permite agruparlas y usarlas en modelos más avanzados. En PyTorch, podemos cargar este conjunto de datos fácilmente desde `torchvision.datasets.CIFAR100`, que nos permite descargarlo y preprocesarlo sin complicaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    # Estandarizamos utilizando la fórmula del z-score. Para ello, usamos las\n",
    "    #  medias y desviaciones típicas para cada canal de color del CIFAR-100.\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=[0.5071, 0.4865, 0.4409],\n",
    "        std=[0.2673, 0.2564, 0.2761],\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=DATASETS_DIR,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform,\n",
    ")\n",
    "test_dataset = torchvision.datasets.CIFAR100(\n",
    "    root=DATASETS_DIR,\n",
    "    train=False,\n",
    "    download=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=256,\n",
    "    num_workers=2,  # Dos workers => Dos hilos cargando datos en lugar de uno\n",
    "    shuffle=True,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7362f3",
   "metadata": {},
   "source": [
    "Vemos que el conjunto de datos se divide en un conjunto de datos de entrenamiento de 60000 ejemplos y un conjunto de test de 10000 ejemplos. El primero es con el que entrenaremos nuestro modelo mientras que el segundo servirá para evaluar el desempeño del modelo con datos que no ha visto nunca.\n",
    "\n",
    "Veamos la forma que tienen los datos de entrada de un ejemplo en concreto (al estar estandarizados, la imagen no es real porque los valores han sido estandarizados):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8315fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 42\n",
    "\n",
    "image, label = train_dataset[INDEX]\n",
    "image = image.permute(1, 2, 0).numpy()  # (C, H, W) -> (H, W, C)\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Index: {INDEX}, label: {label}, shape: {image.shape}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a36bf0",
   "metadata": {},
   "source": [
    "## Creación del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe59a5c6-b982-430e-8000-f9bcb3b202bb",
   "metadata": {},
   "source": [
    "Vamos a construir una CNN sencilla, sin estructuras complejas ni ramificaciones. Dado que las capas se aplicarán en **orden secuencial**, podemos definir el modelo utilizando la API `Sequential` de PyTorch, que nos permite encadenar capas de forma directa.\n",
    "\n",
    "En PyTorch, `torch.nn.Sequential` se comporta como una **lista de capas** que se ejecutan en el orden en que se añaden. Existen dos formas de definir un modelo secuencial:\n",
    "\n",
    "1. Pasando las capas como una lista al constructor de `torch.nn.Sequential`, y\n",
    "2. Añadiendo capas una a una (aunque en PyTorch esto no se hace con `add`, como en Keras).\n",
    "\n",
    "A continuación, crearemos nuestra CNN para CIFAR-100 usando `torch.nn.Sequential`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c47797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn = torch.nn.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb5f805",
   "metadata": {},
   "source": [
    "Ahora añadiremos una capa oculta. Si nos acordamos de teoría, las CNN se componen de dos partes, la parte de extracción de características, compuesta de capas de convolución, y las capas de inferencia, compuestas de capas densas. Comenzaremos con una capa de convolución:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedfcd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add_module(\n",
    "    \"conv1\",\n",
    "    torch.nn.Conv2d(\n",
    "        in_channels=3,\n",
    "        out_channels=8,\n",
    "        kernel_size=3,\n",
    "        padding=1,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb1fb40",
   "metadata": {},
   "source": [
    "Acabamos de crear una capa compuesta de 8 convoluciones de tamaño $3 \\times 3 \\times 3$ que navegará por toda la imagen extrayendo características.\n",
    "\n",
    "Después de la capa de convolución, es importante añadir la función de activación. Usaremos una tipo _leaky_ ReLU con una pendiente negativa de $0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06e8b64-d137-48ed-8fdb-c67399c143f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add_module(\"acti1\", torch.nn.LeakyReLU(negative_slope=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f9fe0c-d74e-4c05-80d6-6447baf22c57",
   "metadata": {},
   "source": [
    "Otra capa que se suele combinar con las de convolución son las de muestreo parcial, _subsampling_ o _pooling_. Si recordamos, se trataba de filtros que reducían la imágen sacando los valores más destacables de cada área que cubre el filtro. En este caso, usaremos una capa `torch.nn.MaxPool2D` que escogerá el valor más alto del área que cubre. El valor por defecto del parámetro opcional `stride` (el salto del filtro) es el del tamaño del filtro, así que definiremos el filtro como $2 \\times 2$ y dejaremos que el _stride_ quede así para que no haya solapamiento. Esto reducirá bastante el tamaño de la salida de la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48663de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add_module(\"pool1\", torch.nn.MaxPool2d(kernel_size=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81aa407",
   "metadata": {},
   "source": [
    "De momento vamos a mantener una única capa de convolución, una función de activación y una de _pooling_, vamos a saltar a la parte de inferencia.\n",
    "\n",
    "Ahora bien, para saltar a la parte de inferencia (que no deja de ser un MLP) necesitamos que todos los datos de entrada estén en una dimensión. Por ello, antes de entrar en la capa de inferencia, vamos a «aplanar» la salida de la convolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546fee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add_module(\"flatten\", torch.nn.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151168d0",
   "metadata": {},
   "source": [
    "No necesitamos parámetros, lo único que hace es poner la matriz multidimensional en dos dimensiones, el número de ejemplos y todas las características extraídas para dicho ejemplo.\n",
    "\n",
    "Ahora vamos a añadir las capas densas que queramos. Vamos a añadir una oculta primero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20342f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add_module(\n",
    "    \"fc1\", torch.nn.Linear(in_features=8 * 16 * 16, out_features=128)\n",
    ")\n",
    "model_cnn.add_module(\"relu2\", torch.nn.ReLU())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c6691",
   "metadata": {},
   "source": [
    "Y la de salida. Recordemos, la salida serán **100 neuronas**, una por cada posible clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6016acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.add_module(\"fc2\", torch.nn.Linear(in_features=128, out_features=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d6af7",
   "metadata": {},
   "source": [
    "Ya explicamos previamente por qué no explicitamos la función _softmax_ o _log-softmax_. Por tanto ya tenemos definido nuestro modelo. Veamos su estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab74944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cnn)\n",
    "total_params = sum(p.numel() for p in model_cnn.parameters())\n",
    "print(f\"No. of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dce57d",
   "metadata": {},
   "source": [
    "Vamos a compararlo con un perceptrón multicapa de un par de capas ocultas con aproximadamente el mismo número de parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2d2b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mlp = torch.nn.Sequential(\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(in_features=3 * 32 * 32, out_features=85),\n",
    "    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "    torch.nn.Linear(in_features=85, out_features=75),\n",
    "    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "    torch.nn.Linear(in_features=75, out_features=100)\n",
    ")\n",
    "print(model_mlp)\n",
    "total_params = sum(p.numel() for p in model_mlp.parameters())\n",
    "print(f\"No. of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a4def",
   "metadata": {},
   "source": [
    "Ahora veamos qué tal se comportan en un par de entrenamientos. Para ello los entrenaremos con los mismos algoritmos y el mismo número de epochs. Usaremos la función `train` definida en el módulo `utils.py` porque es un código muy repetitivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99aa2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn = utils.train(\n",
    "    model=model_cnn,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model_cnn.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedfcd5c-99d7-438c-90b9-f1046c81c3d3",
   "metadata": {},
   "source": [
    "Veamos la evolución de su error y su exactitud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad454c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_cnn).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07184619",
   "metadata": {},
   "source": [
    "Ahora haremos lo mismo con el MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_mlp = utils.train(\n",
    "    model=model_mlp,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model_mlp.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d001e5",
   "metadata": {},
   "source": [
    "Y veamos también la evolución de su error y su exactitud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90813e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_mlp).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5baea",
   "metadata": {},
   "source": [
    "Parece que, aunque el MLP funciona bastante bien, la red no es capaz de llegar a tanta precisión como la CNN. Tiene sentido, ya que muchos de los parámetros se ocupan de relacionar píxeles que probablemente tengan muy poco que ver. Veamos qué tal se comportan con el conjunto de test. Al igual que con el entrenamiento, usaremos la función `evaluate` del módulo `utils` para no repetirnos demasiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fee2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cnn = utils.evaluate(\n",
    "    model=model_cnn,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")\n",
    "eval_mlp = utils.evaluate(\n",
    "    model=model_mlp,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")\n",
    "print(f'Results for CNN: Loss: {eval_cnn.get(\"loss\")}, Acc: {eval_cnn.get(\"metric\")}')\n",
    "print(f'Results for MLP: Loss: {eval_mlp.get(\"loss\")}, Acc: {eval_mlp.get(\"metric\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c2a3d7",
   "metadata": {},
   "source": [
    "Además hemos hecho una pequeña trampa. Nuestra red de convolución sólo tiene una capa, y esto provoca que el resultado sea muy grande. Este, al conectarlo con la capa densa hace que el número de parámetros crezca dramáticamente.\n",
    "\n",
    "Vamos a tratar de solucionar esto y a comparar con un nuevo perceptrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f3cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir el modelo en PyTorch\n",
    "model_cnn_2 = torch.nn.Sequential(\n",
    "    # Primera capa convolucional\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1),\n",
    "    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "    torch.nn.MaxPool2d(kernel_size=2),\n",
    "    # Segunda capa convolucional\n",
    "    torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, padding=1),\n",
    "    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "    torch.nn.MaxPool2d(kernel_size=2),\n",
    "    # Aplanamiento de las salidas\n",
    "    torch.nn.Flatten(),\n",
    "    # Inferencia\n",
    "    torch.nn.Linear(16 * 8 * 8, 8),  # La entrada es 8*8*8 después del último MaxPool\n",
    "    torch.nn.LeakyReLU(negative_slope=0.1),\n",
    "    torch.nn.Linear(8, 100),  # 100 clases en CIFAR-100\n",
    ")\n",
    "\n",
    "print(model_cnn_2)\n",
    "total_params = sum(p.numel() for p in model_cnn_2.parameters())\n",
    "print(f\"No. of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3d79b",
   "metadata": {},
   "source": [
    "Hemos pasado de un plumazo de $275396$ a $10492$. ¡Casi nada!. Vamos su desempeño."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f803a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cnn_2 = utils.train(\n",
    "    model=model_cnn_2,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(model_cnn_2.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e897570",
   "metadata": {},
   "source": [
    "Gráficamente, la evolución queda como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6be095",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_cnn_2).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b18a03f",
   "metadata": {},
   "source": [
    "Parece un entrenamiento más estable y con mejores valores. Comparemos ahora los resultados de los tres modelos con el conjunto de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d920bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cnn_2 = utils.evaluate(\n",
    "    model=model_cnn_2,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")\n",
    "\n",
    "print(f'Results for CNN 2.0: Loss: {eval_cnn.get(\"loss\")}, Acc: {eval_cnn.get(\"metric\")}')\n",
    "print(f'Results for CNN:     Loss: {eval_cnn_2.get(\"loss\")}, Acc: {eval_cnn_2.get(\"metric\")}')\n",
    "print(f'Results for MLP:     Loss: {eval_mlp.get(\"loss\")}, Acc: {eval_mlp.get(\"metric\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379fe8a",
   "metadata": {},
   "source": [
    "El modelo generaliza mejor que el resto y además ocupa muchísimo menos. Parece que la arquitectura de CNN se comporta bastante mejor para este tipo de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109c198a-0acc-493c-a665-a0bd0d8a62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"CNN 2.0\": history_cnn_2['val_metric'],\n",
    "    \"CNN\": history_cnn['val_metric'],\n",
    "    \"MLP\": history_mlp['val_metric'],\n",
    "}).plot(ax=axs[0])\n",
    "axs[0].set_xlabel('Epoch num.')\n",
    "axs[0].set_title('Validation accuracy')\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"CNN 2.0\": history_cnn_2['val_loss'],\n",
    "    \"CNN\": history_cnn['val_loss'],\n",
    "    \"MLP\": history_mlp['val_loss'],\n",
    "}).plot(ax=axs[1])\n",
    "axs[1].set_xlabel('Epoch num.')\n",
    "axs[1].set_title('Validation loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965de9d-521b-4aab-bfea-4fa9bcc057bb",
   "metadata": {},
   "source": [
    "Sin embargo, tiene un problema que es probable que hayamos identificado: La velocidad de entrenamiento. Después de todo, las operaciones de convolución son más lentas que un simple producto de matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5fe593",
   "metadata": {},
   "source": [
    "## Modelos LeNet y AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d93676",
   "metadata": {},
   "source": [
    "El modelo de red convolucional LeNet es una de las primeras redes neuronales que usaron opreaciones de convolución. Fue propuesta por Yann LeCun [1] en 1989 para el problema del reconocimiento (i.e. clasificación) de números manuscritos\n",
    "\n",
    "LeNet es el modelo en el que se inspira AlexNet [2], el modelo de que ganó la competición ImageNet en 2012 y que se considera como el origen de la vorágine del deep learning que nos ha traído hasta el momento actual.\n",
    "\n",
    "<figure>\n",
    "    <center>\n",
    "        <img src=\"https://media.licdn.com/dms/image/D4E12AQFOevexWWMwhA/article-cover_image-shrink_600_2000/0/1680257599468?e=2147483647&v=beta&t=AaLo1l9ln5Tyl1ArH1sRlKELkjFdTrpRf_My-uaHuf4\" alt=\"Lenet vs AlexNet\" style=\"width:70%\" />\n",
    "        <figcaption align = \"center\"><strong>Figura 1</strong>. Diferencias entre arquitecturas LeNet y AlexNet. Fuente: <a href=\"https://www.linkedin.com/pulse/lenet-alexnet-chandrahasa-sreeramaneni\">LinkedIn</a>, <a href=\"https://creativecommons.org/licenses/by-sa/4.0\" title=\"Creative Commons Attribution-Share Alike 4.0\">CC BY-SA 4.0</a>, <a href=\"https://commons.wikimedia.org/w/index.php?curid=104937230\">Link</a></figcaption>\n",
    "    </center>\n",
    "</figure>\n",
    "\n",
    "Fijémonos en el año del primero. Usaron los algoritmos de retropropagación recién propuestos (1986, aunque es cierto que la técnica tiene predecesores) para el entrenamiento de los filtros de convolución. Por ello, sus autores se consideran pioneros en la clasificación de imágenes.\n",
    "\n",
    "Crearemos ahora estas arquitecturas para ver cómo se comportan con este conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c62f93",
   "metadata": {},
   "source": [
    "### Arquitectura LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a2fb8",
   "metadata": {},
   "source": [
    "Existen muchas variantes diferentes. Nosotros implementaremos la mostrada en la Figura 1, pero no es la original (partiendo de que esta además toma como entrada imágenes de $32 \\times 32$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeb42ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, padding='same'),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.AvgPool2d(kernel_size=2),\n",
    "    torch.nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.AvgPool2d(kernel_size=2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(in_features=16 * 6 * 6, out_features=120),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(in_features=120, out_features=84),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(in_features=84, out_features=100),\n",
    ")\n",
    "\n",
    "print(lenet)\n",
    "total_params = sum(p.numel() for p in lenet.parameters())\n",
    "print(f\"No. of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1174b387",
   "metadata": {},
   "source": [
    "Esta arquitectura bastantes más parámetros que las última red. Veamos cómo progresa el entrenamiento durante 50 epochs como en los anteriores ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861e9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_lenet = utils.train(\n",
    "    model=lenet,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(lenet.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b409a9",
   "metadata": {},
   "source": [
    "Gráficamente, la evolución queda como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b9a1f-1732-4446-9cd7-e6a60eb2b518",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_lenet).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30690699",
   "metadata": {},
   "source": [
    "### Arquitectura AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cddef3",
   "metadata": {},
   "source": [
    "Como hemos visto, la arquitectura AlexNet es una ampliación de la arquitectura LeNet, diseñada para la competición ImageNet que consistía en la clasificación de imágenes de $224 \\times 224 \\times 3$ en una entre mil clases.\n",
    "\n",
    "A diferencia que con LeNet, aquí implementaremos una versión un tanto diferente a la de la Figura 2:\n",
    "\n",
    "1. a última capa no será de 1000, sino de 10, ya que las clases en las que clasificar son 10 dígitos\n",
    "2. Reduciremos las dimensiones de los filtros de convolución y _pooling_ ya que la imagen de entrada no es lo suficientemente grande para soportarlos (se reducen por debajo de cero píxeles)\n",
    "3. También eliminaremos los _strides_ por el mismo motivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9443a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=3, out_channels=96, kernel_size=3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),  # -> (96, 13, 13)\n",
    "    torch.nn.Conv2d(in_channels=96, out_channels=256, kernel_size=3, padding=\"same\"),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=2, stride=2),  # -> (256, 6, 6)\n",
    "    torch.nn.Conv2d(in_channels=256, out_channels=384, kernel_size=3, padding=\"same\"),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=\"same\"),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=\"same\"),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.MaxPool2d(kernel_size=3, stride=3),  # -> (256, 2, 2)\n",
    "    torch.nn.Flatten(),  # -> 256 * 2 * 2 = 1024\n",
    "    torch.nn.Linear(in_features=1024, out_features=4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(in_features=4096, out_features=4096),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(in_features=4096, out_features=100),\n",
    ")\n",
    "\n",
    "print(alexnet)\n",
    "total_params = sum(p.numel() for p in alexnet.parameters())\n",
    "print(f\"No. of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5135145",
   "metadata": {},
   "source": [
    "Esta arquitectura tiene bastantes más parámetros que las que hemos estado tratando hasta ahora. Además incluye dos capas de _dropout_, las cuales son para regularizar el aprendizaje, y así reducir el _overfitting_ durante el entrenamiento.\n",
    "\n",
    "Veamos cómo progresa el entrenamiento durante 50 epochs como en los anteriores ejemplos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157a4137",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_alexnet = utils.train(\n",
    "    model=alexnet,\n",
    "    train_loader=train_loader,\n",
    "    n_epochs=TRAIN_EPOCHS,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    optimizer=torch.optim.Adam(alexnet.parameters()),\n",
    "    validation_split=0.1,\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8f862d",
   "metadata": {},
   "source": [
    "Gráficamente, la evolución queda como sigue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81414ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history_alexnet).plot()\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3611aa0a",
   "metadata": {},
   "source": [
    "Veamos una comparativa entre todos los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fb114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_lenet = utils.evaluate(\n",
    "    model=model_lenet,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")\n",
    "eval_alexnet = utils.evaluate(\n",
    "    model=model_alexnet,\n",
    "    data_loader=test_loader,\n",
    "    criterion=torch.nn.CrossEntropyLoss(),\n",
    "    metric_fn=torchmetrics.classification.MulticlassAccuracy(num_classes=100),\n",
    ")\n",
    "\n",
    "print(f'Results for AlexNet: Loss: {eval_alexnet.get(\"loss\")}, Acc: {eval_alexnet.get(\"metric\")}')\n",
    "print(f'Results for LeNet:   Loss: {eval_lenet.get(\"loss\")}, Acc: {eval_lenet.get(\"metric\")}')\n",
    "print(f'Results for CNN 2.0: Loss: {eval_cnn_2.get(\"loss\")}, Acc: {eval_cnn_2.get(\"metric\")}')\n",
    "print(f'Results for CNN:     Loss: {eval_cnn.get(\"loss\")}, Acc: {eval_cnn.get(\"metric\")}')\n",
    "print(f'Results for MLP:     Loss: {eval_mlp.get(\"loss\")}, Acc: {eval_mlp.get(\"metric\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbf5bb",
   "metadata": {},
   "source": [
    "Quizá en este ejemplo no es apreciable la potencia de estos modelos respecto a los anteriores, pero lo cierto es que los superan su desempeño en varios órdenes de magnitud en problemas más complejos... y también en tiempo de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68fe435-6f66-4a03-8b2b-d0dec2a69676",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Alexnet\": history_alexnet['val_metric'],\n",
    "    \"LeNet\": history_lenet['val_metric'],\n",
    "    \"CNN 2.0\": history_cnn_2['val_metric'],\n",
    "    \"CNN\": history_cnn['val_metric'],\n",
    "    \"MLP\": history_mlp['val_metric'],\n",
    "}).plot(ax=axs[0])\n",
    "axs[0].set_xlabel('Epoch num.')\n",
    "axs[0].set_title('Validation accuracy')\n",
    "\n",
    "pd.DataFrame({\n",
    "    \"Alexnet\": history_alexnet['val_loss'],\n",
    "    \"LeNet\": history_lenet['val_loss'],\n",
    "    \"CNN 2.0\": history_cnn_2['val_loss'],\n",
    "    \"CNN\": history_cnn['val_loss'],\n",
    "    \"MLP\": history_mlp['val_loss'],\n",
    "}).plot(ax=axs[1])\n",
    "axs[1].set_xlabel('Epoch num.')\n",
    "axs[1].set_title('Validation loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05096b5",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd65fcd4",
   "metadata": {},
   "source": [
    "Las redes convolucionales son una arquitectura de red neuronal que aprovecha las características de la entrada para aprender las relaciones existentes. Es equivalente a aproximar un problema de forma más inteligente que la fuerza bruta.\n",
    "\n",
    "Su principal desventaja es la velocidad de entrenamiento, pero superan con creces la capacidad de resolución de problemas cuando tratamos con elementos como imágenes.\n",
    "\n",
    "Más adelante veremos otros tipos de redes que solucionan ciertos problemas o que permiten que éstas sean todavía más grandes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582efec2",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
