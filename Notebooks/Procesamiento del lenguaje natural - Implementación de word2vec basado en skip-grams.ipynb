{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3e4566",
   "metadata": {},
   "source": [
    "<div><a href=\"https://knodis-research-group.github.io/\"><img style=\"float: right; width: 128px; vertical-align:middle\" src=\"https://knodis-research-group.github.io/knodis-logo_horizontal.png\" alt=\"KNODIS logo\" /></a>\n",
    "\n",
    "# Implementación de Word2vec con skip-grams<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Última actualización: 2025-03-14</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0625fc1d",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbddc17",
   "metadata": {},
   "source": [
    "Empezamos con lo más importante: las técnicas de _word embedding_ son una forma de representar numéricamente las palabras, pero con matices adicionales. Dicho esto, vamos a programar un proceso de aprendizaje de _embeddings_ a partir de texto. Nos centraremos en una técnica llamada **Word2Vec**, que ya tiene bastantes años pero que para pequeños embeddings sigue teniendo uso.\n",
    "\n",
    "Word2Vec se basa en una red neuronal que genera la matriz mediante entrenamiento supervisado en un problema de clasificación. El artículo en el que se presenta el método es [_Efficient Estimation of Word Representations in Vector Space_ (Mikolov et al., 2013)](https://arxiv.org/pdf/1301.3781.pdf) y se utiliza con para medir la **similitud sintáctica y semántica de las palabras**.\n",
    "\n",
    "El artículo explora dos modelos: _Continuous Bag-of-Words_ y _Skip-gram_. Este último es el más utilizado y será el que abordemos aquí.\n",
    "\n",
    "La idea del _Skip-gram_ es la siguiente: dada una palabra (a la que llamaremos _palabra de contexto_), queremos entrenar un modelo que sea capaz de predecir una palabra que pertenezca a una ventana de tamaño $N$. Por ejemplo, asumiendo una ventana de tamaño $N = 3$ y dada la siguiente frase:\n",
    "\n",
    "> All those <span style=\"color:red\">moments will be</span> **lost** <span style=\"color:red\">in time like</span> tears in rain\n",
    "\n",
    "La _palabra de contexto_ sería **lost**, y entrenaríamos el modelo para que predijera una de las palabras existentes dentro de la ventana especificada, es decir, una de `['moments', 'will', 'be', 'in', 'time', 'like']`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84c899",
   "metadata": {},
   "source": [
    "## Objetivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d1ecaa",
   "metadata": {},
   "source": [
    "En este notebook crearemos un _embedding_ utilizando la técnica _skip-gram_ de **Word2Vec**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f806243c",
   "metadata": {},
   "source": [
    "## Librerías y configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "style_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (16, 9), 'figure.dpi': 100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directory_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODELS_DIR = pathlib.Path('../Models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "TEMP_PATH = pathlib.Path('tmp')\n",
    "TEMP_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = True\n",
    "MODEL_PATH = MODELS_DIR / \"skipgrams.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corpus_intro",
   "metadata": {},
   "source": [
    "## Construcción del corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17cdc7d-5e6d-4f16-b82a-df35003dc451",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20031d8e-2230-4d9b-9976-1e0a4ef1dbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 20000\n",
    "WINDOW_SIZE = 2\n",
    "EMBEDDING_DIM = 5\n",
    "TRAIN_BATCH = 256\n",
    "TRAIN_EPOCH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corpus_desc",
   "metadata": {},
   "source": [
    "Utilizaremos un _dataset_ de reseñas de Amazon no muy actual, pero interesante para entrenar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb55541-bcad-403f-99e5-784977dcb432",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://github.com/blazaid/aprendizaje-profundo/raw/refs/heads/gh-pages/Datasets/Video_Games_5.json.gz'\n",
    "DATASET = pathlib.Path('tmp/Video_Games_5.json')\n",
    "\n",
    "print(f\"Downloading dataset to {DATASET.resolve()}\")\n",
    "if not DATASET.exists():\n",
    "    with requests.get(DATASET_URL, stream=True) as response:\n",
    "        response.raise_for_status()\n",
    "        with gzip.GzipFile(fileobj=response.raw) as f_gz:\n",
    "            with DATASET.open(\"wb\") as f:\n",
    "                shutil.copyfileobj(f_gz, f)\n",
    "else:\n",
    "    print(\"File already exists! Nice\")\n",
    "\n",
    "print(\"Loading text corpus\")\n",
    "corpus = pd.read_json(DATASET, lines=True)\n",
    "corpus = corpus['reviewText'].astype(str).str.strip()\n",
    "corpus.head()\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19628920-9748-4b75-a8d8-1c3fe46be063",
   "metadata": {},
   "source": [
    "A continuación crearemos una clase que se encargará de realizar la tokenización de nuestros textos. La idea es que sea una clase que ajusta los textos pasados y convierta dichos textos a secuencias de indices a texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1febf054-561c-4c32-bc16-0e085a09ea35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, max_vocab_size, unk_token=\"<UNK>\", pad_token=\"<PAD>\"):\n",
    "        self.word_index = {}\n",
    "        self.index_word = {}\n",
    "        self.unk_token = unk_token\n",
    "        self.pad_token = pad_token\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "    def fit_on_texts(self, texts):\n",
    "        counter = collections.Counter()\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            counter.update(words)\n",
    "\n",
    "        self.word_index = {self.pad_token: 0, self.unk_token: 1}\n",
    "        for i, (word, _) in enumerate(\n",
    "            counter.most_common(self.max_vocab_size - len(self.word_index)),\n",
    "            len(self.word_index)\n",
    "        ):\n",
    "            self.word_index[word] = i\n",
    "\n",
    "        self.index_word = {i: word for word, i in self.word_index.items()}\n",
    "\n",
    "    def texts_to_sequences(self, texts):\n",
    "        unk_index = self.word_index.get(self.unk_token)\n",
    "        sequences = []\n",
    "        for text in texts:\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            seq = [self.word_index.get(word, unk_index) for word in words]\n",
    "            sequences.append(seq)\n",
    "        return sequences\n",
    "\n",
    "tokenizer = SimpleTokenizer(max_vocab_size=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts([\n",
    "    \"I've seen things you people wouldn't believe\",\n",
    "    \"Attack ships on fire off the shoulder of Orion\",\n",
    "    \"I watched C-beams glitter in the dark near the Tannhäuser Gate\",\n",
    "])\n",
    "tokenizer.texts_to_sequences([\n",
    "    \"All those moments will be lost in time, like tears in rain\",\n",
    "    \"Time to die\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocess_intro",
   "metadata": {},
   "source": [
    "Bueno, parece que el índice asignado al «token desconocido» es 28. Esperemos que haya más variedad de palabras analizando el contenido de nuestro corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer_intro",
   "metadata": {},
   "source": [
    "La variable `corpus` contiene todas las reseñas. _Tokenizaremos_ cada comentario, convirtiéndolo en una lista de palabras, utilizando nuestro tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tokenization",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizer(max_vocab_size=MAX_VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Mostramos algunos de los primeros elementos de los diccionarios\n",
    "print(f'word2id: {dict(list(tokenizer.word_index.items())[0:4])} ...')\n",
    "print(f'id2word: {dict(list(tokenizer.index_word.items())[0:4])} ...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sequences_intro",
   "metadata": {},
   "source": [
    "Convertimos cada reseña en una secuencia de enteros utilizando el tokenizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convert_sequences",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "vocab_size = len(tokenizer.word_index)\n",
    "\n",
    "print(f'Corpus sequences: {len(sequences)} sequences')\n",
    "print(f'Vocabulary Size: {vocab_size} tokens')\n",
    "print('Sentence example:')\n",
    "print(f'- {corpus.iloc[5]}')\n",
    "print(f'- {sequences[5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skipgrams_intro",
   "metadata": {},
   "source": [
    "## Generador de Skip-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2188dd40-d59c-4f5f-9a31-e82407e40710",
   "metadata": {},
   "source": [
    "Ahora, el siguiente paso es definir una función que nos genere los _skip-grams_. Esta función generará pares positivos (dentro de la ventana) y negativos (muestreo aleatorio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5387e9-b25e-48e7-828f-56e67e8759f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skipgrams(sequence, vocabulary_size, window_size, negative_samples=1):\n",
    "    targets, contexts, labels = [], [], []\n",
    "    for i, target in enumerate(sequence):\n",
    "        window_start = max(0, i - window_size)\n",
    "        window_end = min(len(sequence), i + window_size + 1)\n",
    "        context_indices = [j for j in range(window_start, window_end) if j != i]\n",
    "        for j in context_indices:\n",
    "            context_word = sequence[j]\n",
    "            # Agregamos target-context positivo\n",
    "            targets.append(target)\n",
    "            contexts.append(context_word)\n",
    "            labels.append(1)\n",
    "            # Agregamos target-context negativo\n",
    "            negatives_added = 0\n",
    "            while negatives_added < negative_samples:\n",
    "                negative_word = random.randint(1, vocabulary_size - 1)\n",
    "                if negative_word != context_word:\n",
    "                    targets.append(target)\n",
    "                    contexts.append(negative_word)\n",
    "                    labels.append(0)\n",
    "                    negatives_added += 1\n",
    "\n",
    "    return targets, contexts, labels\n",
    "\n",
    "target_tokens = []\n",
    "context_tokens = []\n",
    "labels = []\n",
    "for sequence in sequences:\n",
    "    t_tokens, c_tokens, ls = extract_skipgrams(sequence, vocab_size, WINDOW_SIZE)\n",
    "    target_tokens.extend(t_tokens)\n",
    "    context_tokens.extend(c_tokens)\n",
    "    labels.extend(ls)\n",
    "\n",
    "print(f\"Extracted skipgrams: {len(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41e073e-6464-42ab-8f6a-8dca91ce7d38",
   "metadata": {},
   "source": [
    "El proceso de cálculo de _skip-grams_ es **muy** pesado, tanto en tiempo como en espacio. Por lo tanto, crearemos un dataset que calculará el batch de skipgrams que toca en cada llamada de obtención de dicho batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generate_skipgrams",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, target_tokens, context_tokens, labels):\n",
    "        self.target_tokens = target_tokens\n",
    "        self.context_tokens = context_tokens\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.target_tokens[idx], dtype=torch.long),\n",
    "            torch.tensor(self.context_tokens[idx], dtype=torch.long),\n",
    "            torch.tensor(self.labels[idx], dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "dataset = SkipGramDataset(target_tokens, context_tokens, labels)\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=TRAIN_BATCH,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604db4fa-7530-470b-9bd0-8f270955c119",
   "metadata": {},
   "source": [
    "## Creación y entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model_intro",
   "metadata": {},
   "source": [
    "Ya tenemos un dataset con las entradas y sus respectivas salidas. Ahora el objetivo es entrenar un modelo que sea capaz de determinar si dos palabras pertenecen al mismo contexto.\n",
    "\n",
    "Para ello, crearemos una capa de _embedding_ que transforme las palabras en su vector de características. La similitud entre los embeddings se mide mediante la _cosine similarity_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, dropout_rate=0.25):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.linear = torch.nn.Linear(1, 1)\n",
    "        \n",
    "    def forward(self, target, context):\n",
    "        # target y context tienen forma (batch,)\n",
    "        target_emb = self.embedding(target)    # (batch, embedding_dim)\n",
    "        target_emb = self.dropout(target_emb)\n",
    "        context_emb = self.embedding(context)  # (batch, embedding_dim)\n",
    "        context_emb = self.dropout(context_emb)\n",
    "        \n",
    "        similarity = nn.functional.cosine_similarity(target_emb, context_emb, dim=1).unsqueeze(1)\n",
    "        \n",
    "        out = self.linear(similarity)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "model = SkipGramModel(vocab_size, EMBEDDING_DIM)\n",
    "if LOAD_MODEL and MODEL_PATH.exists():\n",
    "    model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_intro",
   "metadata": {},
   "source": [
    "Ahora entrenamos el modelo con los _skip-grams_ generados. Este proceso puede tardar **mucho** dependiendo de la máquina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "training_loop",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Train no vale porque dataloader devuelve 3 valores, no 2\n",
    "\n",
    "history = utils.train(\n",
    "    model=model,\n",
    "    train_loader=dataloader,\n",
    "    n_epochs=2,\n",
    "    criterion=torch.nn.BCELoss(),\n",
    "    optimizer=torch.optim.Adam(model.parameters()),\n",
    "    validation_split=0.1,\n",
    ")\n",
    "if SAVE_MODEL:\n",
    "    torch.save(music_generator.state_dict(), MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "training_progress",
   "metadata": {},
   "source": [
    "Veamos el progreso del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plot_training",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "embeddings_intro",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Una vez entrenado el modelo, disponemos de una matriz con los pesos de las características para cada palabra. Extraemos esta matriz y la mostramos en un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extract_embeddings",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = model.embedding.weight.data.cpu().numpy()[1:]\n",
    "\n",
    "df = pd.DataFrame(weights, index=list(tokenizer.index_word.values()))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar_words_intro",
   "metadata": {},
   "source": [
    "Realicemos una búsqueda de las palabras más similares a una dada utilizando, por ejemplo, la distancia Euclidiana de sus vectores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar_words",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLOSEST_WORDS = 10\n",
    "WORD = 'man'\n",
    "\n",
    "v1 = weights[tokenizer.word_index[WORD] - 1]\n",
    "words = sorted(\n",
    "    [word for word in tokenizer.word_index.keys()],\n",
    "    key=lambda w: np.linalg.norm(v1 - weights[tokenizer.word_index[w]-1])\n",
    ")\n",
    "df.loc[words[:NUM_CLOSEST_WORDS + 1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En resumen, hemos implementado un _embedding_ utilizando la técnica de _skip-grams_ de **Word2Vec** y hemos demostrado su efectividad para representar las palabras de forma más significativa en un espacio vectorial. Esta técnica es capaz de capturar la semántica de las palabras, representándolas en un espacio de dimensión inferior al que ocuparía una representación _one-hot_.\n",
    "\n",
    "Cabe destacar que, aunque el proceso de preprocesamiento y entrenamiento se ha simplificado, existen muchas mejoras posibles (como preprocesamiento avanzado, mayor cantidad de negative sampling, etc.) y muchos _embeddings_ preentrenados disponibles para uso."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final_notes",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
