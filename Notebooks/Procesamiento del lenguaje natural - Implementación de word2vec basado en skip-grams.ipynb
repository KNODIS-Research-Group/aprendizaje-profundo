{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb3e4566",
   "metadata": {},
   "source": [
    "<div><img style=\"float: right; width: 120px; vertical-align:middle\" src=\"https://www.upm.es/sfs/Rectorado/Gabinete%20del%20Rector/Logos/EU_Informatica/ETSI%20SIST_INFORM_COLOR.png\" alt=\"ETSISI logo\" />\n",
    "\n",
    "\n",
    "# Word2vec _skip-grams_ implementation<a id=\"top\"></a>\n",
    "\n",
    "<i><small>Author: Alberto Díaz Álvarez<br>Last update: 2023-05-25</small></i></div>\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0625fc1d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cbddc17",
   "metadata": {},
   "source": [
    "We start with the most important of all: _Word embedding techniques_ is a way of saying _representing words numerically_ but with more hooks. And having said that, we are going to program a process of learning _embeddings_ from text corpora. We will focus on a technique called _Word2Vec_, although we have already seen that there are more.\n",
    "\n",
    "Word2vec_ is based on a neural network that generates the matrix using supervised training on a classification problem. The article where the method is presented is [Efficient Estimation of Word Representations in Vector Space (Mikolov et al.,2013)](https://arxiv.org/pdf/1301.3781.pdf) and it is a method that is quite successfully used to measure **syntactic and semantic similarity of words**.\n",
    "\n",
    "The article explores two different models: _Continuous Bag-of-Words_ and _Skip-gram_. The latter is the most commonly used, and will be the one we will look at in this exercise.\n",
    "\n",
    "The idea of the _Skip-gram_ is the following: given a word (which we will call _context word_), we want to train a model such that it is able to predict a word belonging to a window of size $N$. For example, assuming a window of size $N = 3$ and given the following sentence:\n",
    "\n",
    "> All those <span style=\"color:red\">moments will be</span> **lost** <span style=\"color:red\">in time like</span> tears in rain\n",
    "\n",
    "The _context word_ would be **lost**, and we would train the model to predict one of the existing words within the specified window, i.e., one of `['moments', 'will', 'be', 'in', 'time', 'like']`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de84c899",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8d1ecaa",
   "metadata": {},
   "source": [
    "In this _notebook_ we will create an _embedding_ from the _skip-gram_ technique of _Word2Vec_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f806243c",
   "metadata": {},
   "source": [
    "## Libraries and configuration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e01d185e",
   "metadata": {},
   "source": [
    "Next we will import the libraries that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee694de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7286046",
   "metadata": {},
   "source": [
    "We will also configure some parameters to adapt the graphic presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7ae796",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "plt.rcParams.update({'figure.figsize': (16, 9),'figure.dpi': 100})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf70402e",
   "metadata": {},
   "source": [
    "And create the necessary directories in case they have not been created previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2261e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tmp', exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "694a766e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "808a6843",
   "metadata": {},
   "source": [
    "## Corpus construction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cbb78a61",
   "metadata": {},
   "source": [
    "We will use the Amazon Reviews dataset (from https://nijianmo.github.io/amazon/index.html, not the most current, but useful for us) to train the model. More specifically, we will use the small subset of the category \"Software\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1b454e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_URL = 'https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Software_5.json.gz'\n",
    "DATASET_ZIP = 'tmp/Software_5.json.gz'\n",
    "\n",
    "# Download the remote file if it does not exist\n",
    "if not os.path.exists(DATASET_ZIP):\n",
    "    with open(DATASET_ZIP, 'wb') as f:\n",
    "        print(f'Downloading {DATASET_ZIP}...')\n",
    "        r = requests.get(DATASET_URL, verify=False)\n",
    "        f.write(r.content)\n",
    "        print('OK')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd580e16",
   "metadata": {},
   "source": [
    "Once downloaded, we can proceed to load the dataset. For our purpose (creating an _embedding_), we don't care about the output of the model or the id of the examples; we want the texts, so we are going to extract them, eliminating the blanks at the beginning and at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daaa4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json(DATASET_ZIP, lines=True)\n",
    "corpus = corpus['reviewText'].astype(str).str.strip()\n",
    "corpus.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88339f8",
   "metadata": {},
   "source": [
    "The variable `corpus` points to an array with all the sentences in our set. We are going to tokenize each of the comments, converting them into a list of words. For this we will use the `tokenizer` function included in keras, although it is important to understand that this step is not trivial and will probably require a lot of preprocessing to have a quality dataset (e.g. lemmatization, $n$-grams, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd22c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40a66f01",
   "metadata": {},
   "source": [
    "At this point, our tokenizer has processed all the comments and extracted all the words, assigning an identifier to each one. We will store them in two dictionaries so that we can convert them into integers (to identify the word) and into words (once we have the integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a49b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "index_word = {index: word for word, index in word_index.items()}\n",
    "\n",
    "print(f'word2id: {dict(list(word_index.items())[0:4])} ...')\n",
    "print(f'id2word: {dict(list(index_word.items())[0:4])} ...')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c711e2d2",
   "metadata": {},
   "source": [
    "Finally, each of the comments in the corpus will be transformed into a list of integers where each token in the corpus will be replaced by the integer it represents. We will also obtain the size of our vocabulary from the number of identified words.\n",
    "\n",
    "To convert a text string into a sequence of words we can use the Keras function `tf.keras.preprocessing.text.text_to_word_sequence(text)`. From there getting the index of each word is trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    [word_index[w] for w in tf.keras.preprocessing.text.text_to_word_sequence(text)]\n",
    "    for text in corpus\n",
    "]\n",
    "vocab_size = len(word_index) + 1\n",
    "\n",
    "print(f'Corpus sentences: {len(sentences)} sentences')\n",
    "print(f'Vocabulary Size: {vocab_size} words')\n",
    "print(f'Sentence example:')\n",
    "print(f'- {corpus[5]}')\n",
    "print(f'- {sentences[5]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e1952a1",
   "metadata": {},
   "source": [
    "## Skip-gram generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "af53c104",
   "metadata": {},
   "source": [
    "Now we will generate the _skip-grams_. The idea is, from all the sentences in the corpus (each `sentence` of `sentences`) and given an action window, to extract its context (the surrounding words) to determine for each pair of words whether they are contextual or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f1746c",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "\n",
    "x_train, y_train = [], []\n",
    "for sentence in sentences:\n",
    "    pairs, are_contextual = tf.keras.preprocessing.sequence.skipgrams(\n",
    "        sentence,\n",
    "        vocabulary_size=vocab_size,\n",
    "        window_size=WINDOW_SIZE,\n",
    "    )\n",
    "    x_train.extend(pairs)\n",
    "    y_train.extend(are_contextual)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train).reshape(-1, 1)\n",
    "dataset = np.hstack((x_train, y_train))\n",
    "\n",
    "print(f'Dataset shape: {dataset.shape}')\n",
    "print(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7ada0c3",
   "metadata": {},
   "source": [
    "It can be seen that it has been generated, for each pair of words, whether they are (1) or not (0) contextual. This is because the `skipgrams` function transforms a sequence of words (actually of integers) into tuples of the form:\n",
    "\n",
    "- `(word, word in context)`, label 1 (positive, are contextual).\n",
    "- `(word, random word outside of context)`, label 0 (not contextual)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e72a01dc",
   "metadata": {},
   "source": [
    "## Model creation and training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8fcfeb6",
   "metadata": {},
   "source": [
    "We already have a dataset with inputs and their respective outputs. Now the objective is to train a model that is able to determine if two words belong to the same context.\n",
    "\n",
    "To do this we will create an embedding layer that will transform the words into their feature vector. The words will be those that are or are not contextual, and it will determine how close (more contextual) or far (less contextual) they are, using a distance measure (scalar product).\n",
    "\n",
    "Finally, the output of the network will be a single neuron that will be activated or not if they are contextual.\n",
    "\n",
    "This architecture will force the more contextual words to be closer together, and therefore their feature vectors to be more similar. The weights matrix of the embedding layer is thus expected to converge to a representation of the word features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a44df22",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 5\n",
    "\n",
    "# Inputs to the model\n",
    "input_target = tf.keras.layers.Input((1,))\n",
    "input_context = tf.keras.layers.Input((1,))\n",
    "\n",
    "# Common layers (including the embedding)\n",
    "embedding_layer = tf.keras.layers.Embedding(\n",
    "    input_dim=vocab_size,\n",
    "    output_dim=EMBEDDING_DIM,\n",
    "    input_length=1,\n",
    "    name='embedding',\n",
    ")\n",
    "\n",
    "reshape_layer = tf.keras.layers.Reshape((EMBEDDING_DIM, 1))\n",
    "\n",
    "# Model architecture\n",
    "target_embedding = embedding_layer(input_target)\n",
    "target_embedding = reshape_layer(target_embedding)\n",
    "target_embedding = tf.keras.layers.Dropout(0.1)(target_embedding)\n",
    "\n",
    "context_embedding = embedding_layer(input_context)\n",
    "context_embedding = reshape_layer(context_embedding)\n",
    "context_embedding = tf.keras.layers.Dropout(0.1)(context_embedding)\n",
    "\n",
    "hidden_layer = tf.keras.layers.Dot(axes=1, normalize=True)([target_embedding, context_embedding])\n",
    "hidden_layer = tf.keras.layers.Reshape((1,))(hidden_layer)\n",
    "\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(hidden_layer)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_target, input_context], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1bcbde71",
   "metadata": {},
   "source": [
    "Now we only have to train the model. To do so, we will train it with each of the _skip-grams_ generated previously. We will use a validation separation of 10% and train for 10 epochs.\n",
    "\n",
    "**This step is very expensive**, and can take quite a few minutes (hours), so either we have a powerful machine, or we better leave it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf21a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([dataset[:, 0], dataset[:, 1]], dataset[:, 2], batch_size=8*32768, epochs=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "74812104",
   "metadata": {},
   "source": [
    "Let's take a look at the training progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961fc58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot()\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Epoch num.')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9742d5ba",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8e7f8c8",
   "metadata": {},
   "source": [
    "Once the model is trained, we already have a matrix with the weights of the features for each word. To see a representation we can take them directly and print them in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7527d055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weights = embedding_layer.get_weights()[0][1:]\n",
    "\n",
    "df = pd.DataFrame(weights, index=index_word.values())\n",
    "df.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4cf039e5",
   "metadata": {},
   "source": [
    "Let's make a search with the most similar words to a given one using, for example, the Euclidean distance of its vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10654a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLOSEST_WORDS = 10\n",
    "WORD = 'man'\n",
    "\n",
    "v1 = weights[word_index[WORD] - 1]\n",
    "words = sorted(\n",
    "    [word for word in word_index.keys()],\n",
    "    key=lambda w: np.linalg.norm(v1 - weights[word_index[w]-1])\n",
    ")\n",
    "df.loc[words[:NUM_CLOSEST_WORDS + 1], :]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ad020787",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "691f3670",
   "metadata": {},
   "source": [
    "In summary, we have implemented embedding using the _skip-grams_ technique of _word2vec_ and have demonstrated its effectiveness in representing words more meaningfully in a vector space. This technique is able to capture the semantics of words, representing them in a vector space of lower dimension than it would occupy with an _one-hot_ representation.\n",
    "\n",
    "Note, we also realized that **it is a very expensive technique**, and therefore does not make much sense (in general) since we have many ready-made _embeddings_ available for download."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc70b0a3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "<div><img style=\"float: right; width: 120px; vertical-align:top\" src=\"https://mirrors.creativecommons.org/presskit/buttons/88x31/png/by-nc-sa.png\" alt=\"Creative Commons by-nc-sa logo\" />\n",
    "\n",
    "[Volver al inicio](#top)\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
